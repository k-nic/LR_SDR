{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "F1Y_8jYdjORy",
    "outputId": "36ff5c4b-0fc7-4bb1-e76f-f901264c8989"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import scipy as scipy\n",
    "import scipy.interpolate as interpolate\n",
    "import math\n",
    "from random import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "Lambda = 10.0\n",
    "BATCH_SIZE = 56\n",
    "BATCH_SIZE_2 = 100\n",
    "dimensionality = 30\n",
    "num_epochs = 80\n",
    "num_iters = 1000\n",
    "k=2\n",
    "max_grad_norm = 1000\n",
    "gamma = 1.0\n",
    "\n",
    "# main parameters\n",
    "\n",
    "C=10.0\n",
    "\n",
    "# additional parameters\n",
    "sigma=0.8\n",
    "\n",
    "number_of_neurons_regr = 50\n",
    "number_of_points = 569-65\n",
    "number_of_neurons = number_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5ExlL7s3o9S6",
    "outputId": "82a79e11-a546-4fa7-81a1-de76f5b0011d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "breast = load_breast_cancer(return_X_y=False)\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(569)\n",
    "for x in rr:\n",
    "    data.append(breast['data'][x])\n",
    "    target.append(breast['target'][x])\n",
    "# training data\n",
    "_train_x = np.array(data)\n",
    "_train_y = np.array(target)\n",
    "print(_train_x.shape)\n",
    "print(_train_y.shape)\n",
    "rho = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvKNXjixjm-A"
   },
   "outputs": [],
   "source": [
    "new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "Dataplace = tf.placeholder(tf.float32, shape=(dimensionality, number_of_points))\n",
    "Data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, dimensionality))\n",
    "outputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE,1)) \n",
    "\n",
    "tf_data_x = gamma*tf.random_normal([BATCH_SIZE, dimensionality]) # аргументы функции\n",
    "tf_data_y = tf.reduce_mean(tf.math.cos(tf.matmul(tf_data_x, Dataplace)), axis=1) # значения функции\n",
    "\n",
    "tf_data_w = tf.placeholder(tf.float32, shape=(number_of_neurons,1))\n",
    "tf_data_B = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons))\n",
    "tf_data_P = tf.placeholder(tf.float32, shape=(dimensionality, dimensionality))\n",
    "\n",
    "tf_w_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr,1))\n",
    "tf_B_regr = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons_regr))\n",
    "tf_bias_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr))\n",
    "\n",
    "\n",
    "tf_data_second = gamma*tf.random_normal([BATCH_SIZE_2, dimensionality])\n",
    "Lbd = tf.placeholder(tf.float32, shape=[], name=\"lambda\")\n",
    "\n",
    "# characteristic function parameters\n",
    "w = tf.Variable(tf.random_normal([number_of_neurons,1], stddev=0.0), name=\"neuron_weights\")\n",
    "B = tf.Variable(initial_value=Dataplace, name=\"weights\")\n",
    "# regression function parameters\n",
    "w_regr = tf.Variable(tf.random_normal([number_of_neurons_regr,1], stddev=0.35), name=\"neuron_weights\")\n",
    "B_regr = tf.Variable(tf.random_normal([dimensionality,number_of_neurons_regr], stddev=0.35), name=\"weights\")\n",
    "bias_regr = tf.Variable(tf.random_normal([number_of_neurons_regr], stddev=0.0), name=\"biases\")\n",
    "\n",
    "prediction = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_x, B)), tf.nn.sigmoid(w))\n",
    "penalty = tf.square((2/number_of_neurons)*tf.reduce_sum(tf.nn.sigmoid(w))-1.0)\n",
    "\n",
    "out_loss = (1-rho)*tf.reduce_mean(tf.square(prediction - tf_data_y)) + C*penalty\n",
    "\n",
    "prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, B)), tf.nn.sigmoid(w))\n",
    "prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, B_regr), bias_regr)), w_regr)\n",
    "\n",
    "grad_psi = tf.reshape(tf.gradients(prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "grad_psi_regr = tf.reshape(tf.gradients(prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "tf_prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, tf_data_B)), tf.nn.sigmoid(tf_data_w))\n",
    "tf_prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, tf_B_regr), tf_bias_regr)), tf_w_regr)\n",
    "\n",
    "tf_data_grad_psi = tf.reshape(tf.gradients(tf_prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "tf_data_grad_psi_regr = tf.reshape(tf.gradients(tf_prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "old_part = tf.matmul(tf_data_grad_psi, tf_data_P)\n",
    "old_part_regr = tf.matmul(tf_data_grad_psi_regr, tf_data_P)\n",
    "\n",
    "loss = out_loss + Lbd*(1-rho)*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi, old_part)), axis=1)) + Lbd*rho*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi_regr, old_part_regr)), axis=1))\n",
    "\n",
    "#offset = tf.random.uniform(shape=[], minval=0, maxval=12, dtype=tf.int32)*BATCH_SIZE\n",
    "x_plus_error = Data+sigma*tf.random_normal([BATCH_SIZE, dimensionality])\n",
    "regression = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(x_plus_error, B_regr), bias_regr)), w_regr)\n",
    "sdr = -tf.reduce_mean(tf.multiply(regression, outputs)) + tf.reduce_mean(tf.math.log(1.0+tf.math.exp(regression)))\n",
    "\n",
    "loss_hybrid = loss + rho*sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3HlgJ8XdjtxY",
    "outputId": "8ec5f3f0-92f7-4712-94c2-e9bff821b918"
   },
   "outputs": [],
   "source": [
    "target = tf.train.AdamOptimizer(learning_rate=3e-5, beta1=0.5, beta2=0.9).minimize(loss_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YojZRg6lMgur"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1. /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pnt1, pnt2):\n",
    "    '''Finds the distance between 2 points: pnt1, pnt2'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((pnt1 - pnt2) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "\n",
    "def new_predict(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "def new_predict_regr(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return np.mean(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HLqISRILj5VQ",
    "outputId": "83373660-15c5-4d88-a2da-ec9bcf3a5a56",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current part is 0\n",
      "\n",
      "Epoch 0\n",
      "Iter 1100: loss: 10.6162\n",
      "\n",
      "[26.451145   15.300708   11.768134    9.265918    8.797858    6.078735\n",
      "  4.240258    3.7700136   3.2244456   2.2158964   1.9548602   1.590778\n",
      "  1.4978124   1.3853097   1.1312615   1.0505191   0.9506182   0.6341434\n",
      "  0.56424135  0.49286655  0.40658563  0.3432738   0.29467338  0.24772355\n",
      "  0.20750587  0.15562351  0.13662776  0.11615233  0.09892901  0.0702045 ]\n",
      "iter 1\ttrain cost\t10.616240501403809\ttotal variance to retain\t0.30069243907928467\n",
      "Epoch 1\n",
      "Iter 2200: loss: 5.4034\n",
      "\n",
      "[21.7769     11.627873   10.264717    7.75609     6.429472    4.6648884\n",
      "  3.2958965   2.3296583   2.1756048   1.909525    1.737791    1.4957298\n",
      "  1.2459792   0.8342622   0.77850634  0.74018985  0.6589072   0.5626897\n",
      "  0.38817304  0.34455165  0.2715573   0.24300097  0.20095457  0.16860205\n",
      "  0.15111725  0.11247508  0.09530371  0.07984574  0.07348751  0.04688804]\n",
      "iter 2\ttrain cost\t5.4033684730529785\ttotal variance to retain\t0.3016374111175537\n",
      "Epoch 2\n",
      "Iter 3300: loss: 4.2129\n",
      "\n",
      "[15.186252   10.585781    7.3439894   5.6963305   4.2773786   3.9037032\n",
      "  2.0745664   1.834875    1.4952884   1.417708    1.2466128   0.998287\n",
      "  0.83376604  0.6309172   0.5744431   0.43562493  0.40582344  0.34274405\n",
      "  0.32766932  0.24663574  0.19183896  0.18032186  0.15082888  0.12573507\n",
      "  0.11543465  0.08449405  0.06674688  0.05276725  0.04724345  0.03433244]\n",
      "iter 3\ttrain cost\t4.212902069091797\ttotal variance to retain\t0.28499019145965576\n",
      "Epoch 3\n",
      "Iter 4400: loss: 3.1863\n",
      "\n",
      "[16.550768    8.492933    6.2717566   5.1701746   3.2850099   2.8522236\n",
      "  1.7339941   1.2010485   1.1553445   1.0851916   0.9212675   0.7659484\n",
      "  0.57853717  0.5562237   0.44715515  0.38826033  0.36501393  0.27284023\n",
      "  0.20388284  0.15765594  0.13648152  0.1162503   0.08982381  0.07256655\n",
      "  0.05776285  0.04357059  0.03987716  0.03468766  0.03326615  0.02332997]\n",
      "iter 4\ttrain cost\t3.186272382736206\ttotal variance to retain\t0.21488076448440552\n",
      "Epoch 4\n",
      "Iter 5500: loss: 2.4179\n",
      "\n",
      "[2.02701015e+01 5.90243864e+00 3.72441339e+00 3.61668754e+00\n",
      " 2.76194596e+00 1.67046273e+00 1.38556826e+00 1.04347181e+00\n",
      " 9.91988301e-01 7.58137524e-01 5.84948957e-01 5.20394683e-01\n",
      " 4.34087664e-01 3.56610537e-01 3.08708459e-01 2.64075696e-01\n",
      " 2.43408814e-01 1.92836076e-01 1.44428104e-01 1.21509388e-01\n",
      " 8.67608413e-02 7.19512403e-02 6.23982437e-02 4.90161069e-02\n",
      " 4.11150530e-02 3.65638770e-02 2.48097740e-02 2.18185093e-02\n",
      " 1.95491556e-02 1.25976885e-02]\n",
      "iter 5\ttrain cost\t2.4178528785705566\ttotal variance to retain\t0.08833503723144531\n",
      "Epoch 5\n",
      "Iter 6600: loss: 1.8208\n",
      "\n",
      "[1.52083244e+01 5.15586615e+00 2.78649545e+00 2.56402111e+00\n",
      " 2.31201100e+00 1.16805935e+00 1.00870526e+00 7.08389461e-01\n",
      " 6.76936746e-01 5.76879323e-01 5.68459392e-01 4.29418564e-01\n",
      " 2.91430056e-01 2.30812922e-01 1.91017151e-01 1.57497555e-01\n",
      " 1.32228330e-01 1.20534785e-01 1.03567824e-01 7.03620091e-02\n",
      " 6.37440905e-02 5.47132865e-02 4.90178429e-02 3.85083742e-02\n",
      " 2.68325415e-02 2.24788785e-02 1.73042417e-02 1.17932325e-02\n",
      " 1.01007987e-02 9.00766067e-03]\n",
      "iter 6\ttrain cost\t1.8208087682724\ttotal variance to retain\t0.08555388450622559\n",
      "Epoch 6\n",
      "Iter 7700: loss: 1.4011\n",
      "\n",
      "[1.77009563e+01 3.23567533e+00 2.36087465e+00 1.89375591e+00\n",
      " 1.44713044e+00 8.97438526e-01 6.71545148e-01 4.79902357e-01\n",
      " 4.38548297e-01 3.72010380e-01 3.61287266e-01 2.98039943e-01\n",
      " 2.03717515e-01 1.89675912e-01 1.40404463e-01 1.13659866e-01\n",
      " 1.03138179e-01 8.57184455e-02 7.73138255e-02 5.88712990e-02\n",
      " 4.69771773e-02 4.34934683e-02 2.89190840e-02 2.19618231e-02\n",
      " 1.90343708e-02 1.52156111e-02 1.22021157e-02 8.75375699e-03\n",
      " 7.43230712e-03 6.41109422e-03]\n",
      "iter 7\ttrain cost\t1.4011027812957764\ttotal variance to retain\t0.039839088916778564\n",
      "Epoch 7\n",
      "Iter 8800: loss: 1.0662\n",
      "\n",
      "[1.49512253e+01 3.15866232e+00 1.64283705e+00 1.30622065e+00\n",
      " 9.53030705e-01 7.93914199e-01 6.22979283e-01 3.39229912e-01\n",
      " 3.33921015e-01 2.38213405e-01 2.22675964e-01 1.85488880e-01\n",
      " 1.40071675e-01 1.17790826e-01 1.02933906e-01 7.98669010e-02\n",
      " 7.46374875e-02 6.36663958e-02 5.59870042e-02 4.27152514e-02\n",
      " 3.43542621e-02 2.48382129e-02 2.15020850e-02 1.49004329e-02\n",
      " 1.15343416e-02 1.01481872e-02 8.11655074e-03 7.23157218e-03\n",
      " 4.94269282e-03 3.32320319e-03]\n",
      "iter 8\ttrain cost\t1.0662157535552979\ttotal variance to retain\t0.028162777423858643\n",
      "Epoch 8\n",
      "Iter 9900: loss: 0.8312\n",
      "\n",
      "[1.63632050e+01 2.78725672e+00 1.60584009e+00 8.41739535e-01\n",
      " 7.23798335e-01 6.04485154e-01 3.77613246e-01 2.52894759e-01\n",
      " 2.28852198e-01 1.83551297e-01 1.61471829e-01 1.42062709e-01\n",
      " 1.18512966e-01 9.82766226e-02 7.74335489e-02 6.79572672e-02\n",
      " 5.62478341e-02 4.57194485e-02 3.36590260e-02 2.83014681e-02\n",
      " 2.27015130e-02 1.86704639e-02 1.47424880e-02 1.36630125e-02\n",
      " 9.30394046e-03 7.44574610e-03 6.84397202e-03 5.07210335e-03\n",
      " 3.47796548e-03 2.97661591e-03]\n",
      "iter 9\ttrain cost\t0.8312137126922607\ttotal variance to retain\t0.016275405883789062\n",
      "Epoch 9\n",
      "Iter 11000: loss: 0.6594\n",
      "\n",
      "[1.3994650e+01 1.9020168e+00 1.0871147e+00 5.2706498e-01 4.6030352e-01\n",
      " 4.1508496e-01 3.1867841e-01 2.3963852e-01 1.8074517e-01 1.3213375e-01\n",
      " 1.0685234e-01 8.8440992e-02 7.5998344e-02 6.5602541e-02 5.8849417e-02\n",
      " 4.9053099e-02 4.2409196e-02 2.8439185e-02 2.2550508e-02 1.9347975e-02\n",
      " 1.7265934e-02 1.4171426e-02 8.9410916e-03 7.8996606e-03 7.4033607e-03\n",
      " 4.8188423e-03 4.0542176e-03 3.7704497e-03 2.2767452e-03 2.0124530e-03]\n",
      "iter 10\ttrain cost\t0.6594312191009521\ttotal variance to retain\t0.010380804538726807\n",
      "Epoch 10\n",
      "Iter 12100: loss: 0.5438\n",
      "\n",
      "[1.5982231e+01 1.3506644e+00 8.1079692e-01 3.8175422e-01 3.3026075e-01\n",
      " 2.5921905e-01 2.0920759e-01 1.4852509e-01 1.2531915e-01 9.1821603e-02\n",
      " 8.1593998e-02 7.0675164e-02 5.2285243e-02 4.0453449e-02 3.8250782e-02\n",
      " 3.4921926e-02 2.6322654e-02 2.1732254e-02 1.6430490e-02 1.4869864e-02\n",
      " 1.1023214e-02 9.7240051e-03 7.2042681e-03 6.1308425e-03 4.8690652e-03\n",
      " 4.7030705e-03 3.0670566e-03 2.6442227e-03 1.8700450e-03 1.3596755e-03]\n",
      "iter 11\ttrain cost\t0.5438212752342224\ttotal variance to retain\t0.004219472408294678\n",
      "Epoch 11\n",
      "Iter 13200: loss: 0.4772\n",
      "\n",
      "[1.58899708e+01 1.24055016e+00 8.10396791e-01 3.08489025e-01\n",
      " 1.84143230e-01 1.77017823e-01 1.64180622e-01 1.26164079e-01\n",
      " 9.70605314e-02 6.14874065e-02 5.86233921e-02 4.92374562e-02\n",
      " 3.75777893e-02 3.45758349e-02 2.87021473e-02 2.65509617e-02\n",
      " 2.14138497e-02 1.77953541e-02 1.25967795e-02 1.01984758e-02\n",
      " 8.66919197e-03 7.64336204e-03 5.45312464e-03 4.66199312e-03\n",
      " 4.03381186e-03 2.91747577e-03 2.87818443e-03 2.34181900e-03\n",
      " 1.75733364e-03 1.22898957e-03]\n",
      "iter 12\ttrain cost\t0.47719627618789673\ttotal variance to retain\t0.003469705581665039\n",
      "Epoch 12\n",
      "Iter 14300: loss: 0.4301\n",
      "\n",
      "[1.51550398e+01 1.60999668e+00 5.37027121e-01 3.12536567e-01\n",
      " 1.50773317e-01 1.47252053e-01 1.14850372e-01 9.42453742e-02\n",
      " 8.09193328e-02 6.16365597e-02 5.43518178e-02 4.60454077e-02\n",
      " 3.95642221e-02 2.72161029e-02 2.18379274e-02 1.91619284e-02\n",
      " 1.77514236e-02 1.50502045e-02 1.25631439e-02 1.14497403e-02\n",
      " 7.04936776e-03 5.88067994e-03 4.54016263e-03 3.88662005e-03\n",
      " 3.20725329e-03 2.78074853e-03 2.25659437e-03 1.76114833e-03\n",
      " 1.50757970e-03 9.31666349e-04]\n",
      "iter 13\ttrain cost\t0.43006888031959534\ttotal variance to retain\t0.002028524875640869\n",
      "Epoch 13\n",
      "Iter 15400: loss: 0.4013\n",
      "\n",
      "[1.6151627e+01 1.2756515e+00 5.7452077e-01 2.4643056e-01 1.5043646e-01\n",
      " 1.2187608e-01 9.8973565e-02 8.3908558e-02 6.5284908e-02 5.1020820e-02\n",
      " 4.9727190e-02 3.6931131e-02 3.1313863e-02 2.7378643e-02 2.0235566e-02\n",
      " 1.8204104e-02 1.4387302e-02 1.3348836e-02 1.2666710e-02 8.9372173e-03\n",
      " 5.5037993e-03 4.3781325e-03 4.2418707e-03 3.7257904e-03 3.5081285e-03\n",
      " 3.0929532e-03 1.7617588e-03 1.5950317e-03 1.2478004e-03 9.2570373e-04]\n",
      "iter 14\ttrain cost\t0.4012688100337982\ttotal variance to retain\t0.0017455816268920898\n",
      "Epoch 14\n",
      "Iter 16500: loss: 0.3788\n",
      "\n",
      "[1.7380917e+01 1.1316662e+00 5.6919771e-01 1.9829404e-01 1.3306229e-01\n",
      " 9.5997423e-02 7.5574160e-02 5.5255480e-02 4.2508680e-02 3.7043791e-02\n",
      " 2.8645149e-02 2.5626080e-02 2.4424490e-02 1.9559063e-02 1.7343713e-02\n",
      " 1.3833431e-02 1.1654615e-02 1.1334829e-02 1.0441493e-02 8.7464759e-03\n",
      " 5.5288617e-03 4.6443320e-03 3.8202931e-03 3.0164300e-03 2.8114282e-03\n",
      " 2.1991238e-03 1.7804597e-03 1.4453834e-03 1.2601303e-03 1.0413477e-03]\n",
      "iter 15\ttrain cost\t0.37881672382354736\ttotal variance to retain\t0.0013352036476135254\n",
      "Epoch 15\n",
      "Iter 17600: loss: 0.3635\n",
      "\n",
      "[1.7253983e+01 1.2055945e+00 5.5515540e-01 3.1116340e-01 1.8097663e-01\n",
      " 1.4895526e-01 1.1029190e-01 6.4390332e-02 3.8377419e-02 3.5168000e-02\n",
      " 3.0175250e-02 2.8234810e-02 2.2084497e-02 2.1585427e-02 1.6030051e-02\n",
      " 1.3977229e-02 1.3003218e-02 1.1487828e-02 8.9539140e-03 6.9664121e-03\n",
      " 5.0048712e-03 4.6880697e-03 3.4930015e-03 3.0415736e-03 2.8378121e-03\n",
      " 2.5373460e-03 1.4871028e-03 1.1584572e-03 1.0518719e-03 7.5861212e-04]\n",
      "iter 16\ttrain cost\t0.36348873376846313\ttotal variance to retain\t0.0016106367111206055\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18700: loss: 0.3493\n",
      "\n",
      "[1.54088526e+01 9.26286876e-01 3.57707679e-01 2.43378624e-01\n",
      " 1.26016304e-01 7.32099488e-02 6.54434264e-02 5.30575700e-02\n",
      " 3.72064412e-02 3.11474912e-02 2.43688859e-02 2.10799910e-02\n",
      " 1.73746571e-02 1.52987605e-02 1.28815835e-02 1.23762833e-02\n",
      " 1.13025103e-02 8.96058138e-03 8.16213712e-03 6.61623850e-03\n",
      " 4.22155671e-03 3.87768983e-03 3.27823800e-03 2.88829487e-03\n",
      " 2.62670498e-03 1.92819419e-03 1.30222563e-03 1.13856373e-03\n",
      " 1.06091576e-03 7.93162792e-04]\n",
      "iter 17\ttrain cost\t0.34925830364227295\ttotal variance to retain\t0.0009230375289916992\n",
      "Epoch 17\n",
      "Iter 19800: loss: 0.3391\n",
      "\n",
      "[1.79994164e+01 1.22393548e+00 4.54639524e-01 2.87541091e-01\n",
      " 1.75691992e-01 1.09122895e-01 7.45758042e-02 5.31475917e-02\n",
      " 3.56105417e-02 2.94849817e-02 2.41470635e-02 2.05117911e-02\n",
      " 1.62803680e-02 1.51199251e-02 1.40262553e-02 9.37859807e-03\n",
      " 8.41613486e-03 7.92658981e-03 7.32875708e-03 6.03190297e-03\n",
      " 3.77302687e-03 3.58464639e-03 3.43811722e-03 2.70477240e-03\n",
      " 2.44325260e-03 2.01835320e-03 1.67065708e-03 1.25632703e-03\n",
      " 1.05139811e-03 5.53051708e-04]\n",
      "iter 18\ttrain cost\t0.3391357362270355\ttotal variance to retain\t0.001058042049407959\n",
      "Epoch 18\n",
      "Iter 20900: loss: 0.3288\n",
      "\n",
      "[1.67349377e+01 1.06657529e+00 4.88782197e-01 2.75835603e-01\n",
      " 1.53954178e-01 1.01251975e-01 6.42780140e-02 5.37027530e-02\n",
      " 3.49209271e-02 3.03835981e-02 2.43696645e-02 1.97282173e-02\n",
      " 1.55544486e-02 1.49387708e-02 1.23885358e-02 1.06460238e-02\n",
      " 9.19282157e-03 8.26920569e-03 7.68301869e-03 7.04032835e-03\n",
      " 4.12355782e-03 3.78352450e-03 3.25261289e-03 2.84413970e-03\n",
      " 2.43685418e-03 2.07841676e-03 1.60549372e-03 1.21076161e-03\n",
      " 9.89103923e-04 6.07984897e-04]\n",
      "iter 19\ttrain cost\t0.3287515938282013\ttotal variance to retain\t0.0012791156768798828\n",
      "Epoch 19\n",
      "Iter 22000: loss: 0.3161\n",
      "\n",
      "[1.93324394e+01 1.14850545e+00 4.20612812e-01 2.94573367e-01\n",
      " 1.17570341e-01 1.01885125e-01 7.25943819e-02 6.08836561e-02\n",
      " 3.32332440e-02 2.68706419e-02 2.22540628e-02 1.97737571e-02\n",
      " 1.56737063e-02 1.45276608e-02 1.05743734e-02 1.02737434e-02\n",
      " 8.43703840e-03 7.76644750e-03 6.64932840e-03 5.16943540e-03\n",
      " 3.56851285e-03 3.34773702e-03 3.18798283e-03 2.73292954e-03\n",
      " 2.21300079e-03 1.49388250e-03 1.22630922e-03 1.12260308e-03\n",
      " 9.53437877e-04 6.68867200e-04]\n",
      "iter 20\ttrain cost\t0.31611496210098267\ttotal variance to retain\t0.0008005499839782715\n",
      "Epoch 20\n",
      "Iter 23100: loss: 0.3064\n",
      "\n",
      "[1.6501461e+01 1.3666289e+00 3.4252352e-01 1.9383170e-01 1.3636152e-01\n",
      " 1.0654098e-01 7.2295897e-02 5.0579764e-02 3.4885086e-02 2.4857111e-02\n",
      " 1.9342979e-02 1.7405599e-02 1.4625528e-02 1.1160285e-02 1.0169749e-02\n",
      " 9.4146794e-03 8.1948387e-03 7.8183133e-03 6.2458823e-03 5.5370135e-03\n",
      " 3.9228923e-03 3.5555121e-03 3.3085446e-03 2.6831455e-03 2.4199302e-03\n",
      " 2.0846934e-03 1.2083373e-03 1.1085764e-03 8.6642063e-04 6.2025141e-04]\n",
      "iter 21\ttrain cost\t0.30641478300094604\ttotal variance to retain\t0.0007140040397644043\n",
      "Epoch 21\n",
      "Iter 24200: loss: 0.2959\n",
      "\n",
      "[1.79541473e+01 1.21859169e+00 3.26826513e-01 2.76390523e-01\n",
      " 1.48603082e-01 8.30417275e-02 6.85505345e-02 5.40393479e-02\n",
      " 3.11848167e-02 2.72437334e-02 2.12364830e-02 1.91414524e-02\n",
      " 1.67911481e-02 1.37417270e-02 1.24482727e-02 1.14178695e-02\n",
      " 8.79619550e-03 7.70246983e-03 6.58009714e-03 5.36091765e-03\n",
      " 3.90387629e-03 3.47915292e-03 3.35126440e-03 2.83868005e-03\n",
      " 2.37607537e-03 2.19393941e-03 1.37768290e-03 1.09566853e-03\n",
      " 8.87863222e-04 5.27953729e-04]\n",
      "iter 22\ttrain cost\t0.29587244987487793\ttotal variance to retain\t0.0006893277168273926\n",
      "Epoch 22\n",
      "Iter 25300: loss: 0.2886\n",
      "\n",
      "[1.75789928e+01 1.45697415e+00 3.01611125e-01 2.05144629e-01\n",
      " 1.21811055e-01 9.18205753e-02 6.07419647e-02 4.60507199e-02\n",
      " 2.79043056e-02 2.38357093e-02 2.15606019e-02 1.73066948e-02\n",
      " 1.40512101e-02 1.13238422e-02 1.08194007e-02 8.58869031e-03\n",
      " 7.24768825e-03 6.23403816e-03 5.09978225e-03 4.78141522e-03\n",
      " 4.03522840e-03 3.30845965e-03 2.90383794e-03 2.63967249e-03\n",
      " 2.25758459e-03 2.03119172e-03 1.32963492e-03 1.08753704e-03\n",
      " 9.27633606e-04 5.03005984e-04]\n",
      "iter 23\ttrain cost\t0.2886189818382263\ttotal variance to retain\t0.000529944896697998\n",
      "Epoch 23\n",
      "Iter 26400: loss: 0.2780\n",
      "\n",
      "[2.21759663e+01 1.22737396e+00 3.10931325e-01 2.66205341e-01\n",
      " 2.07179472e-01 1.17771216e-01 6.13951087e-02 4.61905077e-02\n",
      " 3.17116305e-02 2.65355110e-02 2.48349980e-02 1.90005563e-02\n",
      " 1.33403167e-02 1.04122004e-02 9.49300546e-03 8.79413914e-03\n",
      " 6.83940900e-03 5.23531344e-03 5.12327300e-03 4.22992837e-03\n",
      " 3.55385500e-03 3.05035990e-03 2.83404044e-03 2.41176505e-03\n",
      " 1.88863382e-03 1.66095991e-03 1.40656380e-03 1.07893336e-03\n",
      " 8.86023627e-04 4.97323112e-04]\n",
      "iter 24\ttrain cost\t0.2780236005783081\ttotal variance to retain\t0.00047326087951660156\n",
      "Epoch 24\n",
      "Iter 27500: loss: 0.2701\n",
      "\n",
      "[2.1945631e+01 1.1524487e+00 2.6570696e-01 2.3167355e-01 1.1582799e-01\n",
      " 9.7212121e-02 6.8933316e-02 5.0469864e-02 3.0011030e-02 2.4762679e-02\n",
      " 1.7957658e-02 1.6539050e-02 1.3496918e-02 9.8864138e-03 8.6159902e-03\n",
      " 8.4280409e-03 6.6859182e-03 4.9494649e-03 4.5651714e-03 4.2592967e-03\n",
      " 3.2013357e-03 3.0719393e-03 2.8839237e-03 2.2040131e-03 2.0464952e-03\n",
      " 1.6849238e-03 1.2672408e-03 9.0823456e-04 8.6006965e-04 5.9018389e-04]\n",
      "iter 25\ttrain cost\t0.2700670659542084\ttotal variance to retain\t0.00032520294189453125\n",
      "Epoch 25\n",
      "Iter 28600: loss: 0.2616\n",
      "\n",
      "[2.2535261e+01 1.5234870e+00 2.6770446e-01 1.8485960e-01 1.3253535e-01\n",
      " 6.5570116e-02 5.0279021e-02 4.5133166e-02 2.7239786e-02 2.5251869e-02\n",
      " 2.2978302e-02 1.6491257e-02 1.3834363e-02 1.1675714e-02 9.7137904e-03\n",
      " 7.5911600e-03 5.7507632e-03 5.4272260e-03 4.2964472e-03 3.9422321e-03\n",
      " 3.6746834e-03 2.8669136e-03 2.6718557e-03 2.4587044e-03 1.7177634e-03\n",
      " 1.6562692e-03 1.3191953e-03 9.0894062e-04 7.6027942e-04 4.3764903e-04]\n",
      "iter 26\ttrain cost\t0.261636883020401\ttotal variance to retain\t0.0002646446228027344\n",
      "Epoch 26\n",
      "Iter 29700: loss: 0.2561\n",
      "\n",
      "[2.1297544e+01 1.5516205e+00 2.5885901e-01 2.0256437e-01 1.1132280e-01\n",
      " 1.0404043e-01 7.6132268e-02 4.7226354e-02 3.2558817e-02 2.1628728e-02\n",
      " 1.9962540e-02 1.8515324e-02 1.2810196e-02 1.2017006e-02 1.0220204e-02\n",
      " 7.9454528e-03 6.2243468e-03 5.3595924e-03 4.4921534e-03 3.8208566e-03\n",
      " 3.4581528e-03 2.9701430e-03 2.7570559e-03 2.2371558e-03 1.7904039e-03\n",
      " 1.4093746e-03 1.2053708e-03 9.4537373e-04 6.3056534e-04 4.9438764e-04]\n",
      "iter 27\ttrain cost\t0.256130576133728\ttotal variance to retain\t0.0003116130828857422\n",
      "Epoch 27\n",
      "Iter 30800: loss: 0.2474\n",
      "\n",
      "[2.2835566e+01 1.4872767e+00 2.8289428e-01 1.5746744e-01 1.2533166e-01\n",
      " 1.0575563e-01 6.2186424e-02 5.2342031e-02 3.2514151e-02 2.2744708e-02\n",
      " 1.9258985e-02 1.4325003e-02 1.3014602e-02 1.2101227e-02 9.7317779e-03\n",
      " 7.4670692e-03 5.8974689e-03 4.4630151e-03 3.9581498e-03 3.6971788e-03\n",
      " 2.6567047e-03 2.4968097e-03 2.3086739e-03 1.7606576e-03 1.6975699e-03\n",
      " 1.3363149e-03 1.0743497e-03 8.1926823e-04 5.9256778e-04 5.1809935e-04]\n",
      "iter 28\ttrain cost\t0.2474006861448288\ttotal variance to retain\t0.00026929378509521484\n",
      "Epoch 28\n",
      "Iter 31900: loss: 0.2410\n",
      "\n",
      "[2.35590630e+01 1.27459931e+00 2.37713143e-01 1.51100188e-01\n",
      " 1.19634248e-01 9.82594416e-02 5.55520803e-02 4.04300243e-02\n",
      " 3.59318666e-02 3.12964208e-02 2.04149764e-02 1.72711648e-02\n",
      " 1.46760354e-02 1.17481025e-02 1.04199406e-02 9.29175317e-03\n",
      " 5.13995998e-03 4.05247789e-03 3.75316059e-03 3.31230951e-03\n",
      " 2.98312586e-03 2.83096381e-03 2.44660396e-03 2.01998535e-03\n",
      " 1.50927983e-03 1.33339560e-03 1.07347325e-03 7.69830775e-04\n",
      " 5.44939190e-04 4.13786154e-04]\n",
      "iter 29\ttrain cost\t0.2409663051366806\ttotal variance to retain\t0.00020039081573486328\n",
      "Epoch 29\n",
      "Iter 33000: loss: 0.2362\n",
      "\n",
      "[2.5029860e+01 1.3977640e+00 2.2907037e-01 1.4771210e-01 1.2760587e-01\n",
      " 8.0346569e-02 5.4364290e-02 4.2740557e-02 3.0755630e-02 2.1532767e-02\n",
      " 1.9385142e-02 1.6700540e-02 1.3251195e-02 1.2490199e-02 1.0064836e-02\n",
      " 9.5997779e-03 6.0599735e-03 4.3665902e-03 4.0726089e-03 3.6553876e-03\n",
      " 2.7580946e-03 2.3693077e-03 2.1983855e-03 1.7932852e-03 1.7164272e-03\n",
      " 1.3133446e-03 1.0642284e-03 8.2276942e-04 5.8913045e-04 4.2169431e-04]\n",
      "iter 30\ttrain cost\t0.2362401783466339\ttotal variance to retain\t0.00016623735427856445\n",
      "Epoch 30\n",
      "Iter 34100: loss: 0.2331\n",
      "\n",
      "[2.64037476e+01 1.32205641e+00 2.30469778e-01 1.58178627e-01\n",
      " 1.16535194e-01 7.30938539e-02 6.72206283e-02 3.69324498e-02\n",
      " 2.74510533e-02 2.28021406e-02 1.90370213e-02 1.71133615e-02\n",
      " 1.29541773e-02 1.02409972e-02 8.09896830e-03 6.21774979e-03\n",
      " 5.02733607e-03 3.87341436e-03 3.21748736e-03 3.14831338e-03\n",
      " 2.85413512e-03 2.41184630e-03 2.00350722e-03 1.83883542e-03\n",
      " 1.51050475e-03 1.29104254e-03 9.29176225e-04 7.77024776e-04\n",
      " 5.69191412e-04 3.82795930e-04]\n",
      "iter 31\ttrain cost\t0.2331382781267166\ttotal variance to retain\t0.0001506209373474121\n",
      "Epoch 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35200: loss: 0.2249\n",
      "\n",
      "[2.6521061e+01 1.2424796e+00 1.9201313e-01 1.6488983e-01 1.3156673e-01\n",
      " 9.9559128e-02 7.4872062e-02 4.5226432e-02 2.6797628e-02 2.4254216e-02\n",
      " 1.8694839e-02 1.6865002e-02 1.3147608e-02 1.2293542e-02 8.8964039e-03\n",
      " 6.9157816e-03 4.3528681e-03 3.7466653e-03 3.4395801e-03 3.1135431e-03\n",
      " 2.5028703e-03 2.1540404e-03 1.8638951e-03 1.5328020e-03 1.4888860e-03\n",
      " 1.2524114e-03 8.0455269e-04 7.2916556e-04 4.6284113e-04 3.7694621e-04]\n",
      "iter 32\ttrain cost\t0.2248784899711609\ttotal variance to retain\t0.00014382600784301758\n",
      "Epoch 32\n",
      "Iter 36300: loss: 0.2228\n",
      "\n",
      "[2.6597078e+01 1.3174642e+00 2.0537826e-01 1.4885175e-01 1.2351540e-01\n",
      " 8.4939025e-02 7.4946374e-02 5.1529415e-02 3.0753229e-02 2.6237890e-02\n",
      " 2.0888157e-02 1.7706970e-02 1.6049147e-02 1.3310082e-02 9.7434912e-03\n",
      " 8.5715940e-03 5.2382178e-03 3.9290967e-03 3.0562342e-03 2.7927719e-03\n",
      " 2.2176174e-03 2.2008610e-03 1.7071842e-03 1.4331414e-03 1.2530413e-03\n",
      " 1.1126443e-03 9.1543817e-04 7.0194696e-04 4.8163446e-04 2.6603413e-04]\n",
      "iter 33\ttrain cost\t0.22281312942504883\ttotal variance to retain\t0.00013828277587890625\n",
      "Epoch 33\n",
      "Iter 37400: loss: 0.2183\n",
      "\n",
      "[2.7856447e+01 1.3484434e+00 2.1406776e-01 1.5829197e-01 8.6856142e-02\n",
      " 8.6028636e-02 6.1935462e-02 3.7643339e-02 3.1034220e-02 2.0430561e-02\n",
      " 1.8216308e-02 1.6324304e-02 1.3364364e-02 1.2107086e-02 8.5389940e-03\n",
      " 7.1431333e-03 4.3751658e-03 3.7752844e-03 3.2844041e-03 3.0112073e-03\n",
      " 2.5665069e-03 2.0077410e-03 1.7946901e-03 1.4234188e-03 1.2899813e-03\n",
      " 1.1369853e-03 9.4619923e-04 6.7895930e-04 4.4267272e-04 3.1328964e-04]\n",
      "iter 34\ttrain cost\t0.21830682456493378\ttotal variance to retain\t0.00012028217315673828\n",
      "Epoch 34\n",
      "Iter 38500: loss: 0.2140\n",
      "\n",
      "[2.6702621e+01 1.3483821e+00 2.4699186e-01 1.3773091e-01 1.2175871e-01\n",
      " 1.0599611e-01 6.2094349e-02 4.2074427e-02 2.9159939e-02 2.8341537e-02\n",
      " 1.9019101e-02 1.8824492e-02 1.0663789e-02 8.4409779e-03 7.5062006e-03\n",
      " 6.7208069e-03 4.4368650e-03 3.8221958e-03 2.9792422e-03 2.7132982e-03\n",
      " 2.3557732e-03 2.3051545e-03 1.8204529e-03 1.5307707e-03 1.3406806e-03\n",
      " 9.6815906e-04 8.2258921e-04 6.3912122e-04 4.2081060e-04 3.1416060e-04]\n",
      "iter 35\ttrain cost\t0.21395385265350342\ttotal variance to retain\t0.00015997886657714844\n",
      "Epoch 35\n",
      "Iter 39600: loss: 0.2096\n",
      "\n",
      "[2.86187134e+01 1.31848919e+00 1.98184818e-01 1.29625037e-01\n",
      " 1.22857906e-01 9.45147350e-02 6.06607124e-02 5.10476790e-02\n",
      " 3.15546542e-02 2.49158107e-02 1.95309538e-02 1.60569940e-02\n",
      " 1.28805889e-02 1.20118717e-02 8.23414605e-03 7.54673313e-03\n",
      " 5.73654519e-03 4.61611524e-03 3.17278784e-03 2.70459289e-03\n",
      " 2.28335685e-03 1.99108920e-03 1.54823891e-03 1.43040495e-03\n",
      " 1.15410029e-03 9.47405759e-04 7.99380534e-04 6.45586988e-04\n",
      " 4.49716841e-04 2.60234083e-04]\n",
      "iter 36\ttrain cost\t0.20958055555820465\ttotal variance to retain\t0.0001087188720703125\n",
      "Epoch 36\n",
      "Iter 40700: loss: 0.2062\n",
      "\n",
      "[2.8867075e+01 1.2975328e+00 2.8677073e-01 1.4185405e-01 1.2870592e-01\n",
      " 7.8524612e-02 6.1049920e-02 5.1991086e-02 3.3672307e-02 3.0318748e-02\n",
      " 2.0693490e-02 1.9400530e-02 1.6911928e-02 1.4537987e-02 1.0834498e-02\n",
      " 8.1328470e-03 6.2843887e-03 3.8953086e-03 2.9106562e-03 2.7345829e-03\n",
      " 2.2305548e-03 1.8755690e-03 1.7012875e-03 1.5627447e-03 1.4009764e-03\n",
      " 1.0005521e-03 8.9694327e-04 5.1532348e-04 4.6647535e-04 2.6174940e-04]\n",
      "iter 37\ttrain cost\t0.2061789184808731\ttotal variance to retain\t0.00016188621520996094\n",
      "Epoch 37\n",
      "Iter 41800: loss: 0.2049\n",
      "\n",
      "[2.9224043e+01 1.2520843e+00 2.1275006e-01 1.4046538e-01 1.0478123e-01\n",
      " 7.5050749e-02 5.8403723e-02 4.4028204e-02 2.9479345e-02 2.2086900e-02\n",
      " 1.6670331e-02 1.4459562e-02 1.3692974e-02 1.0568642e-02 8.9204963e-03\n",
      " 8.0804098e-03 4.7063781e-03 3.6267757e-03 3.1337747e-03 2.5469728e-03\n",
      " 2.2605415e-03 2.0446940e-03 1.4970414e-03 1.3077658e-03 1.1008491e-03\n",
      " 9.2336128e-04 8.0123468e-04 5.6841300e-04 3.7073874e-04 2.3744188e-04]\n",
      "iter 38\ttrain cost\t0.2049395889043808\ttotal variance to retain\t0.00010436773300170898\n",
      "Epoch 38\n",
      "Iter 42900: loss: 0.2017\n",
      "\n",
      "[3.0672596e+01 1.3880454e+00 2.2325113e-01 1.2302880e-01 1.0439789e-01\n",
      " 8.3322480e-02 5.6468334e-02 4.4413116e-02 2.6795292e-02 2.3406038e-02\n",
      " 1.9790582e-02 1.8435556e-02 1.4834500e-02 1.1155747e-02 8.8651013e-03\n",
      " 7.5280685e-03 4.4947355e-03 3.4211699e-03 3.0887574e-03 2.5128876e-03\n",
      " 2.1067094e-03 1.6746101e-03 1.2772988e-03 1.2525620e-03 1.0905411e-03\n",
      " 9.4356458e-04 7.2036131e-04 5.0535635e-04 3.0677591e-04 2.0065955e-04]\n",
      "iter 39\ttrain cost\t0.20174801349639893\ttotal variance to retain\t9.60230827331543e-05\n",
      "Epoch 39\n",
      "Iter 44000: loss: 0.1971\n",
      "\n",
      "[3.2400547e+01 1.2307862e+00 1.5999065e-01 9.8323971e-02 8.0956861e-02\n",
      " 6.3871711e-02 5.3170204e-02 4.0568303e-02 3.2532554e-02 2.4769792e-02\n",
      " 1.6831111e-02 1.5501381e-02 1.1654097e-02 1.0933042e-02 1.0365331e-02\n",
      " 5.6987409e-03 4.4489899e-03 3.7364094e-03 2.8010148e-03 2.2696343e-03\n",
      " 2.0608215e-03 1.6549913e-03 1.5678736e-03 1.1940787e-03 1.0281258e-03\n",
      " 8.7096589e-04 7.7958457e-04 5.1684579e-04 3.7267830e-04 2.5037455e-04]\n",
      "iter 40\ttrain cost\t0.19714254140853882\ttotal variance to retain\t5.036592483520508e-05\n",
      "Epoch 40\n",
      "Iter 45100: loss: 0.1942\n",
      "\n",
      "[3.08310318e+01 1.43252969e+00 1.66251764e-01 1.19461194e-01\n",
      " 1.03660226e-01 7.89972916e-02 5.21614626e-02 4.07766700e-02\n",
      " 3.38686109e-02 1.91004779e-02 1.61495637e-02 1.35004222e-02\n",
      " 1.25907157e-02 1.22657046e-02 8.95158667e-03 7.57970801e-03\n",
      " 4.58408054e-03 3.79457162e-03 2.98671029e-03 2.58641946e-03\n",
      " 2.34453240e-03 1.86531642e-03 1.37878198e-03 1.22984394e-03\n",
      " 1.21178373e-03 7.92125647e-04 6.76162832e-04 5.98239247e-04\n",
      " 3.87880078e-04 1.94562323e-04]\n",
      "iter 41\ttrain cost\t0.19423000514507294\ttotal variance to retain\t6.902217864990234e-05\n",
      "Epoch 41\n",
      "Iter 46200: loss: 0.1906\n",
      "\n",
      "[3.2286243e+01 1.3501021e+00 1.8047726e-01 1.3893193e-01 8.1998877e-02\n",
      " 6.5171927e-02 5.0746974e-02 3.7299387e-02 2.6065743e-02 2.2101102e-02\n",
      " 1.9371016e-02 1.2599490e-02 1.0889799e-02 9.4781388e-03 7.3351329e-03\n",
      " 6.8780724e-03 4.9760607e-03 3.7391135e-03 2.8147772e-03 2.3338194e-03\n",
      " 1.8563549e-03 1.6418193e-03 1.3737053e-03 1.2086453e-03 9.4651000e-04\n",
      " 7.7952613e-04 6.4119144e-04 5.4033857e-04 3.8703851e-04 2.1296709e-04]\n",
      "iter 42\ttrain cost\t0.190582275390625\ttotal variance to retain\t6.592273712158203e-05\n",
      "Epoch 42\n",
      "Iter 47300: loss: 0.1911\n",
      "\n",
      "[3.3534008e+01 1.2800500e+00 1.3885051e-01 8.6678334e-02 7.6339565e-02\n",
      " 6.4260475e-02 5.9061572e-02 3.2094587e-02 2.5501933e-02 2.1973696e-02\n",
      " 1.8641556e-02 1.6042866e-02 1.3857466e-02 1.1701004e-02 8.3080521e-03\n",
      " 7.4605616e-03 5.2073044e-03 3.3779063e-03 2.3848878e-03 2.2531890e-03\n",
      " 2.1240958e-03 1.7426301e-03 1.4791498e-03 1.2630554e-03 1.0629017e-03\n",
      " 8.7529275e-04 8.2917209e-04 4.4545659e-04 3.4375736e-04 2.1555478e-04]\n",
      "iter 43\ttrain cost\t0.19113969802856445\ttotal variance to retain\t3.8564205169677734e-05\n",
      "Epoch 43\n",
      "Iter 48400: loss: 0.1863\n",
      "\n",
      "[3.4473026e+01 1.1867013e+00 1.7235927e-01 1.3774730e-01 9.5173180e-02\n",
      " 6.4150997e-02 4.9741834e-02 3.6288027e-02 3.2997720e-02 2.3091966e-02\n",
      " 2.0410778e-02 1.6615622e-02 1.5313747e-02 1.3240047e-02 8.5495310e-03\n",
      " 7.3760697e-03 5.1367702e-03 3.4690495e-03 2.7855677e-03 2.6286088e-03\n",
      " 2.1319916e-03 1.7645722e-03 1.4194812e-03 1.2366748e-03 1.0386249e-03\n",
      " 9.5242448e-04 6.5255980e-04 5.2405376e-04 3.5090194e-04 1.8325729e-04]\n",
      "iter 44\ttrain cost\t0.18630416691303253\ttotal variance to retain\t5.7637691497802734e-05\n",
      "Epoch 44\n",
      "Iter 49500: loss: 0.1843\n",
      "\n",
      "[3.4379665e+01 1.1490411e+00 1.7543973e-01 8.8410266e-02 7.9788655e-02\n",
      " 7.1888380e-02 5.2619886e-02 3.9382849e-02 2.9692203e-02 2.1505369e-02\n",
      " 1.6086016e-02 1.5092715e-02 1.3224559e-02 8.9289285e-03 7.9650003e-03\n",
      " 6.3696108e-03 3.9753411e-03 3.1536194e-03 3.0529869e-03 2.3254938e-03\n",
      " 1.9825730e-03 1.8525546e-03 1.4764018e-03 1.2536047e-03 1.0186958e-03\n",
      " 8.2669762e-04 7.2873890e-04 4.8867869e-04 3.6645276e-04 1.6613519e-04]\n",
      "iter 45\ttrain cost\t0.184278205037117\ttotal variance to retain\t4.786252975463867e-05\n",
      "Epoch 45\n",
      "Iter 50600: loss: 0.1832\n",
      "\n",
      "[3.4351131e+01 1.0757816e+00 1.8064210e-01 8.6411402e-02 7.9541072e-02\n",
      " 6.6080622e-02 4.4040803e-02 3.1758316e-02 3.1061701e-02 1.9864241e-02\n",
      " 1.8443199e-02 1.6959799e-02 1.2205132e-02 1.0128019e-02 8.5259015e-03\n",
      " 7.8158882e-03 4.7516194e-03 3.6544295e-03 2.9152134e-03 2.3942171e-03\n",
      " 2.0415790e-03 1.7540175e-03 1.5250877e-03 1.2448929e-03 1.0182684e-03\n",
      " 8.6517341e-04 6.5402116e-04 3.8698153e-04 3.2147556e-04 1.8359533e-04]\n",
      "iter 46\ttrain cost\t0.18316827714443207\ttotal variance to retain\t4.756450653076172e-05\n",
      "Epoch 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 51700: loss: 0.1799\n",
      "\n",
      "[3.6380028e+01 9.4725865e-01 1.4174657e-01 1.0870541e-01 8.2525142e-02\n",
      " 6.0762536e-02 4.5256417e-02 3.5160895e-02 2.2863442e-02 2.0098127e-02\n",
      " 1.5885154e-02 1.4542464e-02 1.1817957e-02 9.4860960e-03 7.8375293e-03\n",
      " 6.5548904e-03 3.3599490e-03 3.0364338e-03 2.9037965e-03 2.1542918e-03\n",
      " 1.7945357e-03 1.4972438e-03 1.4306555e-03 1.0671329e-03 9.0076640e-04\n",
      " 7.4122538e-04 6.2374433e-04 4.4852341e-04 3.5666034e-04 1.9199528e-04]\n",
      "iter 47\ttrain cost\t0.1798686385154724\ttotal variance to retain\t3.5762786865234375e-05\n",
      "Epoch 47\n",
      "Iter 52800: loss: 0.1803\n",
      "\n",
      "[3.5525795e+01 1.0285814e+00 1.5897010e-01 1.0940716e-01 8.3474033e-02\n",
      " 7.3492743e-02 4.0229555e-02 3.5518933e-02 3.2647956e-02 2.3075389e-02\n",
      " 1.6493235e-02 1.3856041e-02 1.2210917e-02 1.1022483e-02 8.9972494e-03\n",
      " 7.4972841e-03 3.9356607e-03 3.1356139e-03 2.6801995e-03 2.1090377e-03\n",
      " 1.7397894e-03 1.6689140e-03 1.4328785e-03 1.2231314e-03 1.1178391e-03\n",
      " 8.7635859e-04 6.9557037e-04 4.8436888e-04 3.2823850e-04 1.8549587e-04]\n",
      "iter 48\ttrain cost\t0.18032698333263397\ttotal variance to retain\t4.3511390686035156e-05\n",
      "Epoch 48\n",
      "Iter 53900: loss: 0.1774\n",
      "\n",
      "[3.6725891e+01 1.1972991e+00 1.5676995e-01 1.2392210e-01 7.2004691e-02\n",
      " 6.6924371e-02 4.3090709e-02 3.6243558e-02 2.6996825e-02 2.3912799e-02\n",
      " 1.9053843e-02 1.2505580e-02 1.1552079e-02 1.1015970e-02 8.9723989e-03\n",
      " 6.7089060e-03 5.9294789e-03 3.0444996e-03 2.3609374e-03 2.0008588e-03\n",
      " 1.9700069e-03 1.7450657e-03 1.4906101e-03 1.1322948e-03 9.9468720e-04\n",
      " 8.8485249e-04 7.4753340e-04 3.8197817e-04 3.6238669e-04 1.7730427e-04]\n",
      "iter 49\ttrain cost\t0.17737947404384613\ttotal variance to retain\t4.07099723815918e-05\n",
      "Epoch 49\n",
      "Iter 55000: loss: 0.1755\n",
      "\n",
      "[3.7551785e+01 1.1538137e+00 1.8478452e-01 9.4249852e-02 8.6737923e-02\n",
      " 6.2494706e-02 5.2521937e-02 3.1408451e-02 2.7437715e-02 2.2599103e-02\n",
      " 2.0452056e-02 1.8551562e-02 1.5098595e-02 1.0039915e-02 7.4565411e-03\n",
      " 6.2970892e-03 3.9020390e-03 3.0808675e-03 2.4058856e-03 2.1536904e-03\n",
      " 2.0109501e-03 1.6248573e-03 1.4802094e-03 1.1787717e-03 1.0385723e-03\n",
      " 7.0841750e-04 5.8476237e-04 3.4418594e-04 3.0937209e-04 1.8059686e-04]\n",
      "iter 50\ttrain cost\t0.17550106346607208\ttotal variance to retain\t4.297494888305664e-05\n",
      "Epoch 50\n",
      "Iter 56100: loss: 0.1719\n",
      "\n",
      "[3.6339138e+01 1.0952642e+00 1.5193188e-01 9.3028121e-02 7.1900867e-02\n",
      " 7.1523547e-02 4.8630305e-02 3.4063779e-02 2.8511314e-02 2.3020212e-02\n",
      " 1.5523207e-02 1.3448549e-02 1.2659223e-02 9.9684410e-03 7.9206405e-03\n",
      " 6.6047586e-03 4.2797597e-03 3.2457728e-03 2.8270870e-03 2.3269472e-03\n",
      " 2.0030758e-03 1.7245563e-03 1.4416672e-03 1.1688386e-03 9.0445159e-04\n",
      " 8.4487692e-04 7.0153276e-04 4.5746026e-04 3.5443850e-04 1.6804581e-04]\n",
      "iter 51\ttrain cost\t0.17192494869232178\ttotal variance to retain\t3.618001937866211e-05\n",
      "Epoch 51\n",
      "Iter 57200: loss: 0.1729\n",
      "\n",
      "[3.7898174e+01 1.0262170e+00 1.4237320e-01 1.1784632e-01 7.9808906e-02\n",
      " 6.9228232e-02 3.7790898e-02 3.4159113e-02 2.7203165e-02 1.9664790e-02\n",
      " 1.8389527e-02 1.6360259e-02 1.1679051e-02 8.4099015e-03 7.3775034e-03\n",
      " 5.7774405e-03 4.6109026e-03 3.5035429e-03 2.7512133e-03 2.3156789e-03\n",
      " 1.7413979e-03 1.3906959e-03 1.3420555e-03 1.1542849e-03 9.8860473e-04\n",
      " 7.9418957e-04 5.9444894e-04 3.2914215e-04 3.1741735e-04 1.8259150e-04]\n",
      "iter 52\ttrain cost\t0.17291676998138428\ttotal variance to retain\t3.4749507904052734e-05\n",
      "Epoch 52\n",
      "Iter 58300: loss: 0.1718\n",
      "\n",
      "[3.8274857e+01 1.1436038e+00 1.6571772e-01 1.0812076e-01 9.1915205e-02\n",
      " 6.2042486e-02 5.0602615e-02 3.0365322e-02 2.3318583e-02 1.9955542e-02\n",
      " 1.7347630e-02 1.5185984e-02 1.2214556e-02 9.9495659e-03 8.6690057e-03\n",
      " 6.6196676e-03 4.6807663e-03 3.3595453e-03 2.3181159e-03 2.2313381e-03\n",
      " 1.8143890e-03 1.7587997e-03 1.6248379e-03 1.4456236e-03 9.8806538e-04\n",
      " 8.6891220e-04 7.1234576e-04 3.9715759e-04 3.2931642e-04 1.6773447e-04]\n",
      "iter 53\ttrain cost\t0.17178462445735931\ttotal variance to retain\t3.8623809814453125e-05\n",
      "Epoch 53\n",
      "Iter 59400: loss: 0.1690\n",
      "\n",
      "[3.95559311e+01 9.96734321e-01 1.15590289e-01 9.26200151e-02\n",
      " 8.03097114e-02 4.41020839e-02 3.51320356e-02 2.60434914e-02\n",
      " 2.14712955e-02 1.74421091e-02 1.61464140e-02 1.31198075e-02\n",
      " 9.80824791e-03 8.75250716e-03 7.42543302e-03 5.37251960e-03\n",
      " 3.61597491e-03 2.84650293e-03 2.65258481e-03 2.07808753e-03\n",
      " 1.84249121e-03 1.61630067e-03 1.26327993e-03 1.05116179e-03\n",
      " 8.47154181e-04 6.17076876e-04 6.07634836e-04 3.37797974e-04\n",
      " 2.89385120e-04 1.56133465e-04]\n",
      "iter 54\ttrain cost\t0.16898077726364136\ttotal variance to retain\t2.1517276763916016e-05\n",
      "Epoch 54\n",
      "Iter 60500: loss: 0.1672\n",
      "\n",
      "[3.9910347e+01 8.7294924e-01 1.2780187e-01 7.2534591e-02 6.6949576e-02\n",
      " 5.2053645e-02 4.1714154e-02 3.1978466e-02 2.4941003e-02 2.2294411e-02\n",
      " 1.7657520e-02 1.3844986e-02 1.2464683e-02 1.1787230e-02 9.4764736e-03\n",
      " 7.4155387e-03 5.6332694e-03 3.4403538e-03 2.8351392e-03 2.1319604e-03\n",
      " 1.7138573e-03 1.6407107e-03 1.1837267e-03 1.1221436e-03 1.0283226e-03\n",
      " 7.8451453e-04 6.2938174e-04 3.3183239e-04 3.0820313e-04 1.6069737e-04]\n",
      "iter 55\ttrain cost\t0.16720423102378845\ttotal variance to retain\t2.104043960571289e-05\n",
      "Epoch 55\n",
      "Iter 61600: loss: 0.1663\n",
      "\n",
      "[4.0463764e+01 9.5091724e-01 2.0193352e-01 9.1425829e-02 7.5632781e-02\n",
      " 6.2456101e-02 4.1025434e-02 3.1579927e-02 2.6418507e-02 1.8837288e-02\n",
      " 1.8724151e-02 1.4966464e-02 1.2864208e-02 1.0293691e-02 8.5797245e-03\n",
      " 7.7630840e-03 4.7151884e-03 4.4807894e-03 2.4931114e-03 1.9907237e-03\n",
      " 1.8739796e-03 1.7523927e-03 1.3366056e-03 1.1211626e-03 1.0320521e-03\n",
      " 8.4779190e-04 7.3280459e-04 3.6147569e-04 3.1719980e-04 2.2187721e-04]\n",
      "iter 56\ttrain cost\t0.16629377007484436\ttotal variance to retain\t3.8743019104003906e-05\n",
      "Epoch 56\n",
      "Iter 62700: loss: 0.1638\n",
      "\n",
      "[4.0533794e+01 8.3077061e-01 1.4486501e-01 1.0629268e-01 9.7115681e-02\n",
      " 7.0235610e-02 4.4333383e-02 3.4919877e-02 2.3202674e-02 2.2927811e-02\n",
      " 1.5970243e-02 1.4281592e-02 1.1821902e-02 9.6209291e-03 6.8724756e-03\n",
      " 5.5466779e-03 4.2281062e-03 2.9339485e-03 2.3840030e-03 2.0170857e-03\n",
      " 1.5898667e-03 1.4493875e-03 1.2777315e-03 1.0121850e-03 8.3256396e-04\n",
      " 7.0745824e-04 5.9705036e-04 4.4440848e-04 3.8164269e-04 1.7644565e-04]\n",
      "iter 57\ttrain cost\t0.1638357788324356\ttotal variance to retain\t3.135204315185547e-05\n",
      "Epoch 57\n",
      "Iter 63800: loss: 0.1618\n",
      "\n",
      "[4.1063831e+01 1.0851873e+00 1.8128628e-01 9.9332348e-02 7.4137606e-02\n",
      " 5.1621705e-02 4.4132680e-02 4.1918244e-02 2.8481293e-02 2.0568937e-02\n",
      " 1.4394360e-02 1.3210952e-02 1.1286897e-02 9.4038136e-03 8.3306339e-03\n",
      " 6.9028027e-03 4.1363928e-03 3.1897202e-03 2.3978292e-03 2.1351774e-03\n",
      " 1.9024661e-03 1.6535346e-03 1.5215537e-03 1.1907349e-03 9.5788972e-04\n",
      " 8.7450742e-04 6.9266342e-04 4.2272781e-04 3.1676862e-04 1.6511700e-04]\n",
      "iter 58\ttrain cost\t0.16180916130542755\ttotal variance to retain\t3.355741500854492e-05\n",
      "Epoch 58\n",
      "Iter 64900: loss: 0.1610\n",
      "\n",
      "[4.1412788e+01 9.2818969e-01 1.3215916e-01 8.4017739e-02 5.7815775e-02\n",
      " 5.5017333e-02 3.5248481e-02 2.5119936e-02 1.9052120e-02 1.8429751e-02\n",
      " 1.4367540e-02 1.3272989e-02 1.2245466e-02 9.6866330e-03 8.7574990e-03\n",
      " 6.4212233e-03 3.9736582e-03 2.5870416e-03 2.3666793e-03 2.0182147e-03\n",
      " 1.7356105e-03 1.5961170e-03 1.4740148e-03 1.1876484e-03 1.1118300e-03\n",
      " 6.3876587e-04 5.9289800e-04 3.7900460e-04 2.5945090e-04 1.6268241e-04]\n",
      "iter 59\ttrain cost\t0.16102220118045807\ttotal variance to retain\t1.996755599975586e-05\n",
      "Epoch 59\n",
      "Iter 66000: loss: 0.1601\n",
      "\n",
      "[4.2304169e+01 9.8597038e-01 1.4479563e-01 9.5725469e-02 7.0964880e-02\n",
      " 4.1037224e-02 3.5856612e-02 2.9574756e-02 2.2864988e-02 1.8727778e-02\n",
      " 1.5764195e-02 1.3353181e-02 1.1296608e-02 9.6314587e-03 7.6110098e-03\n",
      " 5.8393115e-03 4.1872500e-03 3.4945586e-03 2.4779781e-03 2.0701997e-03\n",
      " 1.6740292e-03 1.5078787e-03 1.3221940e-03 1.0233588e-03 9.3840127e-04\n",
      " 8.7194738e-04 7.3788012e-04 3.5851393e-04 3.3152671e-04 1.4707739e-04]\n",
      "iter 60\ttrain cost\t0.16012044250965118\ttotal variance to retain\t2.2649765014648438e-05\n",
      "Epoch 60\n",
      "Iter 67100: loss: 0.1597\n",
      "\n",
      "[4.3173779e+01 8.6303365e-01 1.2336871e-01 9.2448317e-02 7.2147250e-02\n",
      " 5.3484302e-02 3.6086842e-02 2.7728727e-02 2.5005534e-02 2.1451056e-02\n",
      " 1.4667680e-02 1.3451539e-02 1.1302102e-02 9.3873180e-03 7.2550084e-03\n",
      " 5.8290577e-03 3.6500171e-03 3.1925838e-03 2.8923687e-03 2.2836637e-03\n",
      " 2.0499870e-03 1.7565083e-03 1.6696550e-03 1.2044300e-03 8.8121812e-04\n",
      " 8.3023979e-04 6.2504876e-04 3.4558072e-04 2.6265453e-04 1.4684435e-04]\n",
      "iter 61\ttrain cost\t0.159677654504776\ttotal variance to retain\t1.913309097290039e-05\n",
      "Epoch 61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 68200: loss: 0.1570\n",
      "\n",
      "[4.30333710e+01 1.01517022e+00 1.50086030e-01 1.02360554e-01\n",
      " 7.42001161e-02 5.13925180e-02 3.44857983e-02 2.74380296e-02\n",
      " 2.50838585e-02 2.03341451e-02 1.73846539e-02 1.49217974e-02\n",
      " 1.41415512e-02 1.00853397e-02 8.49597529e-03 5.60782198e-03\n",
      " 3.61949229e-03 3.42564750e-03 2.93249195e-03 2.04869010e-03\n",
      " 1.59181282e-03 1.53568340e-03 1.20328611e-03 1.00213988e-03\n",
      " 9.52226867e-04 7.77474896e-04 5.58893371e-04 3.87855922e-04\n",
      " 3.05666472e-04 1.70933796e-04]\n",
      "iter 62\ttrain cost\t0.1570308357477188\ttotal variance to retain\t2.4378299713134766e-05\n",
      "Epoch 62\n",
      "Iter 69300: loss: 0.1556\n",
      "\n",
      "[4.2998375e+01 9.6671855e-01 1.3238408e-01 9.4731160e-02 7.0484780e-02\n",
      " 5.1368084e-02 3.9713912e-02 2.9944265e-02 2.7087940e-02 1.5193489e-02\n",
      " 1.3375401e-02 1.1537392e-02 1.0450213e-02 9.5267510e-03 8.5495310e-03\n",
      " 6.2015946e-03 3.7980885e-03 3.2665508e-03 2.3096793e-03 1.7325676e-03\n",
      " 1.6479507e-03 1.4268877e-03 1.3048325e-03 1.1735631e-03 9.2266256e-04\n",
      " 7.2597939e-04 6.4243202e-04 3.5881271e-04 3.3569764e-04 1.5157390e-04]\n",
      "iter 63\ttrain cost\t0.15559904277324677\ttotal variance to retain\t2.0682811737060547e-05\n",
      "Epoch 63\n",
      "Iter 70400: loss: 0.1545\n",
      "\n",
      "[4.54471054e+01 9.10782337e-01 1.45185396e-01 9.62095186e-02\n",
      " 7.71480128e-02 4.89582270e-02 4.41519171e-02 3.40914875e-02\n",
      " 2.79578548e-02 2.05977410e-02 1.58394519e-02 1.46869607e-02\n",
      " 1.28864655e-02 1.02834580e-02 8.08418915e-03 6.67526526e-03\n",
      " 3.93808633e-03 3.32288002e-03 3.06593953e-03 2.06289487e-03\n",
      " 1.82236906e-03 1.42821809e-03 1.27216929e-03 1.17402431e-03\n",
      " 9.75789153e-04 8.35066836e-04 5.16125700e-04 3.47024790e-04\n",
      " 2.80455512e-04 1.53397326e-04]\n",
      "iter 64\ttrain cost\t0.15446335077285767\ttotal variance to retain\t2.1278858184814453e-05\n",
      "Epoch 64\n",
      "Iter 71500: loss: 0.1535\n",
      "\n",
      "[4.55364380e+01 8.25007677e-01 1.50731757e-01 9.30536836e-02\n",
      " 8.24350566e-02 5.09137623e-02 4.49207500e-02 3.72552425e-02\n",
      " 3.52863446e-02 2.25799028e-02 1.83467958e-02 1.45680699e-02\n",
      " 1.29754841e-02 1.20177586e-02 8.92509613e-03 7.61911459e-03\n",
      " 4.32467181e-03 3.48962541e-03 3.18507128e-03 2.12091603e-03\n",
      " 1.90568552e-03 1.68832671e-03 1.33796781e-03 1.04669260e-03\n",
      " 8.52116616e-04 7.38498115e-04 5.06322715e-04 3.77021526e-04\n",
      " 2.76499166e-04 1.20248114e-04]\n",
      "iter 65\ttrain cost\t0.15351545810699463\ttotal variance to retain\t2.2590160369873047e-05\n",
      "Epoch 65\n",
      "Iter 72600: loss: 0.1540\n",
      "\n",
      "[4.4722912e+01 9.2394400e-01 1.1049917e-01 8.5665405e-02 8.3764449e-02\n",
      " 4.7756538e-02 3.6698084e-02 3.0285990e-02 2.7372774e-02 1.6571848e-02\n",
      " 1.4503761e-02 1.3205091e-02 1.0770971e-02 9.9094519e-03 6.6809263e-03\n",
      " 5.5685830e-03 3.8128749e-03 2.7258268e-03 2.4595675e-03 2.0114435e-03\n",
      " 1.7217261e-03 1.3907263e-03 1.3204757e-03 1.2491219e-03 8.7052869e-04\n",
      " 6.0865638e-04 5.0354982e-04 3.9382439e-04 3.1553302e-04 1.3442381e-04]\n",
      "iter 66\ttrain cost\t0.15395095944404602\ttotal variance to retain\t1.6391277313232422e-05\n",
      "Epoch 66\n",
      "Iter 73700: loss: 0.1531\n",
      "\n",
      "[4.5144249e+01 8.5280901e-01 1.6918308e-01 9.1452658e-02 8.1517793e-02\n",
      " 6.0436450e-02 3.8447279e-02 3.0087808e-02 2.3813386e-02 2.0949148e-02\n",
      " 1.5787274e-02 1.5501380e-02 1.1174054e-02 1.0382990e-02 8.5159792e-03\n",
      " 5.8765518e-03 3.7531436e-03 2.9525431e-03 1.9962706e-03 1.9348328e-03\n",
      " 1.5233476e-03 1.3091376e-03 1.0712276e-03 9.5329323e-04 8.2551700e-04\n",
      " 6.9516082e-04 5.1201129e-04 3.5506039e-04 3.3463334e-04 1.4904074e-04]\n",
      "iter 67\ttrain cost\t0.1530645489692688\ttotal variance to retain\t2.5331974029541016e-05\n",
      "Epoch 67\n",
      "Iter 74800: loss: 0.1513\n",
      "\n",
      "[4.56465492e+01 8.00291896e-01 1.17634706e-01 7.06457049e-02\n",
      " 5.68852723e-02 5.18487915e-02 3.79535593e-02 2.79575605e-02\n",
      " 2.28362996e-02 2.02911980e-02 1.51827149e-02 1.42912725e-02\n",
      " 1.25404093e-02 1.10447546e-02 8.00403301e-03 5.90159884e-03\n",
      " 3.89130320e-03 2.73124222e-03 2.42268224e-03 2.10135570e-03\n",
      " 1.49766123e-03 1.37399102e-03 1.22804509e-03 1.19586254e-03\n",
      " 9.59236757e-04 5.99551713e-04 5.83003683e-04 3.31701129e-04\n",
      " 2.87189585e-04 1.13219932e-04]\n",
      "iter 68\ttrain cost\t0.15125441551208496\ttotal variance to retain\t1.3828277587890625e-05\n",
      "Epoch 68\n",
      "Iter 75900: loss: 0.1503\n",
      "\n",
      "[4.55326958e+01 9.04806554e-01 1.15867004e-01 1.10936232e-01\n",
      " 8.67844000e-02 5.23603819e-02 4.53158729e-02 3.35480198e-02\n",
      " 2.89607197e-02 2.56176256e-02 1.86977088e-02 1.73327290e-02\n",
      " 1.28763039e-02 1.04855541e-02 9.01485328e-03 6.29923074e-03\n",
      " 3.68246296e-03 2.70492304e-03 2.42462102e-03 2.23029568e-03\n",
      " 1.78420579e-03 1.60967605e-03 1.30728504e-03 1.00642408e-03\n",
      " 8.18370143e-04 6.38271740e-04 5.12563158e-04 3.33835254e-04\n",
      " 2.30488935e-04 1.40994933e-04]\n",
      "iter 69\ttrain cost\t0.15033796429634094\ttotal variance to retain\t2.014636993408203e-05\n",
      "Epoch 69\n",
      "Iter 77000: loss: 0.1475\n",
      "\n",
      "[4.84162941e+01 6.82099938e-01 1.15550645e-01 8.40561688e-02\n",
      " 6.85713664e-02 5.19823655e-02 4.06510420e-02 3.23738940e-02\n",
      " 2.41349656e-02 1.61818657e-02 1.59670617e-02 1.55429235e-02\n",
      " 1.19196419e-02 1.02083012e-02 7.79376412e-03 6.77911844e-03\n",
      " 3.59045831e-03 3.29311588e-03 2.47702166e-03 2.28918553e-03\n",
      " 1.93621987e-03 1.46361650e-03 1.20170636e-03 1.03446981e-03\n",
      " 6.77764707e-04 6.12304080e-04 4.40082280e-04 2.76782608e-04\n",
      " 2.47232179e-04 1.31525958e-04]\n",
      "iter 70\ttrain cost\t0.14747707545757294\ttotal variance to retain\t1.3768672943115234e-05\n",
      "Epoch 70\n",
      "Iter 78100: loss: 0.1483\n",
      "\n",
      "[4.76478271e+01 8.47610295e-01 1.14424415e-01 8.15376490e-02\n",
      " 5.83381392e-02 4.83960398e-02 2.70967241e-02 2.22570878e-02\n",
      " 1.98798627e-02 1.76653937e-02 1.47266109e-02 1.37824342e-02\n",
      " 1.17070703e-02 9.40863509e-03 8.80311057e-03 6.13096496e-03\n",
      " 3.58242448e-03 3.11592431e-03 2.72401003e-03 1.99729670e-03\n",
      " 1.73216814e-03 1.50439888e-03 1.36490690e-03 1.00599590e-03\n",
      " 8.54072743e-04 6.76783849e-04 6.62140083e-04 3.59658472e-04\n",
      " 3.43211839e-04 1.18524324e-04]\n",
      "iter 71\ttrain cost\t0.14825165271759033\ttotal variance to retain\t1.245737075805664e-05\n",
      "Epoch 71\n",
      "Iter 79200: loss: 0.1463\n",
      "\n",
      "[4.9088501e+01 7.8345245e-01 1.0614834e-01 6.3272387e-02 5.5668376e-02\n",
      " 4.7816038e-02 3.5512853e-02 3.0472813e-02 2.0570926e-02 1.6578112e-02\n",
      " 1.3992629e-02 1.2321837e-02 1.1833308e-02 9.5626609e-03 6.2398496e-03\n",
      " 5.3949030e-03 3.8889197e-03 3.1324988e-03 2.6112339e-03 2.0616741e-03\n",
      " 2.0159972e-03 1.6797147e-03 1.3841398e-03 1.1672544e-03 9.3332771e-04\n",
      " 7.4006512e-04 6.3736702e-04 3.7252539e-04 2.9116159e-04 1.1333726e-04]\n",
      "iter 72\ttrain cost\t0.14631317555904388\ttotal variance to retain\t1.0013580322265625e-05\n",
      "Epoch 72\n",
      "Iter 80300: loss: 0.1463\n",
      "\n",
      "[5.0260319e+01 7.0930684e-01 8.2948089e-02 5.2055910e-02 4.8758738e-02\n",
      " 4.2779315e-02 2.6159277e-02 2.3320654e-02 2.0228377e-02 1.5348012e-02\n",
      " 1.4199599e-02 1.1818692e-02 1.0866156e-02 7.9399785e-03 6.7617553e-03\n",
      " 4.6045207e-03 3.8293228e-03 3.1244394e-03 2.5850176e-03 2.0568529e-03\n",
      " 1.5695774e-03 1.3826409e-03 1.2585212e-03 9.3547249e-04 7.6195574e-04\n",
      " 6.8029639e-04 6.2204018e-04 2.8263972e-04 2.5672169e-04 1.3205968e-04]\n",
      "iter 73\ttrain cost\t0.1463496834039688\ttotal variance to retain\t6.496906280517578e-06\n",
      "Epoch 73\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "scores3 = []\n",
    "for part in range(8):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((_train_x[0:(part*65)], _train_x[(part+1)*65:569]), axis=0)\n",
    "    train_y = np.concatenate((_train_y[0:(part*65)], _train_y[(part+1)*65:569]), axis=0)\n",
    "    test_x = _train_x[part*65:(part+1)*65]\n",
    "    test_y = _train_y[part*65:(part+1)*65]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    transpose_train_x = np.transpose(train_x)\n",
    "\n",
    "    cur_w = np.random.normal(0, 0.35, (number_of_neurons,1))\n",
    "    cur_B =  np.random.normal(0, 0.35, (dimensionality, number_of_neurons))\n",
    "    cur_biases = np.zeros((number_of_neurons))\n",
    "    cur_P = np.zeros((dimensionality, dimensionality)) \n",
    "    cur_w_regr = np.random.normal(0, 0.35, (number_of_neurons_regr,1))\n",
    "    cur_B_regr = np.random.normal(0, 0.35, (dimensionality, number_of_neurons_regr))\n",
    "    cur_bias_regr = np.random.normal(0, 0.35, (number_of_neurons_regr))\n",
    "    \n",
    "    O = np.zeros((dimensionality, k)) \n",
    "    cur_iter = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init, feed_dict={Dataplace: transpose_train_x})\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" %(epoch))\n",
    "        for iteration in range(num_iters):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            sess.run(target, feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x,\n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "        reses = []\n",
    "        for i in range(100):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            res = sess.run([loss_hybrid], feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x, \n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "            reses.append(res)\n",
    "        print (\"Iter %d: loss: %.4f\\n\" %(cur_iter, np.mean(np.array(reses))))\n",
    "        lib.plot.plot('train cost', np.mean(np.array(reses)))\n",
    "\n",
    "        cur_w, cur_B, cur_w_regr, cur_B_regr, cur_bias_regr = sess.run([w, B, w_regr, B_regr, bias_regr])\n",
    "\n",
    "        third_grad_psi = np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))\n",
    "        third_grad_psi_regr = np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))\n",
    "        for r in range(1000):\n",
    "            sess.run([tf_data_second])\n",
    "            np.concatenate((third_grad_psi, np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))), axis=0) \n",
    "            np.concatenate((third_grad_psi_regr, np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))), axis=0)\n",
    "\n",
    "        M = (1-rho)*np.matmul(np.transpose(third_grad_psi), third_grad_psi)+\\\n",
    "            rho*np.matmul(np.transpose(third_grad_psi_regr), third_grad_psi_regr)\n",
    "        u, s, vh = np.linalg.svd(M, full_matrices=True)\n",
    "        O = u[:,0:k:1]\n",
    "        print(s)\n",
    "        cur_P = np.matmul(O, np.transpose(O))\n",
    "        tvr = 1-np.sum(np.multiply(s[0:k],s[0:k]))/np.sum(np.multiply(s,s))\n",
    "        lib.plot.plot('total variance to retain', tvr)\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "    reduced_train_x = np.matmul(train_x, O)\n",
    "    reduced_test_x = np.matmul(test_x, O)\n",
    "    clf = LogisticRegression(random_state=0).fit(reduced_train_x, train_y)\n",
    "    score = clf.score(reduced_test_x, test_y)\n",
    "    scores.append(score)\n",
    "    print (\"Part %d: rate on test %.4f\\n\" %(part, score))\n",
    "    test_rgb = []\n",
    "    for h in test_y:\n",
    "        if h == 1:\n",
    "            test_rgb.append('r')\n",
    "        else:\n",
    "            test_rgb.append('b')\n",
    "    plt2.close()\n",
    "    plt2.scatter(np.transpose(reduced_test_x)[0], np.transpose(reduced_test_x)[1], alpha=0.2, c=np.array(test_rgb))\n",
    "    plt2.xlabel(\"1st component\")\n",
    "    plt2.ylabel(\"2nd component\")\n",
    "    plt2.show()\n",
    "    sess.close()\n",
    "    print(cur_P)\n",
    "\n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict(10, reduced_train_x, train_y, test_image)\n",
    "        if pred == test_y[i]:\n",
    "            total_correct += 1\n",
    "        score3 = (total_correct / (i+1)) * 100            \n",
    "        i += 1\n",
    "    print('acc:', str(round(score3, 2))+'%')\n",
    "    scores3.append(score3)\n",
    "\n",
    "print (\"Average rate on test %.4f\\n\" %(np.mean(np.array(scores))))\n",
    "print (\"Average 10-NN acc on test %.4f\\n\" %(np.mean(np.array(scores3))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
