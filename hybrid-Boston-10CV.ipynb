{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "F1Y_8jYdjORy",
    "outputId": "36ff5c4b-0fc7-4bb1-e76f-f901264c8989"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import scipy as scipy\n",
    "import scipy.interpolate as interpolate\n",
    "import math\n",
    "from random import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_boston\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt2\n",
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "Lambda = 10.0\n",
    "BATCH_SIZE = 38\n",
    "BATCH_SIZE_2 = 100\n",
    "dimensionality = 13\n",
    "num_epochs = 80\n",
    "num_iters = 1000\n",
    "k=2\n",
    "max_grad_norm = 1000\n",
    "gamma = 1.0\n",
    "\n",
    "# main parameters\n",
    "\n",
    "C=10.0\n",
    "\n",
    "# additional parameters\n",
    "sigma=0.8\n",
    "\n",
    "number_of_neurons_regr = 200\n",
    "number_of_points = 506-50\n",
    "number_of_neurons = number_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5ExlL7s3o9S6",
    "outputId": "82a79e11-a546-4fa7-81a1-de76f5b0011d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "(506, 13)\n",
      "(506,)\n",
      "50.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston(return_X_y=False)\n",
    "print(np.size(np.array(boston['target'])))\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(506)\n",
    "for x in rr:\n",
    "    data.append(boston['data'][x])\n",
    "    target.append(boston['target'][x])\n",
    "# training data\n",
    "_train_x = np.array(data)\n",
    "_train_y = np.array(target)\n",
    "print(_train_x.shape)\n",
    "print(_train_y.shape)\n",
    "print(np.max(_train_y))\n",
    "rho = 0.5\n",
    "var = np.var(_train_y)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvKNXjixjm-A"
   },
   "outputs": [],
   "source": [
    "new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "Dataplace = tf.placeholder(tf.float32, shape=(dimensionality, number_of_points))\n",
    "Data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, dimensionality))\n",
    "outputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE,1)) \n",
    "\n",
    "tf_data_x = gamma*tf.random_normal([BATCH_SIZE, dimensionality]) # аргументы функции\n",
    "tf_data_y = tf.reduce_mean(tf.math.cos(tf.matmul(tf_data_x, Dataplace)), axis=1) # значения функции\n",
    "\n",
    "tf_data_w = tf.placeholder(tf.float32, shape=(number_of_neurons,1))\n",
    "tf_data_B = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons))\n",
    "tf_data_P = tf.placeholder(tf.float32, shape=(dimensionality, dimensionality))\n",
    "\n",
    "tf_w_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr,1))\n",
    "tf_B_regr = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons_regr))\n",
    "tf_bias_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr))\n",
    "\n",
    "\n",
    "tf_data_second = gamma*tf.random_normal([BATCH_SIZE_2, dimensionality])\n",
    "Lbd = tf.placeholder(tf.float32, shape=[], name=\"lambda\")\n",
    "\n",
    "# characteristic function parameters\n",
    "w = tf.Variable(tf.random_normal([number_of_neurons,1], stddev=0.0), name=\"neuron_weights\")\n",
    "B = tf.Variable(initial_value=Dataplace, name=\"weights\")\n",
    "# regression function parameters\n",
    "w_regr = tf.Variable(tf.random_normal([number_of_neurons_regr,1], stddev=0.35), name=\"neuron_weights\")\n",
    "B_regr = tf.Variable(tf.random_normal([dimensionality,number_of_neurons_regr], stddev=0.35), name=\"weights\")\n",
    "bias_regr = tf.Variable(tf.random_normal([number_of_neurons_regr], stddev=0.0), name=\"biases\")\n",
    "\n",
    "prediction = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_x, B)), tf.nn.sigmoid(w))\n",
    "penalty = tf.square((2/number_of_neurons)*tf.reduce_sum(tf.nn.sigmoid(w))-1.0)\n",
    "\n",
    "out_loss = (1-rho)*tf.reduce_mean(tf.square(prediction - tf_data_y)) + C*penalty\n",
    "\n",
    "prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, B)), tf.nn.sigmoid(w))\n",
    "prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, B_regr), bias_regr)), w_regr)\n",
    "\n",
    "grad_psi = tf.reshape(tf.gradients(prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "grad_psi_regr = tf.reshape(tf.gradients(prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "tf_prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, tf_data_B)), tf.nn.sigmoid(tf_data_w))\n",
    "tf_prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, tf_B_regr), tf_bias_regr)), tf_w_regr)\n",
    "\n",
    "tf_data_grad_psi = tf.reshape(tf.gradients(tf_prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "tf_data_grad_psi_regr = tf.reshape(tf.gradients(tf_prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "old_part = tf.matmul(tf_data_grad_psi, tf_data_P)\n",
    "old_part_regr = tf.matmul(tf_data_grad_psi_regr, tf_data_P)\n",
    "\n",
    "loss = out_loss + Lbd*(1-rho)*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi, old_part)), axis=1)) + \\\n",
    "       Lbd*rho*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi_regr, old_part_regr)), axis=1))/var\n",
    "\n",
    "#offset = tf.random.uniform(shape=[], minval=0, maxval=12, dtype=tf.int32)*BATCH_SIZE\n",
    "x_plus_error = Data+sigma*tf.random_normal([BATCH_SIZE, dimensionality])\n",
    "regression = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(x_plus_error, B_regr), bias_regr)), w_regr)\n",
    "sdr = tf.reduce_mean(tf.square(regression-outputs))\n",
    "\n",
    "loss_hybrid = loss + rho*sdr/var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3HlgJ8XdjtxY",
    "outputId": "8ec5f3f0-92f7-4712-94c2-e9bff821b918"
   },
   "outputs": [],
   "source": [
    "target = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(loss_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YojZRg6lMgur"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1. /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pnt1, pnt2):\n",
    "    '''Finds the distance between 2 points: pnt1, pnt2'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((pnt1 - pnt2) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "\n",
    "def new_predict(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "def new_predict_regr(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return np.mean(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HLqISRILj5VQ",
    "outputId": "83373660-15c5-4d88-a2da-ec9bcf3a5a56",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current part is 0\n",
      "\n",
      "Epoch 0\n",
      "Iter 1100: loss: 3.3171\n",
      "\n",
      "iter 1\ttrain cost\t3.31705904006958\n",
      "[2.7557516  0.69724077 0.52576077 0.43784127 0.35854518 0.2871949\n",
      " 0.26906607 0.20652054 0.1803683  0.1570237  0.1448512  0.12224548\n",
      " 0.10088903]\n",
      "iter 2\ttotal variance to retain\t0.09996926784515381\n",
      "Epoch 1\n",
      "Iter 2200: loss: 2.1292\n",
      "\n",
      "iter 3\ttrain cost\t2.129249095916748\n",
      "[3.3697047  0.9652594  0.5595607  0.4557453  0.36512104 0.3126747\n",
      " 0.2946108  0.26110467 0.21535309 0.20989917 0.1790815  0.17342284\n",
      " 0.13528855]\n",
      "iter 4\ttotal variance to retain\t0.08064311742782593\n",
      "Epoch 2\n",
      "Iter 3300: loss: 1.4457\n",
      "\n",
      "iter 5\ttrain cost\t1.4456514120101929\n",
      "[4.4087386  1.1133327  0.75928223 0.70713055 0.5890588  0.5281674\n",
      " 0.47656417 0.40317497 0.32581523 0.30558154 0.27084985 0.23714015\n",
      " 0.1916863 ]\n",
      "iter 6\ttotal variance to retain\t0.10624957084655762\n",
      "Epoch 3\n",
      "Iter 4400: loss: 1.0705\n",
      "\n",
      "iter 7\ttrain cost\t1.0705299377441406\n",
      "[4.849444   1.5880339  0.74230045 0.711669   0.63920057 0.59780866\n",
      " 0.5165392  0.4705295  0.3991978  0.35636115 0.3242391  0.28474897\n",
      " 0.2688862 ]\n",
      "iter 8\ttotal variance to retain\t0.09885674715042114\n",
      "Epoch 4\n",
      "Iter 5500: loss: 0.8214\n",
      "\n",
      "iter 9\ttrain cost\t0.8214209079742432\n",
      "[5.4768724  1.7570693  0.6991523  0.57918197 0.5137604  0.45600268\n",
      " 0.39886472 0.37042543 0.34010637 0.29694298 0.26364908 0.21909142\n",
      " 0.21496746]\n",
      "iter 10\ttotal variance to retain\t0.05593150854110718\n",
      "Epoch 5\n",
      "Iter 6600: loss: 0.6239\n",
      "\n",
      "iter 11\ttrain cost\t0.6239142417907715\n",
      "[5.737901   1.9076699  0.44582462 0.3554082  0.2980729  0.2712307\n",
      " 0.24929304 0.21506287 0.2106251  0.19151065 0.17785206 0.16037033\n",
      " 0.14119479]\n",
      "iter 12\ttotal variance to retain\t0.020210862159729004\n",
      "Epoch 6\n",
      "Iter 7700: loss: 0.4859\n",
      "\n",
      "iter 13\ttrain cost\t0.48587894439697266\n",
      "[4.3366313  1.7865942  0.28343695 0.22401735 0.1801297  0.15774304\n",
      " 0.1343691  0.12864996 0.1201013  0.11316916 0.10065577 0.08202187\n",
      " 0.07551067]\n",
      "iter 14\ttotal variance to retain\t0.012224555015563965\n",
      "Epoch 7\n",
      "Iter 8800: loss: 0.4099\n",
      "\n",
      "iter 15\ttrain cost\t0.40993985533714294\n",
      "[5.5449023  1.9670345  0.26788583 0.1280512  0.11572403 0.10249531\n",
      " 0.09604553 0.09220373 0.07509805 0.07094349 0.05991079 0.05616039\n",
      " 0.04817406]\n",
      "iter 16\ttotal variance to retain\t0.004300832748413086\n",
      "Epoch 8\n",
      "Iter 9900: loss: 0.3790\n",
      "\n",
      "iter 17\ttrain cost\t0.37898319959640503\n",
      "[4.772412   1.738787   0.2091681  0.11621129 0.10109363 0.08541022\n",
      " 0.08035707 0.06535169 0.05453582 0.04826501 0.04475587 0.03258349\n",
      " 0.03155886]\n",
      "iter 18\ttotal variance to retain\t0.003663480281829834\n",
      "Epoch 9\n",
      "Iter 11000: loss: 0.3465\n",
      "\n",
      "iter 19\ttrain cost\t0.34647834300994873\n",
      "[4.9474354  1.9312507  0.24836063 0.0993086  0.09562221 0.07043553\n",
      " 0.06130743 0.05109349 0.04038889 0.03868818 0.03027485 0.02929152\n",
      " 0.02333344]\n",
      "iter 20\ttotal variance to retain\t0.003443479537963867\n",
      "Epoch 10\n",
      "Iter 12100: loss: 0.3342\n",
      "\n",
      "iter 21\ttrain cost\t0.3342052102088928\n",
      "[5.5216026  1.8941765  0.26365477 0.1241195  0.07595579 0.06851491\n",
      " 0.06589976 0.05019011 0.04245038 0.03982549 0.03476859 0.02651658\n",
      " 0.02026282]\n",
      "iter 22\ttotal variance to retain\t0.003158092498779297\n",
      "Epoch 11\n",
      "Iter 13200: loss: 0.3334\n",
      "\n",
      "iter 23\ttrain cost\t0.33343982696533203\n",
      "[5.5013185  1.5598042  0.19046767 0.10981629 0.06270118 0.04997433\n",
      " 0.04528053 0.04340918 0.03274796 0.02972337 0.02586214 0.02489812\n",
      " 0.01763401]\n",
      "iter 24\ttotal variance to retain\t0.00190049409866333\n",
      "Epoch 12\n",
      "Iter 14300: loss: 0.3180\n",
      "\n",
      "iter 25\ttrain cost\t0.3179701566696167\n",
      "[5.621819   1.9948773  0.17476697 0.07734843 0.05370324 0.04594894\n",
      " 0.03920784 0.03555663 0.03110912 0.0285023  0.02538825 0.02048127\n",
      " 0.01687446]\n",
      "iter 26\ttotal variance to retain\t0.0013318061828613281\n",
      "Epoch 13\n",
      "Iter 15400: loss: 0.3139\n",
      "\n",
      "iter 27\ttrain cost\t0.3139253556728363\n",
      "[5.501835   1.6701945  0.24989197 0.07942872 0.04940962 0.04614099\n",
      " 0.03523781 0.03215766 0.03079012 0.02842496 0.02216234 0.02015754\n",
      " 0.01429559]\n",
      "iter 28\ttotal variance to retain\t0.0023679137229919434\n",
      "Epoch 14\n",
      "Iter 16500: loss: 0.3106\n",
      "\n",
      "iter 29\ttrain cost\t0.31061264872550964\n",
      "[6.084286   1.6803669  0.20912725 0.06772775 0.05190303 0.04428704\n",
      " 0.03876875 0.03628597 0.03403471 0.02626708 0.02190146 0.01943076\n",
      " 0.01478001]\n",
      "iter 30\ttotal variance to retain\t0.0014717578887939453\n",
      "Epoch 15\n",
      "Iter 17600: loss: 0.2992\n",
      "\n",
      "iter 31\ttrain cost\t0.2991953194141388\n",
      "[6.416962   1.6068944  0.23780966 0.07772476 0.05785302 0.04540685\n",
      " 0.0377524  0.03590564 0.03034796 0.02513046 0.02355734 0.02032446\n",
      " 0.01543973]\n",
      "iter 32\ttotal variance to retain\t0.0016761422157287598\n",
      "Epoch 16\n",
      "Iter 18700: loss: 0.2939\n",
      "\n",
      "iter 33\ttrain cost\t0.29393962025642395\n",
      "[5.778088   1.7576716  0.13412552 0.084877   0.04541638 0.04240774\n",
      " 0.03867776 0.03220833 0.03014126 0.02815803 0.02281242 0.01573102\n",
      " 0.01147674]\n",
      "iter 34\ttotal variance to retain\t0.0009363889694213867\n",
      "Epoch 17\n",
      "Iter 19800: loss: 0.2975\n",
      "\n",
      "iter 35\ttrain cost\t0.2974811792373657\n",
      "[5.4942894  1.6871927  0.1623074  0.07643814 0.05535372 0.04318928\n",
      " 0.03501881 0.02937804 0.0260107  0.02491122 0.02191278 0.01568718\n",
      " 0.01233756]\n",
      "iter 36\ttotal variance to retain\t0.0012511610984802246\n",
      "Epoch 18\n",
      "Iter 20900: loss: 0.2821\n",
      "\n",
      "iter 37\ttrain cost\t0.2820584774017334\n",
      "[5.9174895  1.7582977  0.13181528 0.06950787 0.05612143 0.04928526\n",
      " 0.03131805 0.03007586 0.02607298 0.02535837 0.02143849 0.01917748\n",
      " 0.01578853]\n",
      "iter 38\ttotal variance to retain\t0.0008407235145568848\n",
      "Epoch 19\n",
      "Iter 22000: loss: 0.2762\n",
      "\n",
      "iter 39\ttrain cost\t0.27615320682525635\n",
      "[5.745056   1.4584574  0.11563286 0.06706314 0.04742572 0.04240543\n",
      " 0.03585872 0.02896512 0.02474336 0.02074337 0.01825011 0.0151274\n",
      " 0.01216336]\n",
      "iter 40\ttotal variance to retain\t0.0007335543632507324\n",
      "Epoch 20\n",
      "Iter 23100: loss: 0.2811\n",
      "\n",
      "iter 41\ttrain cost\t0.28112345933914185\n",
      "[6.549889   1.7088563  0.13599288 0.05849591 0.04608928 0.03681445\n",
      " 0.03198888 0.02583548 0.02446579 0.02238643 0.01880002 0.01498043\n",
      " 0.01345563]\n",
      "iter 42\ttotal variance to retain\t0.0006312131881713867\n",
      "Epoch 21\n",
      "Iter 24200: loss: 0.2731\n",
      "\n",
      "iter 43\ttrain cost\t0.2731002867221832\n",
      "[6.260889   1.7285     0.15798514 0.06374553 0.04924457 0.04629678\n",
      " 0.03717834 0.034804   0.02629237 0.02441718 0.02093322 0.01917029\n",
      " 0.01454203]\n",
      "iter 44\ttotal variance to retain\t0.0009113550186157227\n",
      "Epoch 22\n",
      "Iter 25300: loss: 0.2758\n",
      "\n",
      "iter 45\ttrain cost\t0.27583929896354675\n",
      "[4.973651   1.7866629  0.1518431  0.05909507 0.04393911 0.03719362\n",
      " 0.03332048 0.02533775 0.02161291 0.01810586 0.01736319 0.0139077\n",
      " 0.01187184]\n",
      "iter 46\ttotal variance to retain\t0.00118178129196167\n",
      "Epoch 23\n",
      "Iter 26400: loss: 0.2687\n",
      "\n",
      "iter 47\ttrain cost\t0.26873326301574707\n",
      "[6.0971313  1.4556171  0.14738494 0.06991728 0.04119954 0.03712837\n",
      " 0.03249083 0.02498238 0.02326737 0.01974316 0.01642691 0.01344517\n",
      " 0.01022992]\n",
      "iter 48\ttotal variance to retain\t0.0008353590965270996\n",
      "Epoch 24\n",
      "Iter 27500: loss: 0.2646\n",
      "\n",
      "iter 49\ttrain cost\t0.26462072134017944\n",
      "[6.5837846  1.8751163  0.14594346 0.07038742 0.04221258 0.034819\n",
      " 0.03065418 0.02541585 0.02175015 0.01828052 0.01597458 0.01409972\n",
      " 0.01153682]\n",
      "iter 50\ttotal variance to retain\t0.0006873011589050293\n",
      "Epoch 25\n",
      "Iter 28600: loss: 0.2631\n",
      "\n",
      "iter 51\ttrain cost\t0.2630870044231415\n",
      "[6.7765675  1.8357717  0.12965304 0.05953607 0.04033031 0.03559554\n",
      " 0.02793438 0.02533289 0.02280344 0.01866633 0.0175868  0.0149634\n",
      " 0.01172124]\n",
      "iter 52\ttotal variance to retain\t0.0005313754081726074\n",
      "Epoch 26\n",
      "Iter 29700: loss: 0.2646\n",
      "\n",
      "iter 53\ttrain cost\t0.26462697982788086\n",
      "[6.732654   1.7662202  0.11452895 0.06977131 0.03793243 0.03228353\n",
      " 0.02761049 0.02517606 0.02425537 0.01857966 0.01587307 0.01497326\n",
      " 0.00859723]\n",
      "iter 54\ttotal variance to retain\t0.00048154592514038086\n",
      "Epoch 27\n",
      "Iter 30800: loss: 0.2507\n",
      "\n",
      "iter 55\ttrain cost\t0.25074607133865356\n",
      "[7.133096   1.7893759  0.14150336 0.06573623 0.04771604 0.03607052\n",
      " 0.02804694 0.02523874 0.02338009 0.02231021 0.01679044 0.01470709\n",
      " 0.01194557]\n",
      "iter 56\ttotal variance to retain\t0.0005733966827392578\n",
      "Epoch 28\n",
      "Iter 31900: loss: 0.2490\n",
      "\n",
      "iter 57\ttrain cost\t0.24898336827754974\n",
      "[6.336668   1.4161063  0.09004045 0.05683158 0.05265626 0.03427978\n",
      " 0.02441836 0.02199615 0.02078184 0.01880059 0.01667213 0.01353411\n",
      " 0.01106705]\n",
      "iter 58\ttotal variance to retain\t0.00042051076889038086\n",
      "Epoch 29\n",
      "Iter 33000: loss: 0.2503\n",
      "\n",
      "iter 59\ttrain cost\t0.250268816947937\n",
      "[7.311209   1.7969581  0.09611014 0.0590784  0.03803255 0.02754115\n",
      " 0.0244309  0.02393394 0.01918411 0.01711658 0.01452123 0.01281737\n",
      " 0.01221513]\n",
      "iter 60\ttotal variance to retain\t0.00030481815338134766\n",
      "Epoch 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34100: loss: 0.2388\n",
      "\n",
      "iter 61\ttrain cost\t0.2388288676738739\n",
      "[6.4690123  1.2580606  0.08730327 0.06131354 0.03970566 0.03275031\n",
      " 0.02934401 0.02706987 0.01964388 0.015383   0.01406629 0.01309104\n",
      " 0.01081691]\n",
      "iter 62\ttotal variance to retain\t0.0003851652145385742\n",
      "Epoch 31\n",
      "Iter 35200: loss: 0.2440\n",
      "\n",
      "iter 63\ttrain cost\t0.2439848780632019\n",
      "[7.4582453  1.806137   0.09847982 0.0653986  0.03043828 0.02864425\n",
      " 0.02632103 0.02021304 0.01787254 0.01629215 0.01449614 0.0132751\n",
      " 0.01055631]\n",
      "iter 64\ttotal variance to retain\t0.0003040432929992676\n",
      "Epoch 32\n",
      "Iter 36300: loss: 0.2472\n",
      "\n",
      "iter 65\ttrain cost\t0.24715310335159302\n",
      "[7.9548516  1.8524617  0.12557341 0.05685673 0.03365954 0.03003976\n",
      " 0.02574742 0.02411112 0.02098494 0.01954213 0.01562551 0.01235741\n",
      " 0.01215565]\n",
      "iter 66\ttotal variance to retain\t0.00035440921783447266\n",
      "Epoch 33\n",
      "Iter 37400: loss: 0.2410\n",
      "\n",
      "iter 67\ttrain cost\t0.24100515246391296\n",
      "[7.689691   1.4596567  0.09536027 0.04842052 0.03719857 0.02810878\n",
      " 0.02530279 0.02160107 0.01955392 0.0169672  0.01471819 0.01208971\n",
      " 0.01089715]\n",
      "iter 68\ttotal variance to retain\t0.0002589225769042969\n",
      "Epoch 34\n",
      "Iter 38500: loss: 0.2343\n",
      "\n",
      "iter 69\ttrain cost\t0.23432576656341553\n",
      "[7.327312   1.8787441  0.07848492 0.07009812 0.03708772 0.03103048\n",
      " 0.02534516 0.0246119  0.02140648 0.01479347 0.01335369 0.01198858\n",
      " 0.01123518]\n",
      "iter 70\ttotal variance to retain\t0.0002757906913757324\n",
      "Epoch 35\n",
      "Iter 39600: loss: 0.2375\n",
      "\n",
      "iter 71\ttrain cost\t0.23747190833091736\n",
      "[9.069415   2.3123965  0.1098872  0.05910402 0.03774889 0.03438\n",
      " 0.0309076  0.02858843 0.02435824 0.02115435 0.01906316 0.0156919\n",
      " 0.01246127]\n",
      "iter 72\ttotal variance to retain\t0.00024831295013427734\n",
      "Epoch 36\n",
      "Iter 40700: loss: 0.2237\n",
      "\n",
      "iter 73\ttrain cost\t0.2237025499343872\n",
      "[7.400363   2.0510838  0.07476896 0.0538088  0.03939451 0.03053673\n",
      " 0.02487521 0.0211555  0.0171683  0.01543536 0.01402641 0.01163221\n",
      " 0.00917408]\n",
      "iter 74\ttotal variance to retain\t0.0002200603485107422\n",
      "Epoch 37\n",
      "Iter 41800: loss: 0.2326\n",
      "\n",
      "iter 75\ttrain cost\t0.23263514041900635\n",
      "[7.65404    1.5206455  0.08328906 0.0404013  0.03063184 0.02708329\n",
      " 0.02399084 0.01978235 0.01704734 0.01523243 0.01318957 0.01208558\n",
      " 0.00887788]\n",
      "iter 76\ttotal variance to retain\t0.00019913911819458008\n",
      "Epoch 38\n",
      "Iter 42900: loss: 0.2267\n",
      "\n",
      "iter 77\ttrain cost\t0.22673961520195007\n",
      "[8.075381   1.5543082  0.06612951 0.04702573 0.03507404 0.02665595\n",
      " 0.02422109 0.01964114 0.01952103 0.01616524 0.01402259 0.01174705\n",
      " 0.00983558]\n",
      "iter 78\ttotal variance to retain\t0.0001563429832458496\n",
      "Epoch 39\n",
      "Iter 44000: loss: 0.2284\n",
      "\n",
      "iter 79\ttrain cost\t0.2283848524093628\n",
      "[8.281596   2.150088   0.07690489 0.06977588 0.03350356 0.02934318\n",
      " 0.02151235 0.0188016  0.01846937 0.0149315  0.01126864 0.01083493\n",
      " 0.00886743]\n",
      "iter 80\ttotal variance to retain\t0.00019752979278564453\n",
      "Epoch 40\n",
      "Iter 45100: loss: 0.2234\n",
      "\n",
      "iter 81\ttrain cost\t0.2233666032552719\n",
      "[8.8836107e+00 2.1547885e+00 6.4906470e-02 3.8007900e-02 2.3128582e-02\n",
      " 2.2529230e-02 2.0489626e-02 1.6408417e-02 1.5635807e-02 1.4140402e-02\n",
      " 1.1618110e-02 1.0815597e-02 8.7214652e-03]\n",
      "iter 82\ttotal variance to retain\t9.769201278686523e-05\n",
      "Epoch 41\n",
      "Iter 46200: loss: 0.2276\n",
      "\n",
      "iter 83\ttrain cost\t0.22755378484725952\n",
      "[9.70602    2.3577979  0.08879049 0.05837202 0.04021613 0.0373205\n",
      " 0.03241028 0.02706137 0.02338023 0.01895763 0.0175558  0.01493861\n",
      " 0.0134371 ]\n",
      "iter 84\ttotal variance to retain\t0.0001773834228515625\n",
      "Epoch 42\n",
      "Iter 47300: loss: 0.2241\n",
      "\n",
      "iter 85\ttrain cost\t0.22407649457454681\n",
      "[9.420335   1.8806399  0.05255341 0.04737614 0.03692059 0.02969795\n",
      " 0.02476149 0.02189937 0.01922552 0.01725726 0.01634949 0.01413508\n",
      " 0.01330124]\n",
      "iter 86\ttotal variance to retain\t0.00010448694229125977\n",
      "Epoch 43\n",
      "Iter 48400: loss: 0.2235\n",
      "\n",
      "iter 87\ttrain cost\t0.22351014614105225\n",
      "[8.310677   2.2201557  0.05388584 0.03812689 0.03131157 0.02176807\n",
      " 0.02041727 0.01929609 0.01706036 0.01472454 0.01349124 0.01237276\n",
      " 0.01035675]\n",
      "iter 88\ttotal variance to retain\t0.00010198354721069336\n",
      "Epoch 44\n",
      "Iter 49500: loss: 0.2289\n",
      "\n",
      "iter 89\ttrain cost\t0.2288644015789032\n",
      "[9.1492367e+00 1.4926794e+00 5.4363217e-02 4.1974973e-02 3.6992181e-02\n",
      " 3.1821415e-02 2.2334434e-02 1.9487508e-02 1.7736914e-02 1.4705366e-02\n",
      " 1.1295788e-02 9.6703740e-03 8.4465863e-03]\n",
      "iter 90\ttotal variance to retain\t0.00010228157043457031\n",
      "Epoch 45\n",
      "Iter 50600: loss: 0.2171\n",
      "\n",
      "iter 91\ttrain cost\t0.21709007024765015\n",
      "[8.405975   1.9120241  0.07759795 0.05050997 0.04123144 0.03032214\n",
      " 0.0231441  0.02011913 0.01922033 0.01596178 0.01302046 0.01189522\n",
      " 0.01010095]\n",
      "iter 92\ttotal variance to retain\t0.00017714500427246094\n",
      "Epoch 46\n",
      "Iter 51700: loss: 0.2156\n",
      "\n",
      "iter 93\ttrain cost\t0.21560794115066528\n",
      "[8.8003082e+00 2.2995603e+00 5.5004463e-02 4.9074512e-02 3.2548331e-02\n",
      " 2.1034615e-02 1.9126097e-02 1.8179998e-02 1.6091799e-02 1.2417399e-02\n",
      " 1.2048427e-02 9.2866318e-03 7.8893164e-03]\n",
      "iter 94\ttotal variance to retain\t0.00010067224502563477\n",
      "Epoch 47\n",
      "Iter 52800: loss: 0.2232\n",
      "\n",
      "iter 95\ttrain cost\t0.22317571938037872\n",
      "[8.8394403e+00 2.1707542e+00 7.4879028e-02 4.8042107e-02 3.4640670e-02\n",
      " 2.5893953e-02 2.2322824e-02 2.0423492e-02 1.8307529e-02 1.5523408e-02\n",
      " 1.5199656e-02 1.0608085e-02 8.8238707e-03]\n",
      "iter 96\ttotal variance to retain\t0.000141143798828125\n",
      "Epoch 48\n",
      "Iter 53900: loss: 0.2157\n",
      "\n",
      "iter 97\ttrain cost\t0.21570709347724915\n",
      "[9.9363680e+00 2.0830894e+00 5.4846939e-02 3.9611418e-02 2.7078986e-02\n",
      " 2.1384804e-02 1.9225368e-02 1.6133370e-02 1.5226981e-02 1.3353020e-02\n",
      " 1.1768917e-02 9.1999648e-03 8.6224619e-03]\n",
      "iter 98\ttotal variance to retain\t6.890296936035156e-05\n",
      "Epoch 49\n",
      "Iter 55000: loss: 0.2120\n",
      "\n",
      "iter 99\ttrain cost\t0.21203838288784027\n",
      "[1.0686235e+01 2.0142927e+00 6.6078775e-02 4.0011745e-02 2.4826199e-02\n",
      " 2.2629486e-02 2.0373985e-02 1.8354366e-02 1.3646072e-02 1.2911956e-02\n",
      " 1.1753485e-02 9.5269745e-03 8.7202070e-03]\n",
      "iter 100\ttotal variance to retain\t7.18832015991211e-05\n",
      "Epoch 50\n",
      "Iter 56100: loss: 0.2128\n",
      "\n",
      "iter 101\ttrain cost\t0.21275103092193604\n",
      "[8.9972477e+00 2.5451715e+00 6.5837033e-02 3.8356766e-02 3.0326910e-02\n",
      " 2.6180129e-02 2.2580855e-02 1.9112904e-02 1.6759466e-02 1.4847511e-02\n",
      " 1.2500007e-02 1.2334745e-02 8.6038802e-03]\n",
      "iter 102\ttotal variance to retain\t0.0001049041748046875\n",
      "Epoch 51\n",
      "Iter 57200: loss: 0.2087\n",
      "\n",
      "iter 103\ttrain cost\t0.2086898237466812\n",
      "[8.878982   2.5273526  0.0606609  0.0393488  0.02958075 0.0225901\n",
      " 0.01857208 0.01738254 0.01564056 0.01282695 0.01275524 0.01141213\n",
      " 0.00932263]\n",
      "iter 104\ttotal variance to retain\t9.435415267944336e-05\n",
      "Epoch 52\n",
      "Iter 58300: loss: 0.2148\n",
      "\n",
      "iter 105\ttrain cost\t0.2147846668958664\n",
      "[10.125861    1.8729506   0.05264338  0.04679554  0.03297535  0.02857054\n",
      "  0.02587243  0.0218188   0.01942766  0.01620201  0.01357415  0.01288158\n",
      "  0.01142531]\n",
      "iter 106\ttotal variance to retain\t8.606910705566406e-05\n",
      "Epoch 53\n",
      "Iter 59400: loss: 0.2133\n",
      "\n",
      "iter 107\ttrain cost\t0.2132708728313446\n",
      "[1.0052447e+01 2.2597375e+00 5.8588102e-02 4.8162412e-02 3.4384504e-02\n",
      " 2.5600625e-02 1.8829525e-02 1.8339619e-02 1.7865527e-02 1.5386877e-02\n",
      " 1.4678263e-02 1.1131750e-02 8.8141439e-03]\n",
      "iter 108\ttotal variance to retain\t8.70823860168457e-05\n",
      "Epoch 54\n",
      "Iter 60500: loss: 0.2095\n",
      "\n",
      "iter 109\ttrain cost\t0.20950816571712494\n",
      "[8.8398724e+00 2.9668279e+00 4.9196206e-02 4.3999795e-02 3.7324939e-02\n",
      " 3.0764731e-02 2.4795184e-02 1.7397117e-02 1.5117964e-02 1.2637783e-02\n",
      " 1.1920845e-02 8.6795790e-03 7.4147489e-03]\n",
      "iter 110\ttotal variance to retain\t9.518861770629883e-05\n",
      "Epoch 55\n",
      "Iter 61600: loss: 0.2080\n",
      "\n",
      "iter 111\ttrain cost\t0.20800136029720306\n",
      "[9.6088057e+00 2.2001183e+00 4.9490806e-02 4.5129973e-02 2.7527574e-02\n",
      " 2.2330472e-02 2.1236982e-02 1.8804777e-02 1.4010998e-02 1.2690597e-02\n",
      " 1.0218875e-02 9.6531892e-03 8.0448082e-03]\n",
      "iter 112\ttotal variance to retain\t7.37309455871582e-05\n",
      "Epoch 56\n",
      "Iter 62700: loss: 0.2121\n",
      "\n",
      "iter 113\ttrain cost\t0.2121189534664154\n",
      "[1.1046305e+01 2.0403192e+00 5.9360135e-02 4.7671150e-02 3.4933981e-02\n",
      " 2.5613714e-02 2.3986349e-02 1.8980466e-02 1.7842542e-02 1.4144819e-02\n",
      " 1.1820027e-02 1.0994881e-02 9.2243627e-03]\n",
      "iter 114\ttotal variance to retain\t7.510185241699219e-05\n",
      "Epoch 57\n",
      "Iter 63800: loss: 0.2091\n",
      "\n",
      "iter 115\ttrain cost\t0.20912083983421326\n",
      "[9.3778286e+00 2.0848315e+00 7.6662086e-02 4.8503965e-02 2.3940405e-02\n",
      " 2.3543367e-02 2.0515909e-02 1.7435865e-02 1.4793407e-02 1.3709852e-02\n",
      " 1.1975494e-02 1.0395144e-02 8.3770659e-03]\n",
      "iter 116\ttotal variance to retain\t0.00011718273162841797\n",
      "Epoch 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 64900: loss: 0.2057\n",
      "\n",
      "iter 117\ttrain cost\t0.20574380457401276\n",
      "[1.1109143e+01 2.8045831e+00 5.6254681e-02 4.4968296e-02 2.6758185e-02\n",
      " 2.5006404e-02 2.1656385e-02 1.7564137e-02 1.5961064e-02 1.3434336e-02\n",
      " 1.0419526e-02 8.7266229e-03 7.2110957e-03]\n",
      "iter 118\ttotal variance to retain\t6.079673767089844e-05\n",
      "Epoch 59\n",
      "Iter 66000: loss: 0.2186\n",
      "\n",
      "iter 119\ttrain cost\t0.2185775339603424\n",
      "[1.1593894e+01 2.1990070e+00 4.3421365e-02 3.7589677e-02 2.4603305e-02\n",
      " 2.3498628e-02 2.1674503e-02 1.9827034e-02 1.5169676e-02 1.5058966e-02\n",
      " 1.0107766e-02 9.3027810e-03 6.4876233e-03]\n",
      "iter 120\ttotal variance to retain\t4.315376281738281e-05\n",
      "Epoch 60\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "scores3 = []\n",
    "for part in range(10):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((_train_x[0:(part*50)], _train_x[(part+1)*50:506]), axis=0)\n",
    "    train_y = np.concatenate((_train_y[0:(part*50)], _train_y[(part+1)*50:506]), axis=0)\n",
    "    test_x = _train_x[part*50:(part+1)*50]\n",
    "    test_y = _train_y[part*50:(part+1)*50]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    transpose_train_x = np.transpose(train_x)\n",
    "    \n",
    "    cur_w = np.random.normal(0, 0.35, (number_of_neurons,1))\n",
    "    cur_B =  np.random.normal(0, 0.35, (dimensionality, number_of_neurons))\n",
    "    cur_biases = np.zeros((number_of_neurons))\n",
    "    cur_P = np.zeros((dimensionality, dimensionality)) \n",
    "    cur_w_regr = np.random.normal(0, 0.35, (number_of_neurons_regr,1))\n",
    "    cur_B_regr = np.random.normal(0, 0.35, (dimensionality, number_of_neurons_regr))\n",
    "    cur_bias_regr = np.random.normal(0, 0.35, (number_of_neurons_regr))\n",
    "    \n",
    "    O = np.zeros((dimensionality, k)) \n",
    "    cur_iter = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init, feed_dict={Dataplace: transpose_train_x})\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" %(epoch))\n",
    "        for iteration in range(num_iters):\n",
    "            offset = (cur_iter % 12)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            sess.run(target, feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x,\n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "        reses = []\n",
    "        for i in range(100):\n",
    "            offset = (cur_iter % 12)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            res = sess.run([loss_hybrid], feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x, \n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "            reses.append(res)\n",
    "        print (\"Iter %d: loss: %.4f\\n\" %(cur_iter, np.mean(np.array(reses))))\n",
    "        lib.plot.plot('train cost', np.mean(np.array(reses)))\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "\n",
    "        cur_w, cur_B, cur_w_regr, cur_B_regr, cur_bias_regr = sess.run([w, B, w_regr, B_regr, bias_regr])\n",
    "\n",
    "        third_grad_psi = np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))\n",
    "        third_grad_psi_regr = np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))\n",
    "        for r in range(1000):\n",
    "            sess.run([tf_data_second])\n",
    "            np.concatenate((third_grad_psi, np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))), axis=0) \n",
    "            np.concatenate((third_grad_psi_regr, np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))), axis=0)\n",
    "\n",
    "        M = (1-rho)*np.matmul(np.transpose(third_grad_psi), third_grad_psi)+\\\n",
    "            rho*np.matmul(np.transpose(third_grad_psi_regr), third_grad_psi_regr)/var\n",
    "        u, s, vh = np.linalg.svd(M, full_matrices=True)\n",
    "        O = u[:,0:k:1]\n",
    "        print(s)\n",
    "        cur_P = np.matmul(O, np.transpose(O))\n",
    "        tvr = 1-np.sum(np.multiply(s[0:k],s[0:k]))/np.sum(np.multiply(s,s))\n",
    "        lib.plot.plot('total variance to retain', tvr)\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "    reduced_train_x = np.matmul(train_x, O)\n",
    "    reduced_test_x = np.matmul(test_x, O)\n",
    "    reg = LinearRegression().fit(reduced_train_x, train_y)\n",
    "    score = reg.score(reduced_test_x, test_y)\n",
    "    scores.append(score)\n",
    "    print (\"Part %d: score on test %.4f\\n\" %(part, score))\n",
    "    color = [str(item/50.) for item in test_y]\n",
    "    plt2.close()\n",
    "    plt2.scatter(np.transpose(reduced_test_x)[0], np.transpose(reduced_test_x)[1], s=100, c=color)\n",
    "    plt2.xlabel(\"1st component\")\n",
    "    plt2.ylabel(\"2nd component\")\n",
    "    plt2.show()\n",
    "    sess.close()\n",
    "    print(cur_P)\n",
    "\n",
    "    test_y_prediction = []\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict_regr(10, reduced_train_x, train_y, test_image)\n",
    "        test_y_prediction.append(pred)\n",
    "    score3 = 100*(1.0-np.mean((np.array(test_y_prediction)- test_y)**2)/np.var(test_y))\n",
    "    scores3.append(score3)\n",
    "    print('acc:', str(round(score3, 2))+'%')\n",
    "print (\"Average 10-NN acc on test %.4f\\n\" %(np.mean(np.array(scores3))))\n",
    "print (\"Average score on test %.4f\\n\" %(np.mean(np.array(scores))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
