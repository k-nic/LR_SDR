{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "F1Y_8jYdjORy",
    "outputId": "36ff5c4b-0fc7-4bb1-e76f-f901264c8989"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import scipy as scipy\n",
    "import scipy.interpolate as interpolate\n",
    "import math\n",
    "from random import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "Lambda = 10.0\n",
    "BATCH_SIZE = 30\n",
    "BATCH_SIZE_2 = 100\n",
    "dimensionality = 13\n",
    "num_epochs = 100\n",
    "num_iters = 1000\n",
    "k=2\n",
    "max_grad_norm = 1000\n",
    "gamma = 1.0\n",
    "\n",
    "# main parameters\n",
    "\n",
    "rho = 0.5\n",
    "C=10.0\n",
    "\n",
    "# additional parameters\n",
    "sigma=0.8\n",
    "\n",
    "number_of_neurons_regr = 100\n",
    "number_of_points = 303-33\n",
    "number_of_neurons = number_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5ExlL7s3o9S6",
    "outputId": "82a79e11-a546-4fa7-81a1-de76f5b0011d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n",
      "(303, 13)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "heart = pd.read_csv('/home/rust/Desktop/SDR/heart.csv')\n",
    "X = heart.iloc[:,:-1].values\n",
    "y = heart.iloc[:,13].values\n",
    "print(len(y))\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(303)\n",
    "for x in rr:\n",
    "    data.append(X[x])\n",
    "    target.append(y[x])\n",
    "# training data\n",
    "_train_x = np.array(data)\n",
    "_train_y = np.array(target)\n",
    "print(_train_x.shape)\n",
    "print(_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvKNXjixjm-A"
   },
   "outputs": [],
   "source": [
    "new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "Dataplace = tf.placeholder(tf.float32, shape=(dimensionality, number_of_points))\n",
    "Data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, dimensionality))\n",
    "outputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE,1)) \n",
    "\n",
    "tf_data_x = gamma*tf.random_normal([BATCH_SIZE, dimensionality]) # аргументы функции\n",
    "tf_data_y = tf.reduce_mean(tf.math.cos(tf.matmul(tf_data_x, Dataplace)), axis=1) # значения функции\n",
    "\n",
    "tf_data_w = tf.placeholder(tf.float32, shape=(number_of_neurons,1))\n",
    "tf_data_B = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons))\n",
    "tf_data_P = tf.placeholder(tf.float32, shape=(dimensionality, dimensionality))\n",
    "\n",
    "tf_w_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr,1))\n",
    "tf_B_regr = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons_regr))\n",
    "tf_bias_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr))\n",
    "\n",
    "\n",
    "tf_data_second = gamma*tf.random_normal([BATCH_SIZE_2, dimensionality])\n",
    "Lbd = tf.placeholder(tf.float32, shape=[], name=\"lambda\")\n",
    "\n",
    "# characteristic function parameters\n",
    "w = tf.Variable(tf.random_normal([number_of_neurons,1], stddev=0.0), name=\"neuron_weights\")\n",
    "B = tf.Variable(initial_value=Dataplace, name=\"weights\")\n",
    "# regression function parameters\n",
    "w_regr = tf.Variable(tf.random_normal([number_of_neurons_regr,1], stddev=0.35), name=\"neuron_weights\")\n",
    "B_regr = tf.Variable(tf.random_normal([dimensionality,number_of_neurons_regr], stddev=0.35), name=\"weights\")\n",
    "bias_regr = tf.Variable(tf.random_normal([number_of_neurons_regr], stddev=0.0), name=\"biases\")\n",
    "\n",
    "prediction = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_x, B)), tf.nn.sigmoid(w))\n",
    "penalty = tf.square((2/number_of_neurons)*tf.reduce_sum(tf.nn.sigmoid(w))-1.0)\n",
    "\n",
    "out_loss = (1-rho)*tf.reduce_mean(tf.square(prediction - tf_data_y)) + C*penalty\n",
    "\n",
    "prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, B)), tf.nn.sigmoid(w))\n",
    "prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, B_regr), bias_regr)), w_regr)\n",
    "\n",
    "grad_psi = tf.reshape(tf.gradients(prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "grad_psi_regr = tf.reshape(tf.gradients(prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "tf_prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, tf_data_B)), tf.nn.sigmoid(tf_data_w))\n",
    "tf_prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, tf_B_regr), tf_bias_regr)), tf_w_regr)\n",
    "\n",
    "tf_data_grad_psi = tf.reshape(tf.gradients(tf_prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "tf_data_grad_psi_regr = tf.reshape(tf.gradients(tf_prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "old_part = tf.matmul(tf_data_grad_psi, tf_data_P)\n",
    "old_part_regr = tf.matmul(tf_data_grad_psi_regr, tf_data_P)\n",
    "\n",
    "loss = out_loss + Lbd*(1-rho)*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi, old_part)), axis=1)) + Lbd*rho*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi_regr, old_part_regr)), axis=1))\n",
    "\n",
    "#offset = tf.random.uniform(shape=[], minval=0, maxval=12, dtype=tf.int32)*BATCH_SIZE\n",
    "x_plus_error = Data+sigma*tf.random_normal([BATCH_SIZE, dimensionality])\n",
    "regression = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(x_plus_error, B_regr), bias_regr)), w_regr)\n",
    "sdr = -tf.reduce_mean(tf.multiply(regression, outputs)) + tf.reduce_mean(tf.math.log(1.0+tf.math.exp(regression)))\n",
    "\n",
    "loss_hybrid = loss + rho*sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pnt1, pnt2):\n",
    "    '''Finds the distance between 2 points: pnt1, pnt2'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((pnt1 - pnt2) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "\n",
    "def new_predict(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "def new_predict_regr(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return np.mean(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3HlgJ8XdjtxY",
    "outputId": "8ec5f3f0-92f7-4712-94c2-e9bff821b918"
   },
   "outputs": [],
   "source": [
    "target = tf.train.AdamOptimizer(learning_rate=3e-5, beta1=0.5, beta2=0.9).minimize(loss_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YojZRg6lMgur"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1. /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HLqISRILj5VQ",
    "outputId": "83373660-15c5-4d88-a2da-ec9bcf3a5a56",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current part is 0\n",
      "\n",
      "Epoch 0\n",
      "Iter 1100: loss: 10.7542\n",
      "\n",
      "[41.827614  16.936243   9.577354   8.666677   7.6756167  6.6718674\n",
      "  5.159369   3.9996836  2.9534886  2.2777178  1.826985   1.5583429\n",
      "  1.4812065]\n",
      "iter 1\ttrain cost\t10.754154205322266\ttotal variance to retain\t0.14117854833602905\n",
      "Epoch 1\n",
      "Iter 2200: loss: 4.6002\n",
      "\n",
      "[26.505621  13.711776   7.4149857  6.6231456  4.439984   3.7930393\n",
      "  3.5690017  2.6627913  2.1086833  1.869579   1.3752363  1.3081744\n",
      "  1.094559 ]\n",
      "iter 2\ttrain cost\t4.60021448135376\ttotal variance to retain\t0.15672999620437622\n",
      "Epoch 2\n",
      "Iter 3300: loss: 3.2562\n",
      "\n",
      "[30.805477  12.119826   5.2561827  4.126224   3.373397   2.9705784\n",
      "  2.5360003  2.0524929  1.634194   1.3071406  1.1560993  1.0442059\n",
      "  0.6964951]\n",
      "iter 3\ttrain cost\t3.2561721801757812\ttotal variance to retain\t0.07024288177490234\n",
      "Epoch 3\n",
      "Iter 4400: loss: 2.3257\n",
      "\n",
      "[27.20281    10.908893    4.071526    3.249227    2.5737846   2.071537\n",
      "  1.6959594   1.4078045   0.9774981   0.8524917   0.8177818   0.73552275\n",
      "  0.6327912 ]\n",
      "iter 4\ttrain cost\t2.325732946395874\ttotal variance to retain\t0.051039814949035645\n",
      "Epoch 4\n",
      "Iter 5500: loss: 1.7524\n",
      "\n",
      "[19.684477    8.974135    2.9539878   2.6189451   1.7891064   1.503131\n",
      "  1.1944182   1.145258    0.805053    0.74448526  0.5126536   0.48012036\n",
      "  0.36727086]\n",
      "iter 5\ttrain cost\t1.7524317502975464\ttotal variance to retain\t0.05188941955566406\n",
      "Epoch 5\n",
      "Iter 6600: loss: 1.3343\n",
      "\n",
      "[23.663586    8.421639    2.247072    1.6731886   1.1420573   0.8223103\n",
      "  0.74371195  0.7295795   0.49998087  0.46646068  0.37486497  0.33081442\n",
      "  0.29888868]\n",
      "iter 6\ttrain cost\t1.3342822790145874\ttotal variance to retain\t0.018240630626678467\n",
      "Epoch 6\n",
      "Iter 7700: loss: 1.0396\n",
      "\n",
      "[20.971458    7.499195    1.1948608   1.0449243   0.9521977   0.7337946\n",
      "  0.6198997   0.5077058   0.42123517  0.31186023  0.2944204   0.25489005\n",
      "  0.22868934]\n",
      "iter 7\ttrain cost\t1.039622187614441\ttotal variance to retain\t0.010147929191589355\n",
      "Epoch 7\n",
      "Iter 8800: loss: 0.8500\n",
      "\n",
      "[21.309895    7.3273277   0.931603    0.7445902   0.65396637  0.45148915\n",
      "  0.38360375  0.3670554   0.26724187  0.19414243  0.18357036  0.16372572\n",
      "  0.15032841]\n",
      "iter 8\ttrain cost\t0.8499807119369507\ttotal variance to retain\t0.004953444004058838\n",
      "Epoch 8\n",
      "Iter 9900: loss: 0.7240\n",
      "\n",
      "[19.57407     7.3941574   0.76620954  0.6120718   0.532327    0.36846918\n",
      "  0.328509    0.2913194   0.24606249  0.19420165  0.1645385   0.14534926\n",
      "  0.11660052]\n",
      "iter 9\ttrain cost\t0.7239819169044495\ttotal variance to retain\t0.003944277763366699\n",
      "Epoch 9\n",
      "Iter 11000: loss: 0.6414\n",
      "\n",
      "[16.917507    6.469338    0.6280826   0.4772276   0.37478763  0.3093893\n",
      "  0.25152814  0.22127216  0.18669416  0.1628339   0.1327478   0.12336715\n",
      "  0.10238611]\n",
      "iter 10\ttrain cost\t0.6413998603820801\ttotal variance to retain\t0.003267228603363037\n",
      "Epoch 10\n",
      "Iter 12100: loss: 0.5827\n",
      "\n",
      "[18.090427    5.9059525   0.4196631   0.37035367  0.33145273  0.22127558\n",
      "  0.20842353  0.18651882  0.17617801  0.14222421  0.11353713  0.09511718\n",
      "  0.08336648]\n",
      "iter 11\ttrain cost\t0.5826824307441711\ttotal variance to retain\t0.0017379522323608398\n",
      "Epoch 11\n",
      "Iter 13200: loss: 0.5378\n",
      "\n",
      "[18.636234    5.471415    0.48798186  0.2889264   0.25931424  0.22106713\n",
      "  0.17595474  0.17159456  0.14236909  0.1281255   0.10161781  0.08908165\n",
      "  0.08247586]\n",
      "iter 12\ttrain cost\t0.5378021001815796\ttotal variance to retain\t0.0014818310737609863\n",
      "Epoch 12\n",
      "Iter 14300: loss: 0.5119\n",
      "\n",
      "[18.262613    5.2525296   0.3201073   0.2594898   0.20926037  0.17696391\n",
      "  0.16971768  0.13356131  0.12112944  0.09785407  0.08577377  0.07582192\n",
      "  0.05278434]\n",
      "iter 13\ttrain cost\t0.511935830116272\ttotal variance to retain\t0.0009176135063171387\n",
      "Epoch 13\n",
      "Iter 15400: loss: 0.4955\n",
      "\n",
      "[18.898348    5.559168    0.5181693   0.248929    0.23577595  0.17009006\n",
      "  0.12790634  0.12429293  0.11948247  0.09563728  0.08407169  0.05705896\n",
      "  0.05259924]\n",
      "iter 14\ttrain cost\t0.4955176115036011\ttotal variance to retain\t0.0012440681457519531\n",
      "Epoch 14\n",
      "Iter 16500: loss: 0.4764\n",
      "\n",
      "[19.753637    5.9329996   0.32579634  0.18615517  0.14749674  0.12929967\n",
      "  0.1195209   0.10807984  0.10523202  0.08419961  0.07604025  0.0661919\n",
      "  0.04164681]\n",
      "iter 15\ttrain cost\t0.47641390562057495\ttotal variance to retain\t0.000552833080291748\n",
      "Epoch 15\n",
      "Iter 17600: loss: 0.4633\n",
      "\n",
      "[20.578316    4.3224707   0.24964955  0.2074355   0.1691447   0.15187882\n",
      "  0.11898999  0.09983893  0.08971183  0.08106042  0.0704498   0.05699323\n",
      "  0.04635314]\n",
      "iter 16\ttrain cost\t0.4633151590824127\ttotal variance to retain\t0.0004659295082092285\n",
      "Epoch 16\n",
      "Iter 18700: loss: 0.4579\n",
      "\n",
      "[21.221514    4.1181765   0.22410028  0.17971332  0.15090793  0.14172412\n",
      "  0.11796612  0.10067785  0.09727007  0.09334657  0.06158141  0.05740767\n",
      "  0.04949273]\n",
      "iter 17\ttrain cost\t0.45787644386291504\ttotal variance to retain\t0.0003789663314819336\n",
      "Epoch 17\n",
      "Iter 19800: loss: 0.4447\n",
      "\n",
      "[19.193155    3.81473     0.2248588   0.17285514  0.13740273  0.11509008\n",
      "  0.10536119  0.09267528  0.08061583  0.07521123  0.06417526  0.05652464\n",
      "  0.04289092]\n",
      "iter 18\ttrain cost\t0.44465845823287964\ttotal variance to retain\t0.00040084123611450195\n",
      "Epoch 18\n",
      "Iter 20900: loss: 0.4354\n",
      "\n",
      "[18.166794    5.5398536   0.30199257  0.2181836   0.1605447   0.1290964\n",
      "  0.11423169  0.09817576  0.09398915  0.07733932  0.0723176   0.05328764\n",
      "  0.04622627]\n",
      "iter 19\ttrain cost\t0.4353748559951782\ttotal variance to retain\t0.0006341934204101562\n",
      "Epoch 19\n",
      "Iter 22000: loss: 0.4289\n",
      "\n",
      "[18.18094     4.044674    0.27622235  0.19928211  0.12796816  0.12184852\n",
      "  0.11331704  0.10149559  0.09267535  0.07673802  0.06509662  0.04811366\n",
      "  0.03346224]\n",
      "iter 20\ttrain cost\t0.42888355255126953\ttotal variance to retain\t0.0005546212196350098\n",
      "Epoch 20\n",
      "Iter 23100: loss: 0.4209\n",
      "\n",
      "[17.266048    4.6109185   0.27920836  0.12786813  0.11704776  0.11428342\n",
      "  0.0968823   0.08941431  0.07407094  0.06284329  0.05789392  0.04552431\n",
      "  0.03314621]\n",
      "iter 21\ttrain cost\t0.42089855670928955\ttotal variance to retain\t0.0004831552505493164\n",
      "Epoch 21\n",
      "Iter 24200: loss: 0.4213\n",
      "\n",
      "[16.899626    3.7312071   0.21982425  0.1569058   0.13427137  0.1232918\n",
      "  0.10312711  0.08659701  0.06906337  0.05858218  0.05553185  0.04642219\n",
      "  0.03454515]\n",
      "iter 22\ttrain cost\t0.4212585389614105\ttotal variance to retain\t0.0004636049270629883\n",
      "Epoch 22\n",
      "Iter 25300: loss: 0.4138\n",
      "\n",
      "[18.319742    4.0408773   0.2243432   0.16417645  0.14730796  0.1306859\n",
      "  0.10961381  0.08835271  0.06830955  0.06131013  0.04947269  0.04052207\n",
      "  0.0316987 ]\n",
      "iter 23\ttrain cost\t0.4138302206993103\ttotal variance to retain\t0.00042426586151123047\n",
      "Epoch 23\n",
      "Iter 26400: loss: 0.4077\n",
      "\n",
      "[19.396267    5.113823    0.24692513  0.14890622  0.13838105  0.12263682\n",
      "  0.10580621  0.0938665   0.08086468  0.06889667  0.0581498   0.05021263\n",
      "  0.03139636]\n",
      "iter 24\ttrain cost\t0.4076707065105438\ttotal variance to retain\t0.00038635730743408203\n",
      "Epoch 24\n",
      "Iter 27500: loss: 0.4075\n",
      "\n",
      "[19.08011     4.652673    0.21341488  0.16805786  0.13175006  0.11923523\n",
      "  0.09242305  0.07406206  0.06881145  0.05142063  0.04124007  0.03862667\n",
      "  0.03435834]\n",
      "iter 25\ttrain cost\t0.4074503183364868\ttotal variance to retain\t0.00033992528915405273\n",
      "Epoch 25\n",
      "Iter 28600: loss: 0.4007\n",
      "\n",
      "[20.333662    4.5359335   0.24870485  0.22647937  0.17232424  0.1253689\n",
      "  0.10330214  0.09203341  0.07569541  0.06065162  0.05135524  0.04936932\n",
      "  0.03477457]\n",
      "iter 26\ttrain cost\t0.4006832540035248\ttotal variance to retain\t0.00044542551040649414\n",
      "Epoch 26\n",
      "Iter 29700: loss: 0.3944\n",
      "\n",
      "[19.29725     4.3657446   0.25914726  0.19900624  0.11437045  0.11043897\n",
      "  0.0936181   0.08254798  0.07570834  0.06417219  0.05473158  0.04644229\n",
      "  0.03709011]\n",
      "iter 27\ttrain cost\t0.39437735080718994\ttotal variance to retain\t0.00041878223419189453\n",
      "Epoch 27\n",
      "Iter 30800: loss: 0.3952\n",
      "\n",
      "[18.142237    4.5886064   0.2202059   0.16147628  0.12374465  0.11962057\n",
      "  0.10197283  0.07871652  0.06928554  0.06332579  0.0549789   0.03896894\n",
      "  0.03494382]\n",
      "iter 28\ttrain cost\t0.39523687958717346\ttotal variance to retain\t0.00038623809814453125\n",
      "Epoch 28\n",
      "Iter 31900: loss: 0.3883\n",
      "\n",
      "[22.792395    3.4292479   0.18348554  0.15135016  0.13851044  0.11868583\n",
      "  0.09699214  0.08799063  0.07013476  0.0591455   0.05252305  0.04921708\n",
      "  0.03958338]\n",
      "iter 29\ttrain cost\t0.38833606243133545\ttotal variance to retain\t0.00022995471954345703\n",
      "Epoch 29\n",
      "Iter 33000: loss: 0.3870\n",
      "\n",
      "[20.697842    3.4966848   0.1834341   0.16327626  0.14339118  0.12222137\n",
      "  0.08317107  0.07508956  0.06347078  0.0553432   0.05391761  0.0446134\n",
      "  0.0363302 ]\n",
      "iter 30\ttrain cost\t0.3870210647583008\ttotal variance to retain\t0.0002759695053100586\n",
      "Epoch 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 34100: loss: 0.3841\n",
      "\n",
      "[20.211464    3.571383    0.19473925  0.15468691  0.12071341  0.11109167\n",
      "  0.1025493   0.09071996  0.07454979  0.05945069  0.04773057  0.03330637\n",
      "  0.02924774]\n",
      "iter 31\ttrain cost\t0.38406360149383545\ttotal variance to retain\t0.0002868175506591797\n",
      "Epoch 31\n",
      "Iter 35200: loss: 0.3794\n",
      "\n",
      "[20.52051     4.787543    0.19989178  0.1711772   0.13490643  0.10351316\n",
      "  0.09835576  0.0851928   0.06695773  0.06341775  0.04834347  0.03985502\n",
      "  0.03917544]\n",
      "iter 32\ttrain cost\t0.3793615400791168\ttotal variance to retain\t0.0002905726432800293\n",
      "Epoch 32\n",
      "Iter 36300: loss: 0.3769\n",
      "\n",
      "[18.748327    4.6935806   0.14935635  0.11449033  0.10421561  0.08538733\n",
      "  0.07616009  0.07271955  0.05883348  0.04998924  0.04655363  0.04411939\n",
      "  0.03200497]\n",
      "iter 33\ttrain cost\t0.3769434690475464\ttotal variance to retain\t0.00020271539688110352\n",
      "Epoch 33\n",
      "Iter 37400: loss: 0.3757\n",
      "\n",
      "[20.016495    4.26997     0.19084428  0.13482678  0.12761402  0.10290785\n",
      "  0.08275277  0.0795683   0.06737026  0.06667872  0.05144859  0.04886458\n",
      "  0.04169545]\n",
      "iter 34\ttrain cost\t0.37570735812187195\ttotal variance to retain\t0.00026357173919677734\n",
      "Epoch 34\n",
      "Iter 38500: loss: 0.3732\n",
      "\n",
      "[18.679289    3.9441278   0.23083214  0.14377207  0.11277775  0.09762943\n",
      "  0.07662904  0.06592912  0.05945412  0.05111497  0.04415136  0.03012379\n",
      "  0.02326584]\n",
      "iter 35\ttrain cost\t0.3731623888015747\ttotal variance to retain\t0.00031816959381103516\n",
      "Epoch 35\n",
      "Iter 39600: loss: 0.3719\n",
      "\n",
      "[23.414545    3.331844    0.16693893  0.1286633   0.11651316  0.10062083\n",
      "  0.08289716  0.07666162  0.06873763  0.06388494  0.05432279  0.04155996\n",
      "  0.03964257]\n",
      "iter 36\ttrain cost\t0.3718509376049042\ttotal variance to retain\t0.00017142295837402344\n",
      "Epoch 36\n",
      "Iter 40700: loss: 0.3642\n",
      "\n",
      "[23.27131     4.1073236   0.19796185  0.16175205  0.13948363  0.09085947\n",
      "  0.07904646  0.06612244  0.05950453  0.05163803  0.04900508  0.0454211\n",
      "  0.02467633]\n",
      "iter 37\ttrain cost\t0.3641856908798218\ttotal variance to retain\t0.00020587444305419922\n",
      "Epoch 37\n",
      "Iter 41800: loss: 0.3612\n",
      "\n",
      "[23.039444    4.5472646   0.18890373  0.15826413  0.1443454   0.12353865\n",
      "  0.11191741  0.0772967   0.06349237  0.05972173  0.05052098  0.0446737\n",
      "  0.03936921]\n",
      "iter 38\ttrain cost\t0.36115849018096924\ttotal variance to retain\t0.0002338886260986328\n",
      "Epoch 38\n",
      "Iter 42900: loss: 0.3615\n",
      "\n",
      "[22.926998    3.8120666   0.15307122  0.09939913  0.09672773  0.09033484\n",
      "  0.07009783  0.06533335  0.06400439  0.0520827   0.04635556  0.04201192\n",
      "  0.0325804 ]\n",
      "iter 39\ttrain cost\t0.36153659224510193\ttotal variance to retain\t0.00013273954391479492\n",
      "Epoch 39\n",
      "Iter 44000: loss: 0.3622\n",
      "\n",
      "[22.71114     4.4509683   0.19238131  0.15009916  0.11751925  0.09262779\n",
      "  0.09191109  0.08319815  0.07009208  0.05409317  0.05079098  0.04345728\n",
      "  0.03179877]\n",
      "iter 40\ttrain cost\t0.3621595799922943\ttotal variance to retain\t0.00020653009414672852\n",
      "Epoch 40\n",
      "Iter 45100: loss: 0.3563\n",
      "\n",
      "[25.165243    3.9727786   0.15537693  0.12212233  0.10054798  0.08628915\n",
      "  0.08225523  0.06796565  0.05287348  0.04641127  0.04249863  0.03817151\n",
      "  0.02613883]\n",
      "iter 41\ttrain cost\t0.3563195765018463\ttotal variance to retain\t0.00011849403381347656\n",
      "Epoch 41\n",
      "Iter 46200: loss: 0.3550\n",
      "\n",
      "[23.456335    3.702041    0.12870762  0.10064123  0.08716442  0.0829768\n",
      "  0.06634685  0.06126261  0.05450317  0.04893341  0.04678976  0.03451795\n",
      "  0.03024398]\n",
      "iter 42\ttrain cost\t0.3549897074699402\ttotal variance to retain\t0.00010466575622558594\n",
      "Epoch 42\n",
      "Iter 47300: loss: 0.3506\n",
      "\n",
      "[25.250652    3.6948261   0.15493499  0.1214224   0.1156351   0.10237948\n",
      "  0.08452606  0.07396246  0.06320729  0.05713684  0.04600719  0.03426399\n",
      "  0.03178557]\n",
      "iter 43\ttrain cost\t0.3505815863609314\ttotal variance to retain\t0.00013315677642822266\n",
      "Epoch 43\n",
      "Iter 48400: loss: 0.3457\n",
      "\n",
      "[24.970982    3.969254    0.15003875  0.11819833  0.09952924  0.08289883\n",
      "  0.07779687  0.0695051   0.06278758  0.05253813  0.04010389  0.03870198\n",
      "  0.03045258]\n",
      "iter 44\ttrain cost\t0.34565719962120056\ttotal variance to retain\t0.00011712312698364258\n",
      "Epoch 44\n",
      "Iter 49500: loss: 0.3457\n",
      "\n",
      "[22.375061    4.7192736   0.14819382  0.11360642  0.10720551  0.07369241\n",
      "  0.06895002  0.05898875  0.0558162   0.05120234  0.0490655   0.03840279\n",
      "  0.03262947]\n",
      "iter 45\ttrain cost\t0.3457372188568115\ttotal variance to retain\t0.00013500452041625977\n",
      "Epoch 45\n",
      "Iter 50600: loss: 0.3444\n",
      "\n",
      "[25.656351    3.996295    0.12439234  0.11901508  0.08984834  0.08136483\n",
      "  0.07518693  0.07254175  0.0664902   0.05612582  0.04750418  0.04135007\n",
      "  0.03004915]\n",
      "iter 46\ttrain cost\t0.34436526894569397\ttotal variance to retain\t0.0001004934310913086\n",
      "Epoch 46\n",
      "Iter 51700: loss: 0.3450\n",
      "\n",
      "[23.751913    3.987671    0.1701304   0.13176553  0.09487415  0.08899174\n",
      "  0.07935081  0.07333387  0.06028219  0.05956898  0.0500068   0.03900706\n",
      "  0.02447025]\n",
      "iter 47\ttrain cost\t0.34503069519996643\ttotal variance to retain\t0.00014960765838623047\n",
      "Epoch 47\n",
      "Iter 52800: loss: 0.3429\n",
      "\n",
      "[27.792803    3.5644648   0.13976397  0.11707278  0.1001737   0.09511707\n",
      "  0.07678662  0.06670695  0.05623595  0.05262174  0.04441633  0.03369396\n",
      "  0.03189922]\n",
      "iter 48\ttrain cost\t0.342864990234375\ttotal variance to retain\t9.262561798095703e-05\n",
      "Epoch 48\n",
      "Iter 53900: loss: 0.3398\n",
      "\n",
      "[25.278648    3.782629    0.12177408  0.1075619   0.09650126  0.08039274\n",
      "  0.07698718  0.06930318  0.05951493  0.04608577  0.0420712   0.03503574\n",
      "  0.032263  ]\n",
      "iter 49\ttrain cost\t0.33977818489074707\ttotal variance to retain\t9.584426879882812e-05\n",
      "Epoch 49\n",
      "Iter 55000: loss: 0.3362\n",
      "\n",
      "[25.304043    4.3263497   0.1695823   0.12304244  0.10384413  0.08110667\n",
      "  0.0623057   0.05844055  0.05778662  0.05062725  0.04004378  0.03604186\n",
      "  0.02582144]\n",
      "iter 50\ttrain cost\t0.33624592423439026\ttotal variance to retain\t0.00011837482452392578\n",
      "Epoch 50\n",
      "Iter 56100: loss: 0.3324\n",
      "\n",
      "[2.5544834e+01 4.3403387e+00 1.3044366e-01 1.0204584e-01 8.4580526e-02\n",
      " 7.9382747e-02 6.6957660e-02 6.1509881e-02 5.4466110e-02 4.3372426e-02\n",
      " 3.3704564e-02 3.2099634e-02 2.4495130e-02]\n",
      "iter 51\ttrain cost\t0.33243614435195923\ttotal variance to retain\t8.463859558105469e-05\n",
      "Epoch 51\n",
      "Iter 57200: loss: 0.3339\n",
      "\n",
      "[24.914202    4.5227823   0.10428037  0.09319448  0.07511769  0.06873337\n",
      "  0.05752822  0.05113799  0.0486444   0.04311563  0.03938648  0.03008861\n",
      "  0.02710322]\n",
      "iter 52\ttrain cost\t0.3339073657989502\ttotal variance to retain\t6.74128532409668e-05\n",
      "Epoch 52\n",
      "Iter 58300: loss: 0.3306\n",
      "\n",
      "[24.421928    3.9866147   0.11479963  0.09266187  0.08802148  0.07195454\n",
      "  0.07086468  0.06197526  0.05613688  0.04447198  0.03783572  0.03591956\n",
      "  0.02845076]\n",
      "iter 53\ttrain cost\t0.3306211233139038\ttotal variance to retain\t8.52346420288086e-05\n",
      "Epoch 53\n",
      "Iter 59400: loss: 0.3305\n",
      "\n",
      "[2.7607128e+01 4.1412411e+00 1.2040050e-01 9.1342688e-02 7.6323368e-02\n",
      " 7.0549726e-02 6.4005032e-02 5.8897957e-02 4.9795177e-02 4.5145564e-02\n",
      " 4.4326682e-02 3.7335999e-02 2.4130099e-02]\n",
      "iter 54\ttrain cost\t0.33050671219825745\ttotal variance to retain\t6.377696990966797e-05\n",
      "Epoch 54\n",
      "Iter 60500: loss: 0.3272\n",
      "\n",
      "[2.7438837e+01 4.2630849e+00 1.1313328e-01 1.0682226e-01 8.6217985e-02\n",
      " 6.8044350e-02 6.1297175e-02 5.7399988e-02 4.7920749e-02 4.6276689e-02\n",
      " 4.0697366e-02 3.2458402e-02 2.4959486e-02]\n",
      "iter 55\ttrain cost\t0.3272075355052948\ttotal variance to retain\t6.622076034545898e-05\n",
      "Epoch 55\n",
      "Iter 61600: loss: 0.3269\n",
      "\n",
      "[29.090773    3.6506562   0.11430476  0.09255965  0.08855492  0.07461473\n",
      "  0.06605935  0.05976766  0.05447565  0.04900615  0.04725716  0.03906963\n",
      "  0.03117558]\n",
      "iter 56\ttrain cost\t0.3269021213054657\ttotal variance to retain\t6.175041198730469e-05\n",
      "Epoch 56\n",
      "Iter 62700: loss: 0.3259\n",
      "\n",
      "[26.810358    3.1837711   0.15388928  0.11123145  0.08744406  0.07297669\n",
      "  0.06686681  0.06160817  0.04919671  0.04123507  0.03832112  0.03219465\n",
      "  0.02878112]\n",
      "iter 57\ttrain cost\t0.3258976340293884\ttotal variance to retain\t8.893013000488281e-05\n",
      "Epoch 57\n",
      "Iter 63800: loss: 0.3235\n",
      "\n",
      "[3.0725393e+01 4.1684794e+00 1.1522110e-01 1.0709173e-01 8.3509617e-02\n",
      " 8.2014263e-02 6.7435458e-02 5.9623133e-02 4.9586255e-02 4.5116916e-02\n",
      " 3.7971415e-02 3.2713044e-02 2.3565786e-02]\n",
      "iter 58\ttrain cost\t0.32350730895996094\ttotal variance to retain\t5.626678466796875e-05\n",
      "Epoch 58\n",
      "Iter 64900: loss: 0.3189\n",
      "\n",
      "[2.7424017e+01 4.3397527e+00 1.1100506e-01 9.5089935e-02 8.1915922e-02\n",
      " 7.2086424e-02 6.0926482e-02 4.9617272e-02 4.7738537e-02 4.5330543e-02\n",
      " 3.2606021e-02 3.0612269e-02 2.4552701e-02]\n",
      "iter 59\ttrain cost\t0.3188706338405609\ttotal variance to retain\t6.008148193359375e-05\n",
      "Epoch 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 66000: loss: 0.3204\n",
      "\n",
      "[2.98679733e+01 3.91836405e+00 1.39315397e-01 1.07797444e-01\n",
      " 8.02638158e-02 7.75370523e-02 6.44958615e-02 5.83101399e-02\n",
      " 5.21612614e-02 4.48609181e-02 3.88478637e-02 3.82175744e-02\n",
      " 2.68350709e-02]\n",
      "iter 60\ttrain cost\t0.32036668062210083\ttotal variance to retain\t6.556510925292969e-05\n",
      "Epoch 60\n",
      "Iter 67100: loss: 0.3154\n",
      "\n",
      "[2.9724022e+01 3.9391670e+00 1.1143416e-01 8.5991234e-02 7.7949770e-02\n",
      " 6.6767298e-02 6.6271193e-02 5.3362343e-02 4.9302958e-02 4.1550096e-02\n",
      " 3.6755398e-02 2.5564456e-02 2.1215176e-02]\n",
      "iter 61\ttrain cost\t0.31538164615631104\ttotal variance to retain\t4.9173831939697266e-05\n",
      "Epoch 61\n",
      "Iter 68200: loss: 0.3160\n",
      "\n",
      "[3.05096912e+01 4.20830727e+00 1.05439976e-01 8.93008709e-02\n",
      " 7.85195157e-02 5.63405901e-02 5.11824302e-02 4.91598584e-02\n",
      " 4.80686910e-02 3.97210233e-02 2.92215105e-02 2.57232562e-02\n",
      " 2.41441820e-02]\n",
      "iter 62\ttrain cost\t0.31600141525268555\ttotal variance to retain\t4.172325134277344e-05\n",
      "Epoch 62\n",
      "Iter 69300: loss: 0.3166\n",
      "\n",
      "[3.0431829e+01 4.7168026e+00 1.5257972e-01 8.8010721e-02 6.9900207e-02\n",
      " 6.8741195e-02 6.0565539e-02 5.3521469e-02 4.8881982e-02 4.1328106e-02\n",
      " 3.8210653e-02 3.0442597e-02 2.8443677e-02]\n",
      "iter 63\ttrain cost\t0.31655409932136536\ttotal variance to retain\t5.739927291870117e-05\n",
      "Epoch 63\n",
      "Iter 70400: loss: 0.3121\n",
      "\n",
      "[3.1281153e+01 4.4671278e+00 1.3283721e-01 9.4880372e-02 8.7015308e-02\n",
      " 7.3662929e-02 5.8847990e-02 4.9286772e-02 4.2357832e-02 3.9852820e-02\n",
      " 3.6201347e-02 3.2543454e-02 2.6015123e-02]\n",
      "iter 64\ttrain cost\t0.31206268072128296\ttotal variance to retain\t5.1975250244140625e-05\n",
      "Epoch 64\n",
      "Iter 71500: loss: 0.3146\n",
      "\n",
      "[3.1204189e+01 4.2902889e+00 1.3798979e-01 1.1209687e-01 8.9135297e-02\n",
      " 8.2052417e-02 7.1653448e-02 5.7710920e-02 5.1043682e-02 5.0461572e-02\n",
      " 3.3166632e-02 3.2236326e-02 2.2634419e-02]\n",
      "iter 65\ttrain cost\t0.3146195709705353\ttotal variance to retain\t6.306171417236328e-05\n",
      "Epoch 65\n",
      "Iter 72600: loss: 0.3097\n",
      "\n",
      "[3.2024822e+01 4.1227961e+00 9.8224759e-02 7.3397554e-02 6.8673283e-02\n",
      " 5.7391010e-02 4.7278419e-02 4.5151416e-02 3.8269352e-02 3.3289172e-02\n",
      " 3.2778032e-02 2.4951203e-02 2.3012346e-02]\n",
      "iter 66\ttrain cost\t0.30968335270881653\ttotal variance to retain\t3.081560134887695e-05\n",
      "Epoch 66\n",
      "Iter 73700: loss: 0.3108\n",
      "\n",
      "[3.0326212e+01 4.5278349e+00 9.1055997e-02 8.3898515e-02 6.6271320e-02\n",
      " 5.8710832e-02 5.2719861e-02 4.8020564e-02 4.5911741e-02 4.0211543e-02\n",
      " 3.5820186e-02 3.3287652e-02 3.0067191e-02]\n",
      "iter 67\ttrain cost\t0.3107927441596985\ttotal variance to retain\t3.7550926208496094e-05\n",
      "Epoch 67\n",
      "Iter 74800: loss: 0.3087\n",
      "\n",
      "[3.2244152e+01 4.1937947e+00 8.6894192e-02 8.4355660e-02 7.3708452e-02\n",
      " 6.0117979e-02 5.0039925e-02 4.5856010e-02 4.1432973e-02 3.8330596e-02\n",
      " 3.0325444e-02 2.3238076e-02 2.0319790e-02]\n",
      "iter 68\ttrain cost\t0.3086751699447632\ttotal variance to retain\t3.153085708618164e-05\n",
      "Epoch 68\n",
      "Iter 75900: loss: 0.3050\n",
      "\n",
      "[3.0344706e+01 4.3558264e+00 8.3472699e-02 7.1180895e-02 6.5496579e-02\n",
      " 6.3113235e-02 5.8924552e-02 4.2553555e-02 3.9226420e-02 3.6662888e-02\n",
      " 3.0573826e-02 2.7909342e-02 2.4053747e-02]\n",
      "iter 69\ttrain cost\t0.30498579144477844\ttotal variance to retain\t3.266334533691406e-05\n",
      "Epoch 69\n",
      "Iter 77000: loss: 0.3071\n",
      "\n",
      "[2.8686865e+01 4.5225558e+00 1.0295139e-01 8.8643298e-02 7.6700911e-02\n",
      " 6.9773242e-02 5.9211884e-02 5.5991340e-02 4.1303605e-02 3.6014516e-02\n",
      " 3.4244888e-02 3.2283790e-02 2.8301375e-02]\n",
      "iter 70\ttrain cost\t0.3070833683013916\ttotal variance to retain\t4.9591064453125e-05\n",
      "Epoch 70\n",
      "Iter 78100: loss: 0.3042\n",
      "\n",
      "[3.4245323e+01 4.3311892e+00 1.1935555e-01 8.2805179e-02 7.3965177e-02\n",
      " 6.5150939e-02 5.5080086e-02 5.0786648e-02 4.4582870e-02 3.5996333e-02\n",
      " 3.2133911e-02 2.9829234e-02 2.3710124e-02]\n",
      "iter 71\ttrain cost\t0.3041774034500122\ttotal variance to retain\t3.546476364135742e-05\n",
      "Epoch 71\n",
      "Iter 79200: loss: 0.3031\n",
      "\n",
      "[3.2510239e+01 4.5408511e+00 9.9956356e-02 8.2890399e-02 6.5259591e-02\n",
      " 5.9399892e-02 5.1500082e-02 4.4603590e-02 3.9669756e-02 3.7697822e-02\n",
      " 3.2883663e-02 2.6903551e-02 2.6043320e-02]\n",
      "iter 72\ttrain cost\t0.3030976355075836\ttotal variance to retain\t3.24249267578125e-05\n",
      "Epoch 72\n",
      "Iter 80300: loss: 0.3020\n",
      "\n",
      "[3.2909447e+01 4.3914876e+00 1.3137218e-01 1.0619296e-01 7.6308399e-02\n",
      " 6.7477606e-02 5.9873473e-02 5.1474463e-02 4.5223609e-02 3.6814019e-02\n",
      " 3.4199189e-02 2.5148118e-02 2.2254201e-02]\n",
      "iter 73\ttrain cost\t0.3019667863845825\ttotal variance to retain\t4.6193599700927734e-05\n",
      "Epoch 73\n",
      "Iter 81400: loss: 0.3020\n",
      "\n",
      "[3.45063095e+01 4.43435383e+00 1.17321014e-01 7.98785016e-02\n",
      " 7.33840242e-02 6.80194870e-02 5.52507825e-02 4.79575507e-02\n",
      " 4.45488617e-02 3.95924635e-02 3.66262384e-02 2.80202702e-02\n",
      " 2.18363479e-02]\n",
      "iter 74\ttrain cost\t0.3019725978374481\ttotal variance to retain\t3.439188003540039e-05\n",
      "Epoch 74\n",
      "Iter 82500: loss: 0.2997\n",
      "\n",
      "[3.2284946e+01 4.1010985e+00 1.0649774e-01 6.7545071e-02 5.6269620e-02\n",
      " 5.0287962e-02 4.0624019e-02 3.9295174e-02 3.6826123e-02 3.2909088e-02\n",
      " 3.1841964e-02 2.8614145e-02 2.3483386e-02]\n",
      "iter 75\ttrain cost\t0.29973217844963074\ttotal variance to retain\t2.8014183044433594e-05\n",
      "Epoch 75\n",
      "Iter 83600: loss: 0.3000\n",
      "\n",
      "[3.3333305e+01 4.3163323e+00 8.0186836e-02 7.2443567e-02 6.7944951e-02\n",
      " 6.1084956e-02 5.3719401e-02 4.8475571e-02 3.9444044e-02 3.0718340e-02\n",
      " 2.7337039e-02 2.4882758e-02 1.8565502e-02]\n",
      "iter 76\ttrain cost\t0.29998573660850525\ttotal variance to retain\t2.6166439056396484e-05\n",
      "Epoch 76\n",
      "Iter 84700: loss: 0.2977\n",
      "\n",
      "[3.5112675e+01 3.9253516e+00 9.7493663e-02 7.9499662e-02 6.2940344e-02\n",
      " 5.5515166e-02 5.2877828e-02 4.4343948e-02 4.2509142e-02 2.9785210e-02\n",
      " 2.7643805e-02 2.2690589e-02 2.0616148e-02]\n",
      "iter 77\ttrain cost\t0.29772964119911194\ttotal variance to retain\t2.562999725341797e-05\n",
      "Epoch 77\n",
      "Iter 85800: loss: 0.3007\n",
      "\n",
      "[3.6525150e+01 4.1229196e+00 8.5339144e-02 8.0271348e-02 7.7592224e-02\n",
      " 7.2081804e-02 5.4062240e-02 4.7096945e-02 4.3251965e-02 3.6961772e-02\n",
      " 2.9907439e-02 2.9337279e-02 2.4214588e-02]\n",
      "iter 78\ttrain cost\t0.30073869228363037\ttotal variance to retain\t2.6285648345947266e-05\n",
      "Epoch 78\n",
      "Iter 86900: loss: 0.2959\n",
      "\n",
      "[3.6633923e+01 4.8656750e+00 7.3503077e-02 6.4605623e-02 5.9864707e-02\n",
      " 5.8253046e-02 5.2799001e-02 4.5166314e-02 3.8201928e-02 3.4841504e-02\n",
      " 3.1058483e-02 2.5329137e-02 2.3184814e-02]\n",
      "iter 79\ttrain cost\t0.29585883021354675\ttotal variance to retain\t1.913309097290039e-05\n",
      "Epoch 79\n",
      "Iter 88000: loss: 0.2957\n",
      "\n",
      "[3.3773937e+01 4.9779058e+00 9.9447317e-02 8.6466961e-02 7.0169881e-02\n",
      " 5.3719539e-02 5.1222283e-02 4.3657955e-02 4.0516026e-02 3.4182824e-02\n",
      " 3.1266090e-02 2.8902804e-02 2.1291900e-02]\n",
      "iter 80\ttrain cost\t0.2956991493701935\ttotal variance to retain\t2.9861927032470703e-05\n",
      "Epoch 80\n",
      "Iter 89100: loss: 0.2969\n",
      "\n",
      "[3.5787037e+01 4.6906004e+00 1.0996750e-01 7.7661283e-02 6.2587477e-02\n",
      " 5.7887305e-02 4.9126089e-02 4.0870924e-02 2.9530773e-02 2.8207671e-02\n",
      " 2.5000094e-02 2.2723557e-02 2.1080527e-02]\n",
      "iter 81\ttrain cost\t0.29694151878356934\ttotal variance to retain\t2.5093555450439453e-05\n",
      "Epoch 81\n",
      "Iter 90200: loss: 0.2952\n",
      "\n",
      "[3.6286812e+01 3.9730418e+00 7.2205819e-02 6.9435306e-02 6.1899554e-02\n",
      " 5.1067226e-02 4.3481097e-02 3.7252773e-02 3.2855924e-02 3.0665334e-02\n",
      " 2.5156526e-02 2.2434767e-02 1.6897287e-02]\n",
      "iter 82\ttrain cost\t0.295157253742218\ttotal variance to retain\t1.7404556274414062e-05\n",
      "Epoch 82\n",
      "Iter 91300: loss: 0.2910\n",
      "\n",
      "[3.7110512e+01 4.9184031e+00 1.2678610e-01 7.2655253e-02 6.7091323e-02\n",
      " 6.2093794e-02 4.4629496e-02 4.1492540e-02 3.9349932e-02 3.4893434e-02\n",
      " 2.8722387e-02 2.5674360e-02 2.2600342e-02]\n",
      "iter 83\ttrain cost\t0.29096323251724243\ttotal variance to retain\t2.7239322662353516e-05\n",
      "Epoch 83\n",
      "Iter 92400: loss: 0.2939\n",
      "\n",
      "[3.6716366e+01 4.5672445e+00 6.9216959e-02 6.5884329e-02 5.3945225e-02\n",
      " 4.7164433e-02 4.1096393e-02 3.9420344e-02 3.3018917e-02 3.1600308e-02\n",
      " 3.0192737e-02 2.5582425e-02 2.0550471e-02]\n",
      "iter 84\ttrain cost\t0.2938559353351593\ttotal variance to retain\t1.5676021575927734e-05\n",
      "Epoch 84\n",
      "Iter 93500: loss: 0.2911\n",
      "\n",
      "[3.8763622e+01 4.7229075e+00 8.1216060e-02 7.6480485e-02 6.1331406e-02\n",
      " 5.0123814e-02 4.3224402e-02 3.5371721e-02 3.1965658e-02 2.8151549e-02\n",
      " 2.4635974e-02 2.0822611e-02 1.8446680e-02]\n",
      "iter 85\ttrain cost\t0.2911447286605835\ttotal variance to retain\t1.6391277313232422e-05\n",
      "Epoch 85\n",
      "Iter 94600: loss: 0.2915\n",
      "\n",
      "[3.7119869e+01 3.9270546e+00 1.1773691e-01 9.0030000e-02 5.6780744e-02\n",
      " 5.2192826e-02 4.9684376e-02 4.3224454e-02 4.0218204e-02 3.5335675e-02\n",
      " 3.1044492e-02 2.9040424e-02 2.5313262e-02]\n",
      "iter 86\ttrain cost\t0.29151880741119385\ttotal variance to retain\t2.6881694793701172e-05\n",
      "Epoch 86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 95700: loss: 0.2880\n",
      "\n",
      "[3.7843834e+01 4.5017910e+00 9.9407546e-02 7.1558684e-02 5.5395581e-02\n",
      " 5.0702270e-02 4.1920528e-02 3.8633756e-02 3.4843329e-02 2.8262980e-02\n",
      " 2.4408275e-02 2.4245633e-02 1.8116925e-02]\n",
      "iter 87\ttrain cost\t0.2880011796951294\ttotal variance to retain\t1.901388168334961e-05\n",
      "Epoch 87\n",
      "Iter 96800: loss: 0.2876\n",
      "\n",
      "[3.8425575e+01 4.7005563e+00 6.7439355e-02 6.3901976e-02 5.5045571e-02\n",
      " 4.5110397e-02 4.1749593e-02 3.7656423e-02 3.1651452e-02 2.9796749e-02\n",
      " 2.6703807e-02 2.5180154e-02 1.7762261e-02]\n",
      "iter 88\ttrain cost\t0.28760531544685364\ttotal variance to retain\t1.3589859008789062e-05\n",
      "Epoch 88\n",
      "Iter 97900: loss: 0.2924\n",
      "\n",
      "[3.7033035e+01 5.0383539e+00 6.7387320e-02 6.5423280e-02 5.3016804e-02\n",
      " 4.2706329e-02 4.1185290e-02 3.6643524e-02 3.1660225e-02 2.8882442e-02\n",
      " 2.5280286e-02 2.4115831e-02 2.2497622e-02]\n",
      "iter 89\ttrain cost\t0.29242226481437683\ttotal variance to retain\t1.430511474609375e-05\n",
      "Epoch 89\n",
      "Iter 99000: loss: 0.2880\n",
      "\n",
      "[3.9653172e+01 4.2228394e+00 1.0059174e-01 6.6314489e-02 5.0369669e-02\n",
      " 4.3725811e-02 4.1409843e-02 3.8252104e-02 3.2302421e-02 2.4589812e-02\n",
      " 2.1398855e-02 1.8389648e-02 1.5629021e-02]\n",
      "iter 90\ttrain cost\t0.2879590690135956\ttotal variance to retain\t1.5676021575927734e-05\n",
      "Epoch 90\n",
      "Iter 100100: loss: 0.2880\n",
      "\n",
      "[3.9703251e+01 4.6765604e+00 7.6245710e-02 5.8633436e-02 5.5363730e-02\n",
      " 4.9048450e-02 4.6656638e-02 3.7047397e-02 2.9932285e-02 2.5756793e-02\n",
      " 2.2211093e-02 2.1997705e-02 1.5867095e-02]\n",
      "iter 91\ttrain cost\t0.2879885733127594\ttotal variance to retain\t1.3113021850585938e-05\n",
      "Epoch 91\n",
      "Iter 101200: loss: 0.2892\n",
      "\n",
      "[3.8958996e+01 4.3388972e+00 7.5512722e-02 5.9865125e-02 5.7629600e-02\n",
      " 4.9031369e-02 4.3504987e-02 4.0390156e-02 3.8877461e-02 3.0449398e-02\n",
      " 2.7281916e-02 2.3647899e-02 2.0269088e-02]\n",
      "iter 92\ttrain cost\t0.2892151474952698\ttotal variance to retain\t1.4781951904296875e-05\n",
      "Epoch 92\n",
      "Iter 102300: loss: 0.2877\n",
      "\n",
      "[3.9611790e+01 4.6599660e+00 6.5786324e-02 6.0458288e-02 5.5007149e-02\n",
      " 4.4299897e-02 3.7576724e-02 3.3923723e-02 3.0154763e-02 2.4986723e-02\n",
      " 2.3981201e-02 1.9602757e-02 1.5932277e-02]\n",
      "iter 93\ttrain cost\t0.28768590092658997\ttotal variance to retain\t1.1444091796875e-05\n",
      "Epoch 93\n",
      "Iter 103400: loss: 0.2806\n",
      "\n",
      "[3.9969299e+01 4.1052208e+00 7.0143268e-02 6.2778287e-02 5.6565966e-02\n",
      " 4.6284974e-02 3.9912175e-02 3.5301294e-02 3.0637473e-02 2.8452776e-02\n",
      " 2.2848295e-02 1.9649345e-02 1.6542327e-02]\n",
      "iter 94\ttrain cost\t0.28064367175102234\ttotal variance to retain\t1.239776611328125e-05\n",
      "Epoch 94\n",
      "Iter 104500: loss: 0.2841\n",
      "\n",
      "[3.9970528e+01 4.6653810e+00 6.9716215e-02 5.5302031e-02 4.7153141e-02\n",
      " 4.1924134e-02 4.0201273e-02 3.1962622e-02 3.0504452e-02 2.5345426e-02\n",
      " 2.3632400e-02 2.0517182e-02 1.6796190e-02]\n",
      "iter 95\ttrain cost\t0.2841295897960663\ttotal variance to retain\t1.0728836059570312e-05\n",
      "Epoch 95\n",
      "Iter 105600: loss: 0.2850\n",
      "\n",
      "[4.0354164e+01 4.0116792e+00 1.1539374e-01 7.4197352e-02 5.9981097e-02\n",
      " 5.0604586e-02 4.6228189e-02 4.3921202e-02 3.6984041e-02 3.0668903e-02\n",
      " 2.7191095e-02 2.3555653e-02 2.0968031e-02]\n",
      "iter 96\ttrain cost\t0.2850233018398285\ttotal variance to retain\t2.0205974578857422e-05\n",
      "Epoch 96\n",
      "Iter 106700: loss: 0.2850\n",
      "\n",
      "[4.0296486e+01 5.4574361e+00 6.5957345e-02 5.4975647e-02 4.8813440e-02\n",
      " 4.5157757e-02 3.9881252e-02 3.8042620e-02 3.4760274e-02 3.0511467e-02\n",
      " 2.7357331e-02 2.2625638e-02 1.8012872e-02]\n",
      "iter 97\ttrain cost\t0.28497037291526794\ttotal variance to retain\t1.1205673217773438e-05\n",
      "Epoch 97\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "scores3 = []\n",
    "for part in range(9):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((_train_x[0:(part*33)], _train_x[(part+1)*33:303]), axis=0)\n",
    "    train_y = np.concatenate((_train_y[0:(part*33)], _train_y[(part+1)*33:303]), axis=0)\n",
    "    test_x = _train_x[part*33:(part+1)*33]\n",
    "    test_y = _train_y[part*33:(part+1)*33]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    transpose_train_x = np.transpose(train_x)\n",
    "\n",
    "    cur_w = np.random.normal(0, 0.35, (number_of_neurons,1))\n",
    "    cur_B =  np.random.normal(0, 0.35, (dimensionality, number_of_neurons))\n",
    "    cur_biases = np.zeros((number_of_neurons))\n",
    "    cur_P = np.zeros((dimensionality, dimensionality)) \n",
    "    cur_w_regr = np.random.normal(0, 0.35, (number_of_neurons_regr,1))\n",
    "    cur_B_regr = np.random.normal(0, 0.35, (dimensionality, number_of_neurons_regr))\n",
    "    cur_bias_regr = np.random.normal(0, 0.35, (number_of_neurons_regr))\n",
    "    \n",
    "    O = np.zeros((dimensionality, k)) \n",
    "    cur_iter = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init, feed_dict={Dataplace: transpose_train_x})\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" %(epoch))\n",
    "        for iteration in range(num_iters):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            sess.run(target, feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x,\n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "        reses = []\n",
    "        for i in range(100):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            res = sess.run([loss_hybrid], feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x, \n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "            reses.append(res)\n",
    "        print (\"Iter %d: loss: %.4f\\n\" %(cur_iter, np.mean(np.array(reses))))\n",
    "        lib.plot.plot('train cost', np.mean(np.array(reses)))\n",
    "\n",
    "        cur_w, cur_B, cur_w_regr, cur_B_regr, cur_bias_regr = sess.run([w, B, w_regr, B_regr, bias_regr])\n",
    "\n",
    "        third_grad_psi = np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))\n",
    "        third_grad_psi_regr = np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))\n",
    "        for r in range(1000):\n",
    "            sess.run([tf_data_second])\n",
    "            np.concatenate((third_grad_psi, np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))), axis=0) \n",
    "            np.concatenate((third_grad_psi_regr, np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))), axis=0)\n",
    "\n",
    "        M = (1-rho)*np.matmul(np.transpose(third_grad_psi), third_grad_psi)+\\\n",
    "            rho*np.matmul(np.transpose(third_grad_psi_regr), third_grad_psi_regr)\n",
    "        u, s, vh = np.linalg.svd(M, full_matrices=True)\n",
    "        O = u[:,0:k:1]\n",
    "        print(s)\n",
    "        cur_P = np.matmul(O, np.transpose(O))\n",
    "        tvr = 1-np.sum(np.multiply(s[0:k],s[0:k]))/np.sum(np.multiply(s,s))\n",
    "        lib.plot.plot('total variance to retain', tvr)\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "    reduced_train_x = np.matmul(train_x, O)\n",
    "    reduced_test_x = np.matmul(test_x, O)\n",
    "    clf = LogisticRegression(random_state=0).fit(reduced_train_x, train_y)\n",
    "    score = clf.score(reduced_test_x, test_y)\n",
    "    scores.append(score)\n",
    "    print (\"Part %d: rate on test %.4f\\n\" %(part, score))\n",
    "    test_rgb = []\n",
    "    for h in test_y:\n",
    "        if h == 1:\n",
    "            test_rgb.append('r')\n",
    "        else:\n",
    "            test_rgb.append('b')\n",
    "    plt2.close()\n",
    "    plt2.scatter(np.transpose(reduced_test_x)[0], np.transpose(reduced_test_x)[1], alpha=0.2, c=np.array(test_rgb))\n",
    "    plt2.xlabel(\"1st component\")\n",
    "    plt2.ylabel(\"2nd component\")\n",
    "    plt2.show()\n",
    "    sess.close()\n",
    "    print(cur_P)\n",
    "    \n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict(10, reduced_train_x, train_y, test_image)\n",
    "        if pred == test_y[i]:\n",
    "            total_correct += 1\n",
    "        score3 = (total_correct / (i+1)) * 100            \n",
    "        i += 1\n",
    "    print('acc:', str(round(score3, 2))+'%')\n",
    "    scores3.append(score3)\n",
    "\n",
    "print (\"Average rate on test %.4f\\n\" %(np.mean(np.array(scores))))\n",
    "print (\"Average 10-NN acc on test %.4f\\n\" %(np.mean(np.array(scores3))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
