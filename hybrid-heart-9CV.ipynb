{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "F1Y_8jYdjORy",
    "outputId": "36ff5c4b-0fc7-4bb1-e76f-f901264c8989"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import scipy as scipy\n",
    "import scipy.interpolate as interpolate\n",
    "import math\n",
    "from random import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "Lambda = 10.0\n",
    "BATCH_SIZE = 30\n",
    "BATCH_SIZE_2 = 100\n",
    "dimensionality = 13\n",
    "num_epochs = 100\n",
    "num_iters = 1000\n",
    "k=2\n",
    "max_grad_norm = 1000\n",
    "gamma = 1.0\n",
    "\n",
    "# main parameters\n",
    "\n",
    "rho = 0.5\n",
    "C=10.0\n",
    "\n",
    "# additional parameters\n",
    "sigma=0.8\n",
    "\n",
    "number_of_neurons_regr = 100\n",
    "number_of_points = 303-33\n",
    "number_of_neurons = number_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5ExlL7s3o9S6",
    "outputId": "82a79e11-a546-4fa7-81a1-de76f5b0011d"
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv('/home/rust/Desktop/SDR/heart.csv')\n",
    "X = heart.iloc[:,:-1].values\n",
    "y = heart.iloc[:,13].values\n",
    "print(len(y))\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(303)\n",
    "for x in rr:\n",
    "    data.append(X[x])\n",
    "    target.append(y[x])\n",
    "# training data\n",
    "_train_x = np.array(data)\n",
    "_train_y = np.array(target)\n",
    "print(_train_x.shape)\n",
    "print(_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvKNXjixjm-A"
   },
   "outputs": [],
   "source": [
    "new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "Dataplace = tf.placeholder(tf.float32, shape=(dimensionality, number_of_points))\n",
    "Data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, dimensionality))\n",
    "outputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE,1)) \n",
    "\n",
    "tf_data_x = gamma*tf.random_normal([BATCH_SIZE, dimensionality]) # аргументы функции\n",
    "tf_data_y = tf.reduce_mean(tf.math.cos(tf.matmul(tf_data_x, Dataplace)), axis=1) # значения функции\n",
    "\n",
    "tf_data_w = tf.placeholder(tf.float32, shape=(number_of_neurons,1))\n",
    "tf_data_B = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons))\n",
    "tf_data_P = tf.placeholder(tf.float32, shape=(dimensionality, dimensionality))\n",
    "\n",
    "tf_w_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr,1))\n",
    "tf_B_regr = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons_regr))\n",
    "tf_bias_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr))\n",
    "\n",
    "\n",
    "tf_data_second = gamma*tf.random_normal([BATCH_SIZE_2, dimensionality])\n",
    "Lbd = tf.placeholder(tf.float32, shape=[], name=\"lambda\")\n",
    "\n",
    "# characteristic function parameters\n",
    "w = tf.Variable(tf.random_normal([number_of_neurons,1], stddev=0.0), name=\"neuron_weights\")\n",
    "B = tf.Variable(initial_value=Dataplace, name=\"weights\")\n",
    "# regression function parameters\n",
    "w_regr = tf.Variable(tf.random_normal([number_of_neurons_regr,1], stddev=0.35), name=\"neuron_weights\")\n",
    "B_regr = tf.Variable(tf.random_normal([dimensionality,number_of_neurons_regr], stddev=0.35), name=\"weights\")\n",
    "bias_regr = tf.Variable(tf.random_normal([number_of_neurons_regr], stddev=0.0), name=\"biases\")\n",
    "\n",
    "prediction = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_x, B)), tf.nn.sigmoid(w))\n",
    "penalty = tf.square((2/number_of_neurons)*tf.reduce_sum(tf.nn.sigmoid(w))-1.0)\n",
    "\n",
    "out_loss = (1-rho)*tf.reduce_mean(tf.square(prediction - tf_data_y)) + C*penalty\n",
    "\n",
    "prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, B)), tf.nn.sigmoid(w))\n",
    "prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, B_regr), bias_regr)), w_regr)\n",
    "\n",
    "grad_psi = tf.reshape(tf.gradients(prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "grad_psi_regr = tf.reshape(tf.gradients(prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "tf_prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, tf_data_B)), tf.nn.sigmoid(tf_data_w))\n",
    "tf_prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, tf_B_regr), tf_bias_regr)), tf_w_regr)\n",
    "\n",
    "tf_data_grad_psi = tf.reshape(tf.gradients(tf_prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "tf_data_grad_psi_regr = tf.reshape(tf.gradients(tf_prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "old_part = tf.matmul(tf_data_grad_psi, tf_data_P)\n",
    "old_part_regr = tf.matmul(tf_data_grad_psi_regr, tf_data_P)\n",
    "\n",
    "loss = out_loss + Lbd*(1-rho)*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi, old_part)), axis=1)) + Lbd*rho*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi_regr, old_part_regr)), axis=1))\n",
    "\n",
    "#offset = tf.random.uniform(shape=[], minval=0, maxval=12, dtype=tf.int32)*BATCH_SIZE\n",
    "x_plus_error = Data+sigma*tf.random_normal([BATCH_SIZE, dimensionality])\n",
    "regression = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(x_plus_error, B_regr), bias_regr)), w_regr)\n",
    "sdr = -tf.reduce_mean(tf.multiply(regression, outputs)) + tf.reduce_mean(tf.math.log(1.0+tf.math.exp(regression)))\n",
    "\n",
    "loss_hybrid = loss + rho*sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pnt1, pnt2):\n",
    "    '''Finds the distance between 2 points: pnt1, pnt2'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((pnt1 - pnt2) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "\n",
    "def new_predict(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "def new_predict_regr(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return np.mean(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3HlgJ8XdjtxY",
    "outputId": "8ec5f3f0-92f7-4712-94c2-e9bff821b918"
   },
   "outputs": [],
   "source": [
    "target = tf.train.AdamOptimizer(learning_rate=3e-5, beta1=0.5, beta2=0.9).minimize(loss_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YojZRg6lMgur"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1. /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HLqISRILj5VQ",
    "outputId": "83373660-15c5-4d88-a2da-ec9bcf3a5a56",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores3 = []\n",
    "for part in range(9):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((_train_x[0:(part*33)], _train_x[(part+1)*33:303]), axis=0)\n",
    "    train_y = np.concatenate((_train_y[0:(part*33)], _train_y[(part+1)*33:303]), axis=0)\n",
    "    test_x = _train_x[part*33:(part+1)*33]\n",
    "    test_y = _train_y[part*33:(part+1)*33]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    transpose_train_x = np.transpose(train_x)\n",
    "\n",
    "    cur_w = np.random.normal(0, 0.35, (number_of_neurons,1))\n",
    "    cur_B =  np.random.normal(0, 0.35, (dimensionality, number_of_neurons))\n",
    "    cur_biases = np.zeros((number_of_neurons))\n",
    "    cur_P = np.zeros((dimensionality, dimensionality)) \n",
    "    cur_w_regr = np.random.normal(0, 0.35, (number_of_neurons_regr,1))\n",
    "    cur_B_regr = np.random.normal(0, 0.35, (dimensionality, number_of_neurons_regr))\n",
    "    cur_bias_regr = np.random.normal(0, 0.35, (number_of_neurons_regr))\n",
    "    \n",
    "    O = np.zeros((dimensionality, k)) \n",
    "    cur_iter = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init, feed_dict={Dataplace: transpose_train_x})\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" %(epoch))\n",
    "        for iteration in range(num_iters):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            sess.run(target, feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x,\n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "        reses = []\n",
    "        for i in range(100):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            res = sess.run([loss_hybrid], feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x, \n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "            reses.append(res)\n",
    "        print (\"Iter %d: loss: %.4f\\n\" %(cur_iter, np.mean(np.array(reses))))\n",
    "        lib.plot.plot('train cost', np.mean(np.array(reses)))\n",
    "\n",
    "        cur_w, cur_B, cur_w_regr, cur_B_regr, cur_bias_regr = sess.run([w, B, w_regr, B_regr, bias_regr])\n",
    "\n",
    "        third_grad_psi = np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))\n",
    "        third_grad_psi_regr = np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))\n",
    "        for r in range(1000):\n",
    "            sess.run([tf_data_second])\n",
    "            np.concatenate((third_grad_psi, np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))), axis=0) \n",
    "            np.concatenate((third_grad_psi_regr, np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))), axis=0)\n",
    "\n",
    "        M = (1-rho)*np.matmul(np.transpose(third_grad_psi), third_grad_psi)+\\\n",
    "            rho*np.matmul(np.transpose(third_grad_psi_regr), third_grad_psi_regr)\n",
    "        u, s, vh = np.linalg.svd(M, full_matrices=True)\n",
    "        O = u[:,0:k:1]\n",
    "        print(s)\n",
    "        cur_P = np.matmul(O, np.transpose(O))\n",
    "        tvr = 1-np.sum(np.multiply(s[0:k],s[0:k]))/np.sum(np.multiply(s,s))\n",
    "        lib.plot.plot('total variance to retain', tvr)\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "    reduced_train_x = np.matmul(train_x, O)\n",
    "    reduced_test_x = np.matmul(test_x, O)\n",
    "    clf = LogisticRegression(random_state=0).fit(reduced_train_x, train_y)\n",
    "    score = clf.score(reduced_test_x, test_y)\n",
    "    scores.append(score)\n",
    "    print (\"Part %d: rate on test %.4f\\n\" %(part, score))\n",
    "    test_rgb = []\n",
    "    for h in test_y:\n",
    "        if h == 1:\n",
    "            test_rgb.append('r')\n",
    "        else:\n",
    "            test_rgb.append('b')\n",
    "    plt2.close()\n",
    "    plt2.scatter(np.transpose(reduced_test_x)[0], np.transpose(reduced_test_x)[1], alpha=0.2, c=np.array(test_rgb))\n",
    "    plt2.xlabel(\"1st component\")\n",
    "    plt2.ylabel(\"2nd component\")\n",
    "    plt2.show()\n",
    "    sess.close()\n",
    "    print(cur_P)\n",
    "    \n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict(10, reduced_train_x, train_y, test_image)\n",
    "        if pred == test_y[i]:\n",
    "            total_correct += 1\n",
    "        score3 = (total_correct / (i+1)) * 100            \n",
    "        i += 1\n",
    "    print('acc:', str(round(score3, 2))+'%')\n",
    "    scores3.append(score3)\n",
    "\n",
    "print (\"Average rate on test %.4f\\n\" %(np.mean(np.array(scores))))\n",
    "print (\"Average 10-NN acc on test %.4f\\n\" %(np.mean(np.array(scores3))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
