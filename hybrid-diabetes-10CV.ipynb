{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "F1Y_8jYdjORy",
    "outputId": "36ff5c4b-0fc7-4bb1-e76f-f901264c8989"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import scipy as scipy\n",
    "import scipy.interpolate as interpolate\n",
    "import math\n",
    "from random import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_diabetes\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "Lambda = 10.0\n",
    "BATCH_SIZE = 45\n",
    "BATCH_SIZE_2 = 100\n",
    "dimensionality = 10\n",
    "num_epochs = 100\n",
    "num_iters = 1000\n",
    "k=2\n",
    "max_grad_norm = 1000\n",
    "gamma = 1.0\n",
    "\n",
    "# main parameters\n",
    "\n",
    "C=10.0\n",
    "\n",
    "# additional parameters\n",
    "sigma=0.8\n",
    "\n",
    "number_of_neurons_regr = 50\n",
    "number_of_points = 442-37\n",
    "number_of_neurons = number_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5ExlL7s3o9S6",
    "outputId": "82a79e11-a546-4fa7-81a1-de76f5b0011d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n",
      "346.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes(return_X_y=False)\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(442)\n",
    "for x in rr:\n",
    "    data.append(diabetes['data'][x])\n",
    "    target.append(diabetes['target'][x])\n",
    "# training data\n",
    "_train_x = np.array(data)\n",
    "_train_y = np.array(target)\n",
    "print(_train_x.shape)\n",
    "print(_train_y.shape)\n",
    "print(np.max(_train_y))\n",
    "rho = 0.5\n",
    "var = np.var(_train_y)\n",
    "print(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvKNXjixjm-A"
   },
   "outputs": [],
   "source": [
    "new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "Dataplace = tf.placeholder(tf.float32, shape=(dimensionality, number_of_points))\n",
    "Data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, dimensionality))\n",
    "outputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE,1)) \n",
    "\n",
    "tf_data_x = gamma*tf.random_normal([BATCH_SIZE, dimensionality]) # аргументы функции\n",
    "tf_data_y = tf.reduce_mean(tf.math.cos(tf.matmul(tf_data_x, Dataplace)), axis=1) # значения функции\n",
    "\n",
    "tf_data_w = tf.placeholder(tf.float32, shape=(number_of_neurons,1))\n",
    "tf_data_B = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons))\n",
    "tf_data_P = tf.placeholder(tf.float32, shape=(dimensionality, dimensionality))\n",
    "\n",
    "tf_w_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr,1))\n",
    "tf_B_regr = tf.placeholder(tf.float32, shape=(dimensionality,number_of_neurons_regr))\n",
    "tf_bias_regr = tf.placeholder(tf.float32, shape=(number_of_neurons_regr))\n",
    "\n",
    "\n",
    "tf_data_second = gamma*tf.random_normal([BATCH_SIZE_2, dimensionality])\n",
    "Lbd = tf.placeholder(tf.float32, shape=[], name=\"lambda\")\n",
    "\n",
    "# characteristic function parameters\n",
    "w = tf.Variable(tf.random_normal([number_of_neurons,1], stddev=0.0), name=\"neuron_weights\")\n",
    "B = tf.Variable(initial_value=Dataplace, name=\"weights\")\n",
    "# regression function parameters\n",
    "w_regr = tf.Variable(tf.random_normal([number_of_neurons_regr,1], stddev=0.35), name=\"neuron_weights\")\n",
    "B_regr = tf.Variable(tf.random_normal([dimensionality,number_of_neurons_regr], stddev=0.35), name=\"weights\")\n",
    "bias_regr = tf.Variable(tf.random_normal([number_of_neurons_regr], stddev=0.0), name=\"biases\")\n",
    "\n",
    "prediction = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_x, B)), tf.nn.sigmoid(w))\n",
    "penalty = tf.square((2/number_of_neurons)*tf.reduce_sum(tf.nn.sigmoid(w))-1.0)\n",
    "\n",
    "out_loss = (1-rho)*tf.reduce_mean(tf.square(prediction - tf_data_y)) + C*penalty\n",
    "\n",
    "prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, B)), tf.nn.sigmoid(w))\n",
    "prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, B_regr), bias_regr)), w_regr)\n",
    "\n",
    "grad_psi = tf.reshape(tf.gradients(prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "grad_psi_regr = tf.reshape(tf.gradients(prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "tf_prediction2 = (2/number_of_neurons)*tf.matmul(tf.math.cos(tf.matmul(tf_data_second, tf_data_B)), tf.nn.sigmoid(tf_data_w))\n",
    "tf_prediction_regr2 = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(tf_data_second, tf_B_regr), tf_bias_regr)), tf_w_regr)\n",
    "\n",
    "tf_data_grad_psi = tf.reshape(tf.gradients(tf_prediction2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "tf_data_grad_psi_regr = tf.reshape(tf.gradients(tf_prediction_regr2, [tf_data_second])[0], [BATCH_SIZE_2, dimensionality])\n",
    "\n",
    "old_part = tf.matmul(tf_data_grad_psi, tf_data_P)\n",
    "old_part_regr = tf.matmul(tf_data_grad_psi_regr, tf_data_P)\n",
    "\n",
    "loss = out_loss + Lbd*(1-rho)*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi, old_part)), axis=1)) + \\\n",
    "         Lbd*rho*tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(grad_psi_regr, old_part_regr)), axis=1))/var\n",
    "\n",
    "#offset = tf.random.uniform(shape=[], minval=0, maxval=12, dtype=tf.int32)*BATCH_SIZE\n",
    "x_plus_error = Data+sigma*tf.random_normal([BATCH_SIZE, dimensionality])\n",
    "regression = tf.matmul(tf.nn.tanh(tf.nn.bias_add(tf.matmul(x_plus_error, B_regr), bias_regr)), w_regr)\n",
    "sdr = tf.reduce_mean(tf.square(regression-outputs))\n",
    "\n",
    "loss_hybrid = loss + rho*sdr/var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3HlgJ8XdjtxY",
    "outputId": "8ec5f3f0-92f7-4712-94c2-e9bff821b918"
   },
   "outputs": [],
   "source": [
    "target = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(loss_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YojZRg6lMgur"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1. /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pnt1, pnt2):\n",
    "    '''Finds the distance between 2 points: pnt1, pnt2'''\n",
    "    # element-wise computations are automatically handled by numpy\n",
    "    return sum((pnt1 - pnt2) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    '''Finds the majority class/label out of the given labels'''\n",
    "    # defaultdict(type) is to automatically add new keys without throwing error.\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    # Finding the majority class.\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "\n",
    "def new_predict(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "def new_predict_regr(k, train_pnts, train_labels, test_pnts):\n",
    "    '''\n",
    "    Predicts the new data-point's category/label by \n",
    "    looking at all other training labels\n",
    "    '''\n",
    "    # distances contains tuples of (distance, label)\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    # sort the distances list by distances\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    # extract only k closest labels\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    # return the majority voted label\n",
    "    return np.mean(k_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HLqISRILj5VQ",
    "outputId": "83373660-15c5-4d88-a2da-ec9bcf3a5a56",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current part is 0\n",
      "\n",
      "Epoch 0\n",
      "Iter 1100: loss: 2.7437\n",
      "\n",
      "iter 1\ttrain cost\t2.74365234375\n",
      "[134.6313       7.3156233    3.6041977    2.6069918    2.0494974\n",
      "   1.3444679    1.3105644    0.94723684   0.6575228    0.3257028 ]\n",
      "iter 2\ttotal variance to retain\t0.0015898346900939941\n",
      "Epoch 1\n",
      "Iter 2200: loss: 2.5736\n",
      "\n",
      "iter 3\ttrain cost\t2.5736265182495117\n",
      "[431.13464      9.700892     6.3684397    5.165432     3.9218736\n",
      "   2.5190513    1.9489843    1.2114486    0.75916314   0.54422545]\n",
      "iter 4\ttotal variance to retain\t0.00051116943359375\n",
      "Epoch 2\n",
      "Iter 3300: loss: 2.5001\n",
      "\n",
      "iter 5\ttrain cost\t2.500061273574829\n",
      "[957.8223     26.84641    13.200857    8.953437    7.388729    4.853086\n",
      "   2.857505    2.318367    1.2841369   1.1097121]\n",
      "iter 6\ttotal variance to retain\t0.00037997961044311523\n",
      "Epoch 3\n",
      "Iter 4400: loss: 2.4165\n",
      "\n",
      "iter 7\ttrain cost\t2.4165337085723877\n",
      "[1.6020767e+03 3.2738171e+01 1.7485039e+01 1.2366930e+01 9.9535522e+00\n",
      " 6.4865603e+00 4.1190543e+00 2.8242567e+00 2.2270827e+00 1.2961607e+00]\n",
      "iter 8\ttotal variance to retain\t0.00024586915969848633\n",
      "Epoch 4\n",
      "Iter 5500: loss: 2.3231\n",
      "\n",
      "iter 9\ttrain cost\t2.3230974674224854\n",
      "[2.1438992e+03 5.1548397e+01 2.1947063e+01 1.5986544e+01 1.2958025e+01\n",
      " 8.1862364e+00 3.6892207e+00 2.9865139e+00 2.4811087e+00 1.0182554e+00]\n",
      "iter 10\ttotal variance to retain\t0.00021773576736450195\n",
      "Epoch 5\n",
      "Iter 6600: loss: 2.2160\n",
      "\n",
      "iter 11\ttrain cost\t2.2159829139709473\n",
      "[2.60372656e+03 7.08338013e+01 2.17860947e+01 1.55107317e+01\n",
      " 1.35802555e+01 6.32668257e+00 3.99044299e+00 2.81709433e+00\n",
      " 1.66673303e+00 1.01904655e+00]\n",
      "iter 12\ttotal variance to retain\t0.00014257431030273438\n",
      "Epoch 6\n",
      "Iter 7700: loss: 2.0931\n",
      "\n",
      "iter 13\ttrain cost\t2.0931410789489746\n",
      "[2.4308469e+03 7.1729431e+01 2.3318268e+01 1.7826536e+01 1.6969320e+01\n",
      " 7.3926396e+00 3.4867282e+00 1.7881389e+00 8.9071786e-01 5.7699335e-01]\n",
      "iter 14\ttotal variance to retain\t0.00020641088485717773\n",
      "Epoch 7\n",
      "Iter 8800: loss: 1.9520\n",
      "\n",
      "iter 15\ttrain cost\t1.9519529342651367\n",
      "[2.2399451e+03 5.0506680e+01 2.1127174e+01 1.7974211e+01 1.2680700e+01\n",
      " 3.7160769e+00 1.5788245e+00 8.3584768e-01 4.8167279e-01 2.7889857e-01]\n",
      "iter 16\ttotal variance to retain\t0.00018870830535888672\n",
      "Epoch 8\n",
      "Iter 9900: loss: 1.8176\n",
      "\n",
      "iter 17\ttrain cost\t1.8176075220108032\n",
      "[1.92182080e+03 4.45104828e+01 1.37990513e+01 1.04134655e+01\n",
      " 4.92853832e+00 1.50766385e+00 7.36618638e-01 3.71019959e-01\n",
      " 2.52652854e-01 1.35505870e-01]\n",
      "iter 18\ttotal variance to retain\t8.821487426757812e-05\n",
      "Epoch 9\n",
      "Iter 11000: loss: 1.6826\n",
      "\n",
      "iter 19\ttrain cost\t1.6826034784317017\n",
      "[1.4509498e+03 2.8247700e+01 9.8493156e+00 4.5667915e+00 3.5601683e+00\n",
      " 6.3389504e-01 3.0306655e-01 1.1782272e-01 9.4896451e-02 8.4171481e-02]\n",
      "iter 20\ttotal variance to retain\t6.222724914550781e-05\n",
      "Epoch 10\n",
      "Iter 12100: loss: 1.5564\n",
      "\n",
      "iter 21\ttrain cost\t1.5564041137695312\n",
      "[1.0733386e+03 1.4130349e+01 6.2729831e+00 1.4790219e+00 7.7040750e-01\n",
      " 2.4498560e-01 1.7802997e-01 1.4612657e-01 9.9489368e-02 5.5863943e-02]\n",
      "iter 22\ttotal variance to retain\t3.6656856536865234e-05\n",
      "Epoch 11\n",
      "Iter 13200: loss: 1.4507\n",
      "\n",
      "iter 23\ttrain cost\t1.45068359375\n",
      "[9.44797913e+02 6.01932573e+00 2.96803856e+00 4.51916099e-01\n",
      " 2.28069112e-01 2.04530880e-01 1.38195664e-01 1.17064536e-01\n",
      " 6.17897585e-02 5.08299917e-02]\n",
      "iter 24\ttotal variance to retain\t1.0192394256591797e-05\n",
      "Epoch 12\n",
      "Iter 14300: loss: 1.3365\n",
      "\n",
      "iter 25\ttrain cost\t1.3364602327346802\n",
      "[7.8437598e+02 3.5171351e+00 9.4380724e-01 2.2389494e-01 1.7307891e-01\n",
      " 1.2657616e-01 9.3263522e-02 5.0038110e-02 4.5202315e-02 4.0351491e-02]\n",
      "iter 26\ttotal variance to retain\t1.6093254089355469e-06\n",
      "Epoch 13\n",
      "Iter 15400: loss: 1.2384\n",
      "\n",
      "iter 27\ttrain cost\t1.2383564710617065\n",
      "[5.41049011e+02 2.05113769e+00 3.68848979e-01 2.15304837e-01\n",
      " 1.06375754e-01 5.22890165e-02 4.58701923e-02 3.44743207e-02\n",
      " 3.21664698e-02 2.92079952e-02]\n",
      "iter 28\ttotal variance to retain\t7.748603820800781e-07\n",
      "Epoch 14\n",
      "Iter 16500: loss: 1.1484\n",
      "\n",
      "iter 29\ttrain cost\t1.1483845710754395\n",
      "[4.9194553e+02 8.3909976e-01 2.3750040e-01 1.6545081e-01 1.1918923e-01\n",
      " 9.6311621e-02 7.6092690e-02 6.1315805e-02 4.7105219e-02 2.2470992e-02]\n",
      "iter 30\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 15\n",
      "Iter 17600: loss: 1.0689\n",
      "\n",
      "iter 31\ttrain cost\t1.068881869316101\n",
      "[4.67215973e+02 4.54400271e-01 9.62311029e-02 7.97681063e-02\n",
      " 6.38693795e-02 5.64716123e-02 5.44236973e-02 4.45053801e-02\n",
      " 3.22647206e-02 1.01528885e-02]\n",
      "iter 32\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 16\n",
      "Iter 18700: loss: 0.9933\n",
      "\n",
      "iter 33\ttrain cost\t0.9933436512947083\n",
      "[3.9374927e+02 2.2118177e-01 9.8562315e-02 6.4324185e-02 5.6138989e-02\n",
      " 4.8577234e-02 4.3886729e-02 3.7260525e-02 2.3166589e-02 5.7340283e-03]\n",
      "iter 34\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 17\n",
      "Iter 19800: loss: 0.9241\n",
      "\n",
      "iter 35\ttrain cost\t0.9240864515304565\n",
      "[3.5454840e+02 3.8269654e-01 1.6270569e-01 1.1579915e-01 7.8190319e-02\n",
      " 5.2239452e-02 4.7434308e-02 3.4289330e-02 2.2893075e-02 8.0914348e-03]\n",
      "iter 36\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 18\n",
      "Iter 20900: loss: 0.8630\n",
      "\n",
      "iter 37\ttrain cost\t0.8630150556564331\n",
      "[4.0667001e+02 2.7763575e-01 2.1391463e-01 1.0282243e-01 9.7831577e-02\n",
      " 5.0159823e-02 3.6980778e-02 3.2492448e-02 2.7078977e-02 8.3672497e-03]\n",
      "iter 38\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 19\n",
      "Iter 22000: loss: 0.8157\n",
      "\n",
      "iter 39\ttrain cost\t0.8156701922416687\n",
      "[3.9936517e+02 2.0311141e-01 1.5547892e-01 1.0127191e-01 7.9149395e-02\n",
      " 6.9990464e-02 5.4042961e-02 3.7941329e-02 3.1704918e-02 9.4190557e-03]\n",
      "iter 40\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 20\n",
      "Iter 23100: loss: 0.7610\n",
      "\n",
      "iter 41\ttrain cost\t0.7609812021255493\n",
      "[4.0704388e+02 2.3822856e-01 9.9093020e-02 7.7531256e-02 6.9739953e-02\n",
      " 5.2244920e-02 3.7199527e-02 3.2278866e-02 2.0401312e-02 1.0165828e-02]\n",
      "iter 42\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 21\n",
      "Iter 24200: loss: 0.7148\n",
      "\n",
      "iter 43\ttrain cost\t0.7147902846336365\n",
      "[5.7938159e+02 2.0202124e-01 1.3084956e-01 8.3104432e-02 6.8774216e-02\n",
      " 5.0915085e-02 4.4163559e-02 4.0076807e-02 2.6410729e-02 7.6677250e-03]\n",
      "iter 44\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 22\n",
      "Iter 25300: loss: 0.6745\n",
      "\n",
      "iter 45\ttrain cost\t0.6745436191558838\n",
      "[6.1866949e+02 2.6723650e-01 9.0485334e-02 7.6856837e-02 5.5702124e-02\n",
      " 5.0103035e-02 3.5405021e-02 3.2833450e-02 2.3628386e-02 7.8420937e-03]\n",
      "iter 46\ttotal variance to retain\t0.0\n",
      "Epoch 23\n",
      "Iter 26400: loss: 0.6440\n",
      "\n",
      "iter 47\ttrain cost\t0.6439539194107056\n",
      "[4.8816583e+02 1.3961044e-01 8.1135817e-02 6.1702348e-02 4.7680974e-02\n",
      " 4.0713351e-02 3.1518288e-02 2.8200949e-02 2.3949064e-02 8.2089482e-03]\n",
      "iter 48\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 24\n",
      "Iter 27500: loss: 0.6124\n",
      "\n",
      "iter 49\ttrain cost\t0.6124036312103271\n",
      "[1.1645701e+03 2.1449265e-01 9.3957849e-02 7.7889822e-02 5.8614239e-02\n",
      " 5.3913873e-02 4.8684768e-02 2.4201874e-02 2.3232892e-02 9.9756690e-03]\n",
      "iter 50\ttotal variance to retain\t0.0\n",
      "Epoch 25\n",
      "Iter 28600: loss: 0.5826\n",
      "\n",
      "iter 51\ttrain cost\t0.5825668573379517\n",
      "[1.9740157e+03 2.6117542e-01 7.9300694e-02 6.1572291e-02 5.7897232e-02\n",
      " 4.7395103e-02 3.5492018e-02 3.0944536e-02 1.9147316e-02 6.9801547e-03]\n",
      "iter 52\ttotal variance to retain\t0.0\n",
      "Epoch 26\n",
      "Iter 29700: loss: 0.5606\n",
      "\n",
      "iter 53\ttrain cost\t0.5605804920196533\n",
      "[1.1842111e+03 2.4279995e-01 8.7884896e-02 7.1124978e-02 5.1206678e-02\n",
      " 4.2793125e-02 3.7269045e-02 2.8406825e-02 2.0269671e-02 8.3021699e-03]\n",
      "iter 54\ttotal variance to retain\t0.0\n",
      "Epoch 27\n",
      "Iter 30800: loss: 0.5398\n",
      "\n",
      "iter 55\ttrain cost\t0.5397681593894958\n",
      "[2.1268572e+03 2.3508789e-01 1.2609300e-01 6.1606511e-02 5.1216215e-02\n",
      " 4.6786688e-02 4.0405836e-02 3.0158114e-02 1.9577073e-02 6.3705463e-03]\n",
      "iter 56\ttotal variance to retain\t0.0\n",
      "Epoch 28\n",
      "Iter 31900: loss: 0.5195\n",
      "\n",
      "iter 57\ttrain cost\t0.5194997191429138\n",
      "[4.4305205e+03 3.4714952e-01 9.4286211e-02 5.6776203e-02 4.7062572e-02\n",
      " 4.0970385e-02 3.1699505e-02 2.7655153e-02 2.4225799e-02 8.7765539e-03]\n",
      "iter 58\ttotal variance to retain\t0.0\n",
      "Epoch 29\n",
      "Iter 33000: loss: 0.5101\n",
      "\n",
      "iter 59\ttrain cost\t0.510095477104187\n",
      "[1.5852444e+03 3.5773993e-01 1.3204622e-01 9.2605904e-02 4.8942782e-02\n",
      " 4.5297652e-02 4.0825564e-02 2.8007716e-02 2.6340317e-02 1.4735104e-02]\n",
      "iter 60\ttotal variance to retain\t0.0\n",
      "Epoch 30\n",
      "Iter 34100: loss: 0.4942\n",
      "\n",
      "iter 61\ttrain cost\t0.4941711723804474\n",
      "[3.58170557e+03 8.78628552e-01 2.95502424e-01 8.47940370e-02\n",
      " 6.60825446e-02 4.29544225e-02 3.62458378e-02 3.18388902e-02\n",
      " 2.41666362e-02 1.28084775e-02]\n",
      "iter 62\ttotal variance to retain\t0.0\n",
      "Epoch 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 35200: loss: 0.4873\n",
      "\n",
      "iter 63\ttrain cost\t0.4872729182243347\n",
      "[3.1777144e+03 1.0198617e+00 2.1725720e-01 8.4261417e-02 4.8371553e-02\n",
      " 3.5695933e-02 3.1523913e-02 2.4548562e-02 1.9478580e-02 6.4906506e-03]\n",
      "iter 64\ttotal variance to retain\t0.0\n",
      "Epoch 32\n",
      "Iter 36300: loss: 0.4753\n",
      "\n",
      "iter 65\ttrain cost\t0.47531500458717346\n",
      "[3.8116248e+03 2.6489606e+00 1.6157946e-01 5.5044446e-02 4.5217399e-02\n",
      " 3.9116424e-02 2.9144255e-02 2.6950072e-02 1.9811541e-02 9.1948723e-03]\n",
      "iter 66\ttotal variance to retain\t0.0\n",
      "Epoch 33\n",
      "Iter 37400: loss: 0.4775\n",
      "\n",
      "iter 67\ttrain cost\t0.477489709854126\n",
      "[6.1393281e+03 9.4654961e+00 1.8805510e-01 6.9944508e-02 5.6080427e-02\n",
      " 4.5192260e-02 3.9717577e-02 3.2540601e-02 2.4352778e-02 1.0495037e-02]\n",
      "iter 68\ttotal variance to retain\t0.0\n",
      "Epoch 34\n",
      "Iter 38500: loss: 0.4679\n",
      "\n",
      "iter 69\ttrain cost\t0.46792933344841003\n",
      "[6.1958555e+03 8.7184458e+00 1.8264753e-01 9.5521003e-02 4.9388468e-02\n",
      " 4.4516061e-02 3.4314770e-02 3.0749789e-02 2.4342868e-02 1.3611665e-02]\n",
      "iter 70\ttotal variance to retain\t0.0\n",
      "Epoch 35\n",
      "Iter 39600: loss: 0.4581\n",
      "\n",
      "iter 71\ttrain cost\t0.45808956027030945\n",
      "[7.3130425e+03 1.7001625e+01 3.0864528e-01 1.4994590e-01 5.5828787e-02\n",
      " 4.8043441e-02 3.9957993e-02 3.1510167e-02 2.4678985e-02 1.2839743e-02]\n",
      "iter 72\ttotal variance to retain\t0.0\n",
      "Epoch 36\n",
      "Iter 40700: loss: 0.4460\n",
      "\n",
      "iter 73\ttrain cost\t0.44603946805000305\n",
      "[1.1359104e+04 3.7529270e+01 5.4228520e-01 9.7315617e-02 4.8815399e-02\n",
      " 4.2778078e-02 3.3930507e-02 2.8300034e-02 2.2788171e-02 1.5194708e-02]\n",
      "iter 74\ttotal variance to retain\t0.0\n",
      "Epoch 37\n",
      "Iter 41800: loss: 0.4452\n",
      "\n",
      "iter 75\ttrain cost\t0.44515708088874817\n",
      "[1.3251668e+04 6.0187786e+01 1.1418877e+00 1.3987535e-01 5.8316536e-02\n",
      " 5.3073242e-02 3.7073746e-02 3.1827156e-02 1.9115325e-02 1.6181389e-02]\n",
      "iter 76\ttotal variance to retain\t0.0\n",
      "Epoch 38\n",
      "Iter 42900: loss: 0.4384\n",
      "\n",
      "iter 77\ttrain cost\t0.43844860792160034\n",
      "[1.3242842e+04 5.9606430e+01 1.4879097e+00 1.3237935e-01 4.9751516e-02\n",
      " 4.7600050e-02 3.5815693e-02 3.2960087e-02 2.3832304e-02 1.5853696e-02]\n",
      "iter 78\ttotal variance to retain\t0.0\n",
      "Epoch 39\n",
      "Iter 44000: loss: 0.4269\n",
      "\n",
      "iter 79\ttrain cost\t0.42686936259269714\n",
      "[1.4482741e+04 9.1335281e+01 2.4535141e+00 1.7197363e-01 9.6086733e-02\n",
      " 6.8526462e-02 4.9081821e-02 4.3205328e-02 2.7245158e-02 2.4820665e-02]\n",
      "iter 80\ttotal variance to retain\t0.0\n",
      "Epoch 40\n",
      "Iter 45100: loss: 0.4308\n",
      "\n",
      "iter 81\ttrain cost\t0.4307640790939331\n",
      "[1.71806855e+04 1.24563774e+02 2.95783663e+00 1.27088666e-01\n",
      " 7.58684874e-02 5.21717183e-02 3.44198346e-02 2.64905039e-02\n",
      " 2.46569104e-02 1.35795921e-02]\n",
      "iter 82\ttotal variance to retain\t0.0\n",
      "Epoch 41\n",
      "Iter 46200: loss: 0.4208\n",
      "\n",
      "iter 83\ttrain cost\t0.4208401143550873\n",
      "[2.1034180e+04 1.9635918e+02 4.5315599e+00 3.3655065e-01 1.3636652e-01\n",
      " 5.6624558e-02 4.5981575e-02 3.4122121e-02 2.6947590e-02 1.4822185e-02]\n",
      "iter 84\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 42\n",
      "Iter 47300: loss: 0.4171\n",
      "\n",
      "iter 85\ttrain cost\t0.41712626814842224\n",
      "[2.0410064e+04 3.9258560e+02 5.4913797e+00 3.8938358e-01 2.2978382e-01\n",
      " 9.7484581e-02 4.3701559e-02 3.9066434e-02 2.6443277e-02 1.3229348e-02]\n",
      "iter 86\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 43\n",
      "Iter 48400: loss: 0.4143\n",
      "\n",
      "iter 87\ttrain cost\t0.41434532403945923\n",
      "[1.8065455e+04 2.4448920e+02 5.9987221e+00 7.0059079e-01 2.8203687e-01\n",
      " 5.5068199e-02 4.9911790e-02 3.2599162e-02 2.2729246e-02 1.3121454e-02]\n",
      "iter 88\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 44\n",
      "Iter 49500: loss: 0.4079\n",
      "\n",
      "iter 89\ttrain cost\t0.40789756178855896\n",
      "[2.1900895e+04 3.3162592e+02 7.1227956e+00 1.0414931e+00 4.5741394e-01\n",
      " 8.2113169e-02 4.5912091e-02 3.2745365e-02 2.5880478e-02 1.4319675e-02]\n",
      "iter 90\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 45\n",
      "Iter 50600: loss: 0.4036\n",
      "\n",
      "iter 91\ttrain cost\t0.4035642147064209\n",
      "[2.5116838e+04 3.6773578e+02 1.0758724e+01 1.6449039e+00 4.8962551e-01\n",
      " 6.4991049e-02 3.5033483e-02 2.8398965e-02 2.0018326e-02 1.5338629e-02]\n",
      "iter 92\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 46\n",
      "Iter 51700: loss: 0.4044\n",
      "\n",
      "iter 93\ttrain cost\t0.4044414162635803\n",
      "[3.0613756e+04 5.3351361e+02 1.8916246e+01 2.0108743e+00 8.6002427e-01\n",
      " 6.6768818e-02 3.9228082e-02 2.4939876e-02 2.1758057e-02 1.5897447e-02]\n",
      "iter 94\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 47\n",
      "Iter 52800: loss: 0.3996\n",
      "\n",
      "iter 95\ttrain cost\t0.3996218740940094\n",
      "[3.2700869e+04 7.4769879e+02 2.5816389e+01 2.9912324e+00 7.5900221e-01\n",
      " 7.0013113e-02 4.0435705e-02 3.8411114e-02 3.1410731e-02 1.6186781e-02]\n",
      "iter 96\ttotal variance to retain\t6.556510925292969e-07\n",
      "Epoch 48\n",
      "Iter 53900: loss: 0.4001\n",
      "\n",
      "iter 97\ttrain cost\t0.40006518363952637\n",
      "[2.88166953e+04 6.66315857e+02 2.00144691e+01 3.49283099e+00\n",
      " 8.98776591e-01 7.31284693e-02 3.88332792e-02 3.60446461e-02\n",
      " 2.54118927e-02 1.29815405e-02]\n",
      "iter 98\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 49\n",
      "Iter 55000: loss: 0.3959\n",
      "\n",
      "iter 99\ttrain cost\t0.39586132764816284\n",
      "[2.6823965e+04 6.3347656e+02 1.7833162e+01 2.5675156e+00 1.0163463e+00\n",
      " 9.8701522e-02 8.1204273e-02 5.3120568e-02 3.5245597e-02 1.7116763e-02]\n",
      "iter 100\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 50\n",
      "Iter 56100: loss: 0.3964\n",
      "\n",
      "iter 101\ttrain cost\t0.3963545262813568\n",
      "[2.6585908e+04 7.4303735e+02 2.4206852e+01 3.2224591e+00 1.0662694e+00\n",
      " 8.7580353e-02 7.2224997e-02 5.0629519e-02 3.5265379e-02 1.7345028e-02]\n",
      "iter 102\ttotal variance to retain\t8.344650268554688e-07\n",
      "Epoch 51\n",
      "Iter 57200: loss: 0.3883\n",
      "\n",
      "iter 103\ttrain cost\t0.3882695436477661\n",
      "[3.3122238e+04 8.6304449e+02 2.2371254e+01 4.7283683e+00 1.6840382e+00\n",
      " 1.2697999e-01 5.7255469e-02 4.6525236e-02 2.3313906e-02 1.3613813e-02]\n",
      "iter 104\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 52\n",
      "Iter 58300: loss: 0.3859\n",
      "\n",
      "iter 105\ttrain cost\t0.3859124779701233\n",
      "[3.9596449e+04 8.9771973e+02 3.3248978e+01 6.9488692e+00 1.6892208e+00\n",
      " 2.4748917e-01 8.6311057e-02 4.5853619e-02 3.1493429e-02 1.7679900e-02]\n",
      "iter 106\ttotal variance to retain\t7.152557373046875e-07\n",
      "Epoch 53\n",
      "Iter 59400: loss: 0.3839\n",
      "\n",
      "iter 107\ttrain cost\t0.3838571310043335\n",
      "[3.7956039e+04 9.4619910e+02 2.7628119e+01 4.8172059e+00 2.1394973e+00\n",
      " 2.7375716e-01 1.0196406e-01 5.3173266e-02 2.6334912e-02 1.6376521e-02]\n",
      "iter 108\ttotal variance to retain\t5.364418029785156e-07\n",
      "Epoch 54\n",
      "Iter 60500: loss: 0.3818\n",
      "\n",
      "iter 109\ttrain cost\t0.3818368911743164\n",
      "[3.5732738e+04 9.2169379e+02 3.2737892e+01 5.9496360e+00 1.7120039e+00\n",
      " 3.1785333e-01 8.2320608e-02 4.8565216e-02 3.0420613e-02 1.3501536e-02]\n",
      "iter 110\ttotal variance to retain\t8.940696716308594e-07\n",
      "Epoch 55\n",
      "Iter 61600: loss: 0.3820\n",
      "\n",
      "iter 111\ttrain cost\t0.3820036053657532\n",
      "[3.7061289e+04 1.0463357e+03 2.8389826e+01 7.0605555e+00 1.9399052e+00\n",
      " 5.1584816e-01 8.6074688e-02 6.8242639e-02 3.6828540e-02 1.3181598e-02]\n",
      "iter 112\ttotal variance to retain\t6.556510925292969e-07\n",
      "Epoch 56\n",
      "Iter 62700: loss: 0.3828\n",
      "\n",
      "iter 113\ttrain cost\t0.38282862305641174\n",
      "[4.0146605e+04 1.0798873e+03 3.2249516e+01 7.6229801e+00 2.2653856e+00\n",
      " 5.2255696e-01 9.1283225e-02 6.6050895e-02 3.7470110e-02 2.3404783e-02]\n",
      "iter 114\ttotal variance to retain\t7.152557373046875e-07\n",
      "Epoch 57\n",
      "Iter 63800: loss: 0.3778\n",
      "\n",
      "iter 115\ttrain cost\t0.3778207302093506\n",
      "[3.7371773e+04 9.8019635e+02 2.4540161e+01 6.6931443e+00 2.2107377e+00\n",
      " 6.6901821e-01 8.0975808e-02 4.7478296e-02 4.0993378e-02 1.5239576e-02]\n",
      "iter 116\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 58\n",
      "Iter 64900: loss: 0.3709\n",
      "\n",
      "iter 117\ttrain cost\t0.37092986702919006\n",
      "[4.6282969e+04 1.1710127e+03 2.5570324e+01 6.6353440e+00 2.3227625e+00\n",
      " 9.5205796e-01 1.2000631e-01 7.2465479e-02 4.1134264e-02 1.5833458e-02]\n",
      "iter 118\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 59\n",
      "Iter 66000: loss: 0.3748\n",
      "\n",
      "iter 119\ttrain cost\t0.3748241066932678\n",
      "[4.0093059e+04 1.1217734e+03 2.7884277e+01 8.7563543e+00 2.6383309e+00\n",
      " 8.3167100e-01 1.7849603e-01 5.8892708e-02 3.9775509e-02 1.9003868e-02]\n",
      "iter 120\ttotal variance to retain\t5.364418029785156e-07\n",
      "Epoch 60\n",
      "Iter 67100: loss: 0.3727\n",
      "\n",
      "iter 121\ttrain cost\t0.3726819157600403\n",
      "[3.8063789e+04 1.2479841e+03 3.0111666e+01 9.4994802e+00 2.5704830e+00\n",
      " 1.0228468e+00 2.0375234e-01 7.1071200e-02 4.0921383e-02 2.3037584e-02]\n",
      "iter 122\ttotal variance to retain\t7.152557373046875e-07\n",
      "Epoch 61\n",
      "Iter 68200: loss: 0.3690\n",
      "\n",
      "iter 123\ttrain cost\t0.36895236372947693\n",
      "[4.1635062e+04 1.3310741e+03 3.8156223e+01 8.0429344e+00 3.0955825e+00\n",
      " 1.5538397e+00 1.8613407e-01 7.5332843e-02 5.4140989e-02 2.1848809e-02]\n",
      "iter 124\ttotal variance to retain\t8.940696716308594e-07\n",
      "Epoch 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 69300: loss: 0.3667\n",
      "\n",
      "iter 125\ttrain cost\t0.3667239844799042\n",
      "[5.0636566e+04 1.1427576e+03 3.0258713e+01 9.3911409e+00 2.8452609e+00\n",
      " 1.7402030e+00 2.6462105e-01 7.2228603e-02 4.6090733e-02 1.7229456e-02]\n",
      "iter 126\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 63\n",
      "Iter 70400: loss: 0.3685\n",
      "\n",
      "iter 127\ttrain cost\t0.3685009777545929\n",
      "[4.7855746e+04 1.4379625e+03 3.3142414e+01 8.4402952e+00 3.6027048e+00\n",
      " 2.5224168e+00 2.8996965e-01 9.1140330e-02 5.8258522e-02 2.4262516e-02]\n",
      "iter 128\ttotal variance to retain\t5.364418029785156e-07\n",
      "Epoch 64\n",
      "Iter 71500: loss: 0.3691\n",
      "\n",
      "iter 129\ttrain cost\t0.36910590529441833\n",
      "[4.7265648e+04 1.4662294e+03 2.2559736e+01 9.5822716e+00 4.3687544e+00\n",
      " 2.4666550e+00 2.7872610e-01 6.9891036e-02 4.7433592e-02 1.4783161e-02]\n",
      "iter 130\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 65\n",
      "Iter 72600: loss: 0.3669\n",
      "\n",
      "iter 131\ttrain cost\t0.36690154671669006\n",
      "[4.5486258e+04 1.4596661e+03 2.0531984e+01 8.2293301e+00 4.5127802e+00\n",
      " 2.0520105e+00 3.0460319e-01 1.1355446e-01 5.1686790e-02 1.5556663e-02]\n",
      "iter 132\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 66\n",
      "Iter 73700: loss: 0.3611\n",
      "\n",
      "iter 133\ttrain cost\t0.36111488938331604\n",
      "[5.05737891e+04 1.69571545e+03 2.57682247e+01 9.31873798e+00\n",
      " 5.05036736e+00 2.46924305e+00 4.09080833e-01 1.01245336e-01\n",
      " 6.63562790e-02 2.65131630e-02]\n",
      "iter 134\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 67\n",
      "Iter 74800: loss: 0.3571\n",
      "\n",
      "iter 135\ttrain cost\t0.3570520877838135\n",
      "[4.49085820e+04 1.35389355e+03 3.57652588e+01 7.91924047e+00\n",
      " 4.86056519e+00 2.55459142e+00 4.04849678e-01 1.15086064e-01\n",
      " 4.44571786e-02 2.00602338e-02]\n",
      "iter 136\ttotal variance to retain\t6.556510925292969e-07\n",
      "Epoch 68\n",
      "Iter 75900: loss: 0.3602\n",
      "\n",
      "iter 137\ttrain cost\t0.36024248600006104\n",
      "[4.8841559e+04 1.3054174e+03 2.4300619e+01 7.1711435e+00 5.3286519e+00\n",
      " 3.0260632e+00 4.7568882e-01 7.4497938e-02 4.3966085e-02 1.5571725e-02]\n",
      "iter 138\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 69\n",
      "Iter 77000: loss: 0.3625\n",
      "\n",
      "iter 139\ttrain cost\t0.3625331223011017\n",
      "[4.7241234e+04 1.4645753e+03 3.2480877e+01 8.3866205e+00 4.6814990e+00\n",
      " 2.4893081e+00 5.6243789e-01 9.7500227e-02 6.3970551e-02 1.6307779e-02]\n",
      "iter 140\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 70\n",
      "Iter 78100: loss: 0.3617\n",
      "\n",
      "iter 141\ttrain cost\t0.3617161214351654\n",
      "[5.3456887e+04 1.2233090e+03 2.9005886e+01 7.9494677e+00 5.2601500e+00\n",
      " 2.2968311e+00 5.3960764e-01 1.2479495e-01 5.0037399e-02 1.6526900e-02]\n",
      "iter 142\ttotal variance to retain\t3.5762786865234375e-07\n",
      "Epoch 71\n",
      "Iter 79200: loss: 0.3583\n",
      "\n",
      "iter 143\ttrain cost\t0.3583280146121979\n",
      "[5.3519891e+04 1.5383518e+03 2.7487394e+01 7.3934488e+00 6.3403654e+00\n",
      " 2.8796751e+00 6.4611363e-01 1.3512284e-01 6.2454809e-02 1.8973408e-02]\n",
      "iter 144\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 72\n",
      "Iter 80300: loss: 0.3507\n",
      "\n",
      "iter 145\ttrain cost\t0.3506811261177063\n",
      "[5.1948074e+04 1.1266329e+03 4.0514954e+01 1.0215448e+01 6.3311272e+00\n",
      " 2.4715142e+00 7.7177685e-01 1.2551911e-01 4.8680365e-02 1.2637819e-02]\n",
      "iter 146\ttotal variance to retain\t6.556510925292969e-07\n",
      "Epoch 73\n",
      "Iter 81400: loss: 0.3540\n",
      "\n",
      "iter 147\ttrain cost\t0.35399436950683594\n",
      "[5.3089090e+04 1.4539893e+03 3.7021946e+01 8.3324013e+00 7.1072731e+00\n",
      " 2.7671313e+00 7.4246645e-01 1.7579736e-01 6.3072272e-02 2.2292854e-02]\n",
      "iter 148\ttotal variance to retain\t5.364418029785156e-07\n",
      "Epoch 74\n",
      "Iter 82500: loss: 0.3602\n",
      "\n",
      "iter 149\ttrain cost\t0.3602462410926819\n",
      "[5.3498242e+04 1.1022881e+03 2.4667526e+01 9.0733728e+00 6.0517673e+00\n",
      " 3.2814116e+00 9.2252988e-01 1.4432120e-01 6.8908185e-02 2.3082381e-02]\n",
      "iter 150\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 75\n",
      "Iter 83600: loss: 0.3535\n",
      "\n",
      "iter 151\ttrain cost\t0.35348209738731384\n",
      "[5.8685070e+04 1.1637883e+03 2.5694971e+01 8.4256525e+00 6.0728040e+00\n",
      " 2.9737105e+00 8.1884086e-01 2.2046205e-01 6.7478955e-02 1.7131148e-02]\n",
      "iter 152\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 76\n",
      "Iter 84700: loss: 0.3514\n",
      "\n",
      "iter 153\ttrain cost\t0.3514009118080139\n",
      "[5.06958633e+04 1.55566956e+03 2.34506702e+01 9.61285782e+00\n",
      " 5.94997072e+00 1.85180783e+00 6.62493229e-01 1.89159974e-01\n",
      " 6.32010251e-02 1.53552005e-02]\n",
      "iter 154\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 77\n",
      "Iter 85800: loss: 0.3541\n",
      "\n",
      "iter 155\ttrain cost\t0.35405367612838745\n",
      "[4.8757223e+04 1.1866163e+03 2.3853939e+01 6.9253426e+00 5.1460438e+00\n",
      " 2.9103150e+00 6.9671458e-01 1.7853150e-01 7.0638910e-02 1.9550607e-02]\n",
      "iter 156\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 78\n",
      "Iter 86900: loss: 0.3459\n",
      "\n",
      "iter 157\ttrain cost\t0.3459167182445526\n",
      "[5.5848094e+04 1.5534349e+03 2.5616228e+01 9.2738514e+00 7.3489695e+00\n",
      " 2.9355369e+00 1.1855416e+00 2.8650439e-01 7.5397752e-02 1.4437328e-02]\n",
      "iter 158\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 79\n",
      "Iter 88000: loss: 0.3508\n",
      "\n",
      "iter 159\ttrain cost\t0.35080742835998535\n",
      "[5.3323457e+04 1.3728221e+03 2.4745466e+01 1.0932052e+01 6.4392629e+00\n",
      " 3.1334968e+00 1.1548769e+00 1.7800966e-01 6.0946640e-02 1.3979252e-02]\n",
      "iter 160\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 80\n",
      "Iter 89100: loss: 0.3490\n",
      "\n",
      "iter 161\ttrain cost\t0.3490447700023651\n",
      "[5.4108992e+04 1.4696552e+03 1.8327997e+01 8.7162790e+00 7.9056993e+00\n",
      " 2.1457343e+00 1.0617230e+00 2.0119613e-01 8.7179162e-02 2.2460992e-02]\n",
      "iter 162\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 81\n",
      "Iter 90200: loss: 0.3483\n",
      "\n",
      "iter 163\ttrain cost\t0.34834834933280945\n",
      "[5.6322855e+04 1.5141005e+03 2.5449518e+01 1.1111171e+01 6.8351316e+00\n",
      " 2.8393977e+00 9.1757333e-01 2.1177073e-01 8.2642227e-02 1.6324366e-02]\n",
      "iter 164\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 82\n",
      "Iter 91300: loss: 0.3460\n",
      "\n",
      "iter 165\ttrain cost\t0.346007764339447\n",
      "[5.4025996e+04 1.1891418e+03 2.3517929e+01 1.1227114e+01 6.5984349e+00\n",
      " 2.3944330e+00 9.1481906e-01 2.4780251e-01 1.1823255e-01 1.9724311e-02]\n",
      "iter 166\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 83\n",
      "Iter 92400: loss: 0.3494\n",
      "\n",
      "iter 167\ttrain cost\t0.3494422435760498\n",
      "[5.4117074e+04 1.4760226e+03 1.8435402e+01 1.1149260e+01 5.7087798e+00\n",
      " 2.6575229e+00 7.7757883e-01 2.2748576e-01 8.1415839e-02 1.5749989e-02]\n",
      "iter 168\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 84\n",
      "Iter 93500: loss: 0.3415\n",
      "\n",
      "iter 169\ttrain cost\t0.3415446877479553\n",
      "[5.3096277e+04 1.1573279e+03 3.0001719e+01 1.2949133e+01 4.1302466e+00\n",
      " 2.9786685e+00 8.1159401e-01 3.0291155e-01 1.1542596e-01 1.8700577e-02]\n",
      "iter 170\ttotal variance to retain\t3.5762786865234375e-07\n",
      "Epoch 85\n",
      "Iter 94600: loss: 0.3447\n",
      "\n",
      "iter 171\ttrain cost\t0.34468042850494385\n",
      "[5.0995172e+04 1.2359661e+03 2.1524832e+01 1.4033520e+01 6.7353859e+00\n",
      " 2.5446792e+00 7.7832735e-01 3.1488159e-01 9.2447557e-02 1.2656528e-02]\n",
      "iter 172\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 86\n",
      "Iter 95700: loss: 0.3420\n",
      "\n",
      "iter 173\ttrain cost\t0.3420037031173706\n",
      "[5.76115547e+04 1.14312256e+03 2.18381310e+01 1.19873495e+01\n",
      " 5.37592936e+00 2.32126880e+00 8.70357811e-01 2.76396453e-01\n",
      " 9.54266489e-02 1.71617214e-02]\n",
      "iter 174\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 87\n",
      "Iter 96800: loss: 0.3401\n",
      "\n",
      "iter 175\ttrain cost\t0.34005802869796753\n",
      "[5.7638355e+04 1.3382819e+03 2.3650259e+01 1.3307287e+01 5.4296708e+00\n",
      " 2.6382535e+00 7.1399158e-01 2.9928795e-01 1.6411498e-01 1.5656138e-02]\n",
      "iter 176\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 88\n",
      "Iter 97900: loss: 0.3424\n",
      "\n",
      "iter 177\ttrain cost\t0.3424224853515625\n",
      "[5.5426934e+04 1.1412207e+03 2.3988148e+01 1.1897438e+01 4.7323375e+00\n",
      " 2.9121521e+00 6.6947085e-01 2.6773933e-01 1.3933079e-01 1.4714635e-02]\n",
      "iter 178\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 89\n",
      "Iter 99000: loss: 0.3385\n",
      "\n",
      "iter 179\ttrain cost\t0.338509738445282\n",
      "[5.3853898e+04 1.6056556e+03 2.2941021e+01 1.4602727e+01 4.2147560e+00\n",
      " 2.2448943e+00 6.3110763e-01 3.6277920e-01 1.0571039e-01 2.3400243e-02]\n",
      "iter 180\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 90\n",
      "Iter 100100: loss: 0.3376\n",
      "\n",
      "iter 181\ttrain cost\t0.33757689595222473\n",
      "[5.6941629e+04 1.6416503e+03 2.7405251e+01 1.3954009e+01 5.8017817e+00\n",
      " 1.6376681e+00 5.0673574e-01 2.5840133e-01 1.3850375e-01 2.2185534e-02]\n",
      "iter 182\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 91\n",
      "Iter 101200: loss: 0.3387\n",
      "\n",
      "iter 183\ttrain cost\t0.3387015163898468\n",
      "[5.5682121e+04 1.3649298e+03 2.1983679e+01 1.6579292e+01 5.3249998e+00\n",
      " 2.3093636e+00 5.6321269e-01 2.8405792e-01 1.5260941e-01 2.2321291e-02]\n",
      "iter 184\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 102300: loss: 0.3413\n",
      "\n",
      "iter 185\ttrain cost\t0.3413030207157135\n",
      "[5.2561355e+04 1.3273147e+03 2.3953461e+01 1.4809236e+01 4.8643260e+00\n",
      " 2.2417128e+00 5.5525041e-01 2.3300251e-01 1.4389697e-01 1.9947954e-02]\n",
      "iter 186\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 93\n",
      "Iter 103400: loss: 0.3390\n",
      "\n",
      "iter 187\ttrain cost\t0.33900290727615356\n",
      "[5.6012434e+04 1.5967843e+03 1.9384203e+01 1.2724107e+01 4.7184296e+00\n",
      " 2.4618561e+00 5.7037157e-01 2.5554052e-01 1.5457113e-01 1.2751694e-02]\n",
      "iter 188\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 94\n",
      "Iter 104500: loss: 0.3410\n",
      "\n",
      "iter 189\ttrain cost\t0.34103497862815857\n",
      "[5.4434441e+04 9.0835059e+02 1.9913277e+01 1.6071119e+01 6.2340426e+00\n",
      " 2.2788453e+00 5.9455335e-01 3.2351458e-01 1.5252200e-01 1.9728750e-02]\n",
      "iter 190\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 95\n",
      "Iter 105600: loss: 0.3346\n",
      "\n",
      "iter 191\ttrain cost\t0.3345780670642853\n",
      "[5.3545594e+04 1.0863149e+03 2.7302219e+01 1.4721621e+01 5.1771564e+00\n",
      " 2.4889095e+00 5.4580188e-01 2.9982024e-01 1.9220676e-01 2.6669769e-02]\n",
      "iter 192\ttotal variance to retain\t3.5762786865234375e-07\n",
      "Epoch 96\n",
      "Iter 106700: loss: 0.3354\n",
      "\n",
      "iter 193\ttrain cost\t0.3353641927242279\n",
      "[5.7948906e+04 1.2857692e+03 2.0654850e+01 1.5610509e+01 2.8761315e+00\n",
      " 2.2034774e+00 6.0658163e-01 2.2953822e-01 1.5223072e-01 1.9309681e-02]\n",
      "iter 194\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 97\n",
      "Iter 107800: loss: 0.3397\n",
      "\n",
      "iter 195\ttrain cost\t0.33966064453125\n",
      "[5.4881582e+04 1.2325127e+03 2.1822107e+01 1.2497669e+01 3.2288375e+00\n",
      " 2.4448586e+00 4.9521008e-01 2.4876229e-01 1.3548343e-01 2.2679025e-02]\n",
      "iter 196\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Epoch 98\n",
      "Iter 108900: loss: 0.3338\n",
      "\n",
      "iter 197\ttrain cost\t0.3337944746017456\n",
      "[5.4182570e+04 1.1351906e+03 2.0184885e+01 1.6279263e+01 4.1384788e+00\n",
      " 1.8753947e+00 5.0919187e-01 3.0828434e-01 1.8447192e-01 1.3335852e-02]\n",
      "iter 198\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 99\n",
      "Iter 110000: loss: 0.3379\n",
      "\n",
      "iter 199\ttrain cost\t0.3378766179084778\n",
      "[5.2378199e+04 1.0861539e+03 2.1606089e+01 1.3023710e+01 4.0359702e+00\n",
      " 2.2097347e+00 5.2239203e-01 2.8598189e-01 1.8995592e-01 1.8122524e-02]\n",
      "iter 200\ttotal variance to retain\t1.7881393432617188e-07\n",
      "Part 0: score on test 0.3090\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00872134 -0.05449141  0.04177202  0.03871249 -0.00875796 -0.02798139\n",
      "   0.02569963 -0.00618478  0.00933585  0.02805118]\n",
      " [-0.05449141  0.36478764 -0.20245987 -0.21113455  0.06884134  0.1955166\n",
      "  -0.24516606  0.08784385  0.03734946 -0.17605875]\n",
      " [ 0.04177202 -0.20245987  0.340939    0.25940424 -0.0079637  -0.0842346\n",
      "  -0.08048892  0.08878326  0.2749775   0.13244504]\n",
      " [ 0.03871249 -0.21113455  0.25940424  0.2106965  -0.02102614 -0.09805587\n",
      "   0.00715168  0.03473596  0.16237824  0.12351112]\n",
      " [-0.00875796  0.06884134 -0.0079637  -0.02102614  0.01699321  0.0401096\n",
      "  -0.07492088  0.03477594  0.04617517 -0.02862966]\n",
      " [-0.02798139  0.1955166  -0.0842346  -0.09805587  0.0401096   0.10737061\n",
      "  -0.15440503  0.06169099  0.05142785 -0.09067379]\n",
      " [ 0.02569963 -0.24516606 -0.08048892  0.00715168 -0.07492088 -0.15440503\n",
      "   0.369946   -0.18934594 -0.30526543  0.08541981]\n",
      " [-0.00618478  0.08784385  0.08878326  0.03473596  0.03477594  0.06169099\n",
      "  -0.18934594  0.1039129   0.18692769 -0.02149787]\n",
      " [ 0.00933585  0.03734946  0.2749775   0.16237824  0.04617517  0.05142785\n",
      "  -0.30526543  0.18692769  0.38638356  0.02690597]\n",
      " [ 0.02805118 -0.17605875  0.13244504  0.12351112 -0.02862966 -0.09067379\n",
      "   0.08541981 -0.02149787  0.02690597  0.09024918]]\n",
      "acc: 21.43%\n",
      "Current part is 1\n",
      "\n",
      "Epoch 0\n",
      "Iter 1100: loss: 2.7176\n",
      "\n",
      "iter 201\ttrain cost\t2.7175750732421875\n",
      "[106.3174       5.1976857    4.888479     3.5720108    3.2140183\n",
      "   2.778429     1.9621997    1.4911125    0.96044713   0.52392536]\n",
      "iter 202\ttotal variance to retain\t0.005440175533294678\n",
      "Epoch 1\n",
      "Iter 2200: loss: 2.5715\n",
      "\n",
      "iter 203\ttrain cost\t2.5715320110321045\n",
      "[366.715        8.865243     7.108823     6.2346377    4.136222\n",
      "   3.5098903    2.9345014    2.0978985    1.5474716    0.71906453]\n",
      "iter 204\ttotal variance to retain\t0.0010004043579101562\n",
      "Epoch 2\n",
      "Iter 3300: loss: 2.4850\n",
      "\n",
      "iter 205\ttrain cost\t2.4850034713745117\n",
      "[848.1237     15.923752   13.612707   10.187278    7.1199775   6.316614\n",
      "   3.6079457   2.3010569   1.7826976   1.1694138]\n",
      "iter 206\ttotal variance to retain\t0.0005590319633483887\n",
      "Epoch 3\n",
      "Iter 4400: loss: 2.3892\n",
      "\n",
      "iter 207\ttrain cost\t2.389216184616089\n",
      "[1543.3884      27.621119    22.651283    12.817227     8.740954\n",
      "    7.632521     5.9808636    4.0079346    2.265411     1.6166761]\n",
      "iter 208\ttotal variance to retain\t0.0003656148910522461\n",
      "Epoch 4\n",
      "Iter 5500: loss: 2.2966\n",
      "\n",
      "iter 209\ttrain cost\t2.2966082096099854\n",
      "[1.9121437e+03 4.6730438e+01 2.9425743e+01 1.7768879e+01 1.2843347e+01\n",
      " 9.9023695e+00 4.7354980e+00 3.8292074e+00 2.2536831e+00 1.2649074e+00]\n",
      "iter 210\ttotal variance to retain\t0.00040656328201293945\n",
      "Epoch 5\n",
      "Iter 6600: loss: 2.1843\n",
      "\n",
      "iter 211\ttrain cost\t2.184250593185425\n",
      "[2.3135168e+03 4.6560230e+01 3.3660675e+01 1.7430061e+01 1.3268283e+01\n",
      " 6.4438906e+00 4.8365779e+00 3.6582711e+00 1.9524428e+00 6.8209499e-01]\n",
      "iter 212\ttotal variance to retain\t0.000316619873046875\n",
      "Epoch 6\n",
      "Iter 7700: loss: 2.0607\n",
      "\n",
      "iter 213\ttrain cost\t2.0606813430786133\n",
      "[2.0207067e+03 6.7324249e+01 3.9914337e+01 1.6133842e+01 1.0722927e+01\n",
      " 6.5207448e+00 2.9249437e+00 2.2733617e+00 1.3093989e+00 4.5720431e-01]\n",
      "iter 214\ttotal variance to retain\t0.0004955530166625977\n",
      "Epoch 7\n",
      "Iter 8800: loss: 1.9165\n",
      "\n",
      "iter 215\ttrain cost\t1.9164764881134033\n",
      "[1.9097899e+03 5.6080627e+01 3.3812748e+01 1.1629155e+01 7.9999280e+00\n",
      " 3.7796886e+00 1.9445959e+00 1.3487445e+00 9.3866682e-01 1.6694251e-01]\n",
      "iter 216\ttotal variance to retain\t0.0003733634948730469\n",
      "Epoch 8\n",
      "Iter 9900: loss: 1.7771\n",
      "\n",
      "iter 217\ttrain cost\t1.7770506143569946\n",
      "[1.5665121e+03 4.1087608e+01 2.0168716e+01 6.7517610e+00 2.8195882e+00\n",
      " 1.7717069e+00 8.1771410e-01 4.5975459e-01 2.9327452e-01 1.5636688e-01]\n",
      "iter 218\ttotal variance to retain\t0.00018900632858276367\n",
      "Epoch 9\n",
      "Iter 11000: loss: 1.6445\n",
      "\n",
      "iter 219\ttrain cost\t1.6445281505584717\n",
      "[1.1649521e+03 3.2424740e+01 1.0276333e+01 2.4929414e+00 1.3368645e+00\n",
      " 6.1202246e-01 3.5139567e-01 2.1702527e-01 1.5253805e-01 1.2483841e-01]\n",
      "iter 220\ttotal variance to retain\t8.410215377807617e-05\n",
      "Epoch 10\n",
      "Iter 12100: loss: 1.5222\n",
      "\n",
      "iter 221\ttrain cost\t1.5222389698028564\n",
      "[8.9652826e+02 1.3432269e+01 4.1434207e+00 7.3240805e-01 4.8335329e-01\n",
      " 2.9221377e-01 1.4370757e-01 1.1332732e-01 6.9826372e-02 5.7324395e-02]\n",
      "iter 222\ttotal variance to retain\t2.2470951080322266e-05\n",
      "Epoch 11\n",
      "Iter 13200: loss: 1.4172\n",
      "\n",
      "iter 223\ttrain cost\t1.4171963930130005\n",
      "[8.0587286e+02 8.0035563e+00 1.4057280e+00 3.2335138e-01 1.8217668e-01\n",
      " 1.5480164e-01 1.3989803e-01 9.9087939e-02 7.5294815e-02 7.2850980e-02]\n",
      "iter 224\ttotal variance to retain\t3.2782554626464844e-06\n",
      "Epoch 12\n",
      "Iter 14300: loss: 1.3088\n",
      "\n",
      "iter 225\ttrain cost\t1.3087987899780273\n",
      "[6.6457056e+02 2.6427457e+00 3.5526726e-01 1.5132767e-01 1.1457471e-01\n",
      " 8.7396152e-02 7.4510902e-02 5.8501210e-02 5.3034630e-02 4.3251880e-02]\n",
      "iter 226\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 13\n",
      "Iter 15400: loss: 1.2109\n",
      "\n",
      "iter 227\ttrain cost\t1.2109043598175049\n",
      "[5.0016873e+02 1.2541550e+00 2.0792678e-01 1.6620621e-01 1.2765171e-01\n",
      " 1.0532865e-01 8.1399575e-02 5.9382007e-02 5.1172942e-02 4.3149903e-02]\n",
      "iter 228\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 14\n",
      "Iter 16500: loss: 1.1183\n",
      "\n",
      "iter 229\ttrain cost\t1.118343472480774\n",
      "[4.6441418e+02 5.5629718e-01 2.4909370e-01 9.2310630e-02 8.4603414e-02\n",
      " 7.2118878e-02 5.1255066e-02 3.5694338e-02 3.4082267e-02 2.1913417e-02]\n",
      "iter 230\ttotal variance to retain\t4.172325134277344e-07\n",
      "Epoch 15\n",
      "Iter 17600: loss: 1.0397\n",
      "\n",
      "iter 231\ttrain cost\t1.0396852493286133\n",
      "[4.7335336e+02 2.3898904e-01 1.8176582e-01 7.7071927e-02 6.3993514e-02\n",
      " 5.5268049e-02 4.4302814e-02 4.0533412e-02 3.6032651e-02 1.8837908e-02]\n",
      "iter 232\ttotal variance to retain\t2.384185791015625e-07\n",
      "Epoch 16\n",
      "Iter 18700: loss: 0.9635\n",
      "\n",
      "iter 233\ttrain cost\t0.9634702801704407\n",
      "[4.09860626e+02 2.35745907e-01 1.83552384e-01 1.46371499e-01\n",
      " 1.04850635e-01 7.60363787e-02 5.71035184e-02 3.81386727e-02\n",
      " 3.53275873e-02 2.02505812e-02]\n",
      "iter 234\ttotal variance to retain\t4.76837158203125e-07\n",
      "Epoch 17\n",
      "Iter 19800: loss: 0.9029\n",
      "\n",
      "iter 235\ttrain cost\t0.9029232263565063\n",
      "[4.0534772e+02 2.0045732e-01 1.5771279e-01 8.6003095e-02 7.0501991e-02\n",
      " 5.5135790e-02 4.7811773e-02 3.9352991e-02 2.5093976e-02 1.2557796e-02]\n",
      "iter 236\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 18\n",
      "Iter 20900: loss: 0.8406\n",
      "\n",
      "iter 237\ttrain cost\t0.8405756950378418\n",
      "[3.81383636e+02 2.02982664e-01 1.43474594e-01 1.24474965e-01\n",
      " 8.52811188e-02 7.32076094e-02 5.21762632e-02 5.07939346e-02\n",
      " 3.70366760e-02 9.03964229e-03]\n",
      "iter 238\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 19\n",
      "Iter 22000: loss: 0.7882\n",
      "\n",
      "iter 239\ttrain cost\t0.788207471370697\n",
      "[4.6519067e+02 1.8321818e-01 1.2307207e-01 6.8340532e-02 6.5239727e-02\n",
      " 5.7808463e-02 5.3210169e-02 3.9082911e-02 2.6593281e-02 9.8720975e-03]\n",
      "iter 240\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 20\n",
      "Iter 23100: loss: 0.7431\n",
      "\n",
      "iter 241\ttrain cost\t0.7430651187896729\n",
      "[3.9956754e+02 1.7709292e-01 1.2500754e-01 9.8221436e-02 6.6920236e-02\n",
      " 6.1923027e-02 4.7145545e-02 3.5656638e-02 2.4524815e-02 9.3143703e-03]\n",
      "iter 242\ttotal variance to retain\t2.980232238769531e-07\n",
      "Epoch 21\n",
      "Iter 24200: loss: 0.6985\n",
      "\n",
      "iter 243\ttrain cost\t0.6985499858856201\n",
      "[5.1555719e+02 1.3816617e-01 8.4882706e-02 6.2150091e-02 5.4352291e-02\n",
      " 4.6488356e-02 3.7781194e-02 2.9467793e-02 2.3746137e-02 1.0209496e-02]\n",
      "iter 244\ttotal variance to retain\t0.0\n",
      "Epoch 22\n",
      "Iter 25300: loss: 0.6588\n",
      "\n",
      "iter 245\ttrain cost\t0.6588220000267029\n",
      "[6.39270630e+02 1.63052559e-01 1.15514524e-01 7.10532740e-02\n",
      " 6.15169294e-02 5.43882027e-02 4.58693542e-02 3.60489786e-02\n",
      " 2.53223367e-02 9.89248604e-03]\n",
      "iter 246\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 23\n",
      "Iter 26400: loss: 0.6256\n",
      "\n",
      "iter 247\ttrain cost\t0.6256301999092102\n",
      "[8.2031445e+02 1.1022816e-01 9.1007881e-02 5.9557367e-02 5.5689044e-02\n",
      " 4.8816673e-02 4.3720812e-02 3.3260804e-02 2.8987018e-02 1.0926325e-02]\n",
      "iter 248\ttotal variance to retain\t0.0\n",
      "Epoch 24\n",
      "Iter 27500: loss: 0.6003\n",
      "\n",
      "iter 249\ttrain cost\t0.6002938747406006\n",
      "[1.3391815e+03 2.2987908e-01 1.0246158e-01 7.8134730e-02 6.1879665e-02\n",
      " 4.4639632e-02 4.0473249e-02 3.0039513e-02 2.7144616e-02 7.8899972e-03]\n",
      "iter 250\ttotal variance to retain\t0.0\n",
      "Epoch 25\n",
      "Iter 28600: loss: 0.5731\n",
      "\n",
      "iter 251\ttrain cost\t0.5730677247047424\n",
      "[1.5457756e+03 1.4525162e-01 1.0937018e-01 7.1465828e-02 5.2331597e-02\n",
      " 4.3574147e-02 3.4995060e-02 3.0391000e-02 1.7667297e-02 1.1111426e-02]\n",
      "iter 252\ttotal variance to retain\t0.0\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 29700: loss: 0.5545\n",
      "\n",
      "iter 253\ttrain cost\t0.5544803142547607\n",
      "[1.04615808e+03 1.06093004e-01 8.37176368e-02 6.64824992e-02\n",
      " 5.53140938e-02 4.29297015e-02 3.78019027e-02 2.53827646e-02\n",
      " 2.05503479e-02 1.13842934e-02]\n",
      "iter 254\ttotal variance to retain\t0.0\n",
      "Epoch 27\n",
      "Iter 30800: loss: 0.5311\n",
      "\n",
      "iter 255\ttrain cost\t0.5311067700386047\n",
      "[2.0286421e+03 1.4871229e-01 7.5235777e-02 6.5923676e-02 5.8508102e-02\n",
      " 4.0462576e-02 3.9974552e-02 2.7059736e-02 1.5709328e-02 1.2319908e-02]\n",
      "iter 256\ttotal variance to retain\t0.0\n",
      "Epoch 28\n",
      "Iter 31900: loss: 0.5158\n",
      "\n",
      "iter 257\ttrain cost\t0.5158287286758423\n",
      "[1.6905160e+03 1.1858839e-01 1.1026296e-01 8.4972501e-02 6.5406807e-02\n",
      " 5.6695223e-02 4.2014748e-02 3.5081297e-02 1.9127211e-02 1.1959597e-02]\n",
      "iter 258\ttotal variance to retain\t0.0\n",
      "Epoch 29\n",
      "Iter 33000: loss: 0.5058\n",
      "\n",
      "iter 259\ttrain cost\t0.5057670474052429\n",
      "[2.2434695e+03 2.4707824e-01 1.2604959e-01 7.3530458e-02 6.6446438e-02\n",
      " 4.8809223e-02 3.7838023e-02 3.3225887e-02 2.0614039e-02 1.2884352e-02]\n",
      "iter 260\ttotal variance to retain\t0.0\n",
      "Epoch 30\n",
      "Iter 34100: loss: 0.4898\n",
      "\n",
      "iter 261\ttrain cost\t0.4898316264152527\n",
      "[4.15034375e+03 1.38012671e+00 1.12371795e-01 6.33842126e-02\n",
      " 5.26919700e-02 3.55590582e-02 3.18907201e-02 2.95920447e-02\n",
      " 2.75268555e-02 1.11616841e-02]\n",
      "iter 262\ttotal variance to retain\t0.0\n",
      "Epoch 31\n",
      "Iter 35200: loss: 0.4806\n",
      "\n",
      "iter 263\ttrain cost\t0.48056402802467346\n",
      "[4.0179724e+03 1.2023216e+00 1.1079662e-01 7.0687249e-02 5.9635065e-02\n",
      " 5.1980156e-02 4.4240460e-02 3.5367742e-02 3.1073339e-02 1.1080615e-02]\n",
      "iter 264\ttotal variance to retain\t0.0\n",
      "Epoch 32\n",
      "Iter 36300: loss: 0.4743\n",
      "\n",
      "iter 265\ttrain cost\t0.47427818179130554\n",
      "[5.3830273e+03 4.5478978e+00 1.0313457e-01 7.4357264e-02 7.0097938e-02\n",
      " 4.9031321e-02 4.0418621e-02 3.5775036e-02 2.3684939e-02 1.5136588e-02]\n",
      "iter 266\ttotal variance to retain\t0.0\n",
      "Epoch 33\n",
      "Iter 37400: loss: 0.4647\n",
      "\n",
      "iter 267\ttrain cost\t0.4646633565425873\n",
      "[6.4314648e+03 7.5636396e+00 9.3839929e-02 8.5123375e-02 6.1384790e-02\n",
      " 4.6146341e-02 3.7922185e-02 3.4229420e-02 2.3140691e-02 1.5363173e-02]\n",
      "iter 268\ttotal variance to retain\t0.0\n",
      "Epoch 34\n",
      "Iter 38500: loss: 0.4622\n",
      "\n",
      "iter 269\ttrain cost\t0.462245911359787\n",
      "[6.1101206e+03 5.3123140e+00 1.7501399e-01 8.2763262e-02 5.8973681e-02\n",
      " 4.7887217e-02 3.8134977e-02 3.6353484e-02 2.3901576e-02 1.2035435e-02]\n",
      "iter 270\ttotal variance to retain\t0.0\n",
      "Epoch 35\n",
      "Iter 39600: loss: 0.4538\n",
      "\n",
      "iter 271\ttrain cost\t0.4538484513759613\n",
      "[9.1386094e+03 1.4678226e+01 1.7027070e-01 9.5603779e-02 7.3324472e-02\n",
      " 5.5354364e-02 4.6163023e-02 2.9364714e-02 2.4697527e-02 1.5821148e-02]\n",
      "iter 272\ttotal variance to retain\t0.0\n",
      "Epoch 36\n",
      "Iter 40700: loss: 0.4470\n",
      "\n",
      "iter 273\ttrain cost\t0.4469675123691559\n",
      "[9.1713105e+03 2.1564287e+01 1.7054634e-01 7.7085167e-02 4.8797615e-02\n",
      " 4.2729788e-02 3.3934381e-02 2.5609782e-02 2.3018116e-02 1.5693950e-02]\n",
      "iter 274\ttotal variance to retain\t0.0\n",
      "Epoch 37\n",
      "Iter 41800: loss: 0.4369\n",
      "\n",
      "iter 275\ttrain cost\t0.43686676025390625\n",
      "[1.0622772e+04 2.4309649e+01 3.7721664e-01 7.4881054e-02 6.0792573e-02\n",
      " 4.0704954e-02 3.9298773e-02 3.0136010e-02 2.6440162e-02 1.3719937e-02]\n",
      "iter 276\ttotal variance to retain\t0.0\n",
      "Epoch 38\n",
      "Iter 42900: loss: 0.4392\n",
      "\n",
      "iter 277\ttrain cost\t0.43920090794563293\n",
      "[1.4244579e+04 4.3389091e+01 5.4603195e-01 7.1567573e-02 5.4611932e-02\n",
      " 4.2061388e-02 3.2377835e-02 2.6414586e-02 2.3647638e-02 1.5526488e-02]\n",
      "iter 278\ttotal variance to retain\t0.0\n",
      "Epoch 39\n",
      "Iter 44000: loss: 0.4323\n",
      "\n",
      "iter 279\ttrain cost\t0.4323188364505768\n",
      "[1.39950996e+04 6.20116119e+01 7.68934190e-01 6.76600710e-02\n",
      " 5.61054312e-02 4.33440804e-02 3.69700268e-02 3.21166366e-02\n",
      " 2.39436366e-02 1.36148855e-02]\n",
      "iter 280\ttotal variance to retain\t0.0\n",
      "Epoch 40\n",
      "Iter 45100: loss: 0.4269\n",
      "\n",
      "iter 281\ttrain cost\t0.4269200563430786\n",
      "[1.7250281e+04 9.7293434e+01 2.2617514e+00 9.3855143e-02 8.4768794e-02\n",
      " 5.6474976e-02 4.5107998e-02 4.1252352e-02 3.0601345e-02 1.5594677e-02]\n",
      "iter 282\ttotal variance to retain\t0.0\n",
      "Epoch 41\n",
      "Iter 46200: loss: 0.4203\n",
      "\n",
      "iter 283\ttrain cost\t0.4203247129917145\n",
      "[1.3320620e+04 5.0804798e+01 2.0395834e+00 1.1761162e-01 7.6866344e-02\n",
      " 5.3489465e-02 4.5422167e-02 3.7892289e-02 2.6880747e-02 1.5156655e-02]\n",
      "iter 284\ttotal variance to retain\t0.0\n",
      "Epoch 42\n",
      "Iter 47300: loss: 0.4221\n",
      "\n",
      "iter 285\ttrain cost\t0.42213666439056396\n",
      "[1.6809828e+04 1.1037409e+02 2.5272508e+00 2.0664982e-01 9.0041727e-02\n",
      " 5.4614052e-02 5.2373681e-02 4.0272266e-02 3.0018052e-02 1.5300070e-02]\n",
      "iter 286\ttotal variance to retain\t0.0\n",
      "Epoch 43\n",
      "Iter 48400: loss: 0.4097\n",
      "\n",
      "iter 287\ttrain cost\t0.40974757075309753\n",
      "[1.7971904e+04 1.6805257e+02 2.8589218e+00 2.6627800e-01 1.1361054e-01\n",
      " 6.3494921e-02 4.6454273e-02 4.1670736e-02 2.6474338e-02 1.7073495e-02]\n",
      "iter 288\ttotal variance to retain\t0.0\n",
      "Epoch 44\n",
      "Iter 49500: loss: 0.4077\n",
      "\n",
      "iter 289\ttrain cost\t0.40773510932922363\n",
      "[2.4525176e+04 2.2570398e+02 4.9462504e+00 5.1998544e-01 7.0160605e-02\n",
      " 5.5412054e-02 3.4958553e-02 2.9273676e-02 2.3374967e-02 1.6735096e-02]\n",
      "iter 290\ttotal variance to retain\t0.0\n",
      "Epoch 45\n",
      "Iter 50600: loss: 0.4028\n",
      "\n",
      "iter 291\ttrain cost\t0.40276390314102173\n",
      "[2.7738240e+04 4.1075366e+02 4.3596039e+00 8.7826824e-01 6.3881904e-02\n",
      " 5.8259469e-02 3.4599431e-02 3.4028359e-02 2.7619120e-02 2.3041360e-02]\n",
      "iter 292\ttotal variance to retain\t0.0\n",
      "Epoch 46\n",
      "Iter 51700: loss: 0.4050\n",
      "\n",
      "iter 293\ttrain cost\t0.4050183892250061\n",
      "[2.6448633e+04 3.4650848e+02 6.3370137e+00 8.0564070e-01 7.2697103e-02\n",
      " 6.2064122e-02 5.6911819e-02 3.2796308e-02 2.8814532e-02 2.0651460e-02]\n",
      "iter 294\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 47\n",
      "Iter 52800: loss: 0.4014\n",
      "\n",
      "iter 295\ttrain cost\t0.401407927274704\n",
      "[2.7275768e+04 5.2408899e+02 5.0441322e+00 1.0384218e+00 7.3040277e-02\n",
      " 5.6081519e-02 4.0751223e-02 3.8185321e-02 3.1371780e-02 1.3227918e-02]\n",
      "iter 296\ttotal variance to retain\t0.0\n",
      "Epoch 48\n",
      "Iter 53900: loss: 0.3917\n",
      "\n",
      "iter 297\ttrain cost\t0.3916831612586975\n",
      "[2.8493686e+04 3.9222372e+02 4.9739232e+00 1.1762217e+00 8.1349224e-02\n",
      " 6.3240871e-02 4.5811091e-02 3.3543997e-02 2.2735992e-02 1.7205846e-02]\n",
      "iter 298\ttotal variance to retain\t0.0\n",
      "Epoch 49\n",
      "Iter 55000: loss: 0.3872\n",
      "\n",
      "iter 299\ttrain cost\t0.38715746998786926\n",
      "[2.9046936e+04 4.4246066e+02 3.1724906e+00 1.3412209e+00 1.3865820e-01\n",
      " 5.7601154e-02 4.3201961e-02 3.7837703e-02 2.5837841e-02 1.5458762e-02]\n",
      "iter 300\ttotal variance to retain\t0.0\n",
      "Epoch 50\n",
      "Iter 56100: loss: 0.3855\n",
      "\n",
      "iter 301\ttrain cost\t0.3854919672012329\n",
      "[3.6346461e+04 7.5189563e+02 6.3632154e+00 1.5660230e+00 2.0612507e-01\n",
      " 1.0131681e-01 5.8358599e-02 3.7714373e-02 2.8929440e-02 1.8898454e-02]\n",
      "iter 302\ttotal variance to retain\t0.0\n",
      "Epoch 51\n",
      "Iter 57200: loss: 0.3874\n",
      "\n",
      "iter 303\ttrain cost\t0.3873693346977234\n",
      "[3.0114490e+04 7.0117365e+02 8.1873999e+00 1.2478595e+00 3.5315901e-01\n",
      " 8.1012242e-02 4.8908528e-02 3.0208351e-02 2.4465779e-02 1.8493965e-02]\n",
      "iter 304\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 52\n",
      "Iter 58300: loss: 0.3847\n",
      "\n",
      "iter 305\ttrain cost\t0.3847189247608185\n",
      "[3.5090676e+04 6.7235278e+02 6.4899273e+00 1.8391078e+00 4.3132976e-01\n",
      " 1.2759998e-01 4.1138396e-02 3.0799828e-02 2.8171757e-02 1.5785495e-02]\n",
      "iter 306\ttotal variance to retain\t0.0\n",
      "Epoch 53\n",
      "Iter 59400: loss: 0.3830\n",
      "\n",
      "iter 307\ttrain cost\t0.38301676511764526\n",
      "[3.2494537e+04 8.7606854e+02 4.8301749e+00 1.5815030e+00 5.7907522e-01\n",
      " 8.1719324e-02 6.0963865e-02 3.6615349e-02 2.7132226e-02 1.8724516e-02]\n",
      "iter 308\ttotal variance to retain\t0.0\n",
      "Epoch 54\n",
      "Iter 60500: loss: 0.3803\n",
      "\n",
      "iter 309\ttrain cost\t0.38033831119537354\n",
      "[4.05649961e+04 6.88877319e+02 5.15734816e+00 2.31422782e+00\n",
      " 9.44121778e-01 1.49328232e-01 5.13856970e-02 4.16745730e-02\n",
      " 2.85539906e-02 1.41922925e-02]\n",
      "iter 310\ttotal variance to retain\t0.0\n",
      "Epoch 55\n",
      "Iter 61600: loss: 0.3771\n",
      "\n",
      "iter 311\ttrain cost\t0.3770529627799988\n",
      "[3.9962750e+04 8.0889587e+02 8.3501816e+00 2.3641405e+00 1.7408545e+00\n",
      " 2.1421212e-01 6.4167701e-02 3.0310972e-02 2.7764775e-02 2.4843469e-02]\n",
      "iter 312\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 56\n",
      "Iter 62700: loss: 0.3793\n",
      "\n",
      "iter 313\ttrain cost\t0.3793017864227295\n",
      "[3.7980992e+04 6.5653143e+02 7.2914090e+00 3.3371298e+00 2.1676080e+00\n",
      " 2.1079183e-01 6.5496579e-02 3.8211875e-02 2.6967643e-02 1.9667009e-02]\n",
      "iter 314\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 57\n",
      "Iter 63800: loss: 0.3779\n",
      "\n",
      "iter 315\ttrain cost\t0.37786805629730225\n",
      "[3.9396527e+04 6.1219647e+02 8.2503691e+00 5.1970353e+00 1.9607674e+00\n",
      " 2.7432257e-01 9.9716038e-02 6.3162394e-02 5.5331610e-02 1.7191419e-02]\n",
      "iter 316\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 64900: loss: 0.3664\n",
      "\n",
      "iter 317\ttrain cost\t0.3664402663707733\n",
      "[4.1533109e+04 8.5718719e+02 7.9615645e+00 4.5015354e+00 3.1174932e+00\n",
      " 3.0289227e-01 6.4233877e-02 3.8532488e-02 2.6320057e-02 1.6946759e-02]\n",
      "iter 318\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 59\n",
      "Iter 66000: loss: 0.3700\n",
      "\n",
      "iter 319\ttrain cost\t0.3699699342250824\n",
      "[4.1002523e+04 7.5784369e+02 8.4452295e+00 7.6871099e+00 3.2227159e+00\n",
      " 3.4553713e-01 6.9600463e-02 3.7907444e-02 3.1043749e-02 2.3662720e-02]\n",
      "iter 320\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 60\n",
      "Iter 67100: loss: 0.3697\n",
      "\n",
      "iter 321\ttrain cost\t0.3696810007095337\n",
      "[4.64760703e+04 7.23611938e+02 1.03748198e+01 7.39457321e+00\n",
      " 3.48308945e+00 3.69705707e-01 6.73668832e-02 3.41761075e-02\n",
      " 2.85248123e-02 1.43677415e-02]\n",
      "iter 322\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 61\n",
      "Iter 68200: loss: 0.3656\n",
      "\n",
      "iter 323\ttrain cost\t0.3656355142593384\n",
      "[4.5869918e+04 8.7538214e+02 1.0069300e+01 6.8868418e+00 3.0255194e+00\n",
      " 4.4867307e-01 1.0029540e-01 4.8268292e-02 3.5723440e-02 1.8739400e-02]\n",
      "iter 324\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 62\n",
      "Iter 69300: loss: 0.3676\n",
      "\n",
      "iter 325\ttrain cost\t0.3675884008407593\n",
      "[4.2440645e+04 6.9337451e+02 8.6849527e+00 7.5962625e+00 3.2387257e+00\n",
      " 6.0647756e-01 9.3390770e-02 5.5721428e-02 3.4560747e-02 1.8895019e-02]\n",
      "iter 326\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 63\n",
      "Iter 70400: loss: 0.3605\n",
      "\n",
      "iter 327\ttrain cost\t0.3604665994644165\n",
      "[4.2760410e+04 7.3143463e+02 9.6134796e+00 8.3290806e+00 3.6617651e+00\n",
      " 4.9660093e-01 1.4409153e-01 4.5438144e-02 2.9819241e-02 2.2535874e-02]\n",
      "iter 328\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 64\n",
      "Iter 71500: loss: 0.3613\n",
      "\n",
      "iter 329\ttrain cost\t0.36132490634918213\n",
      "[4.9959605e+04 9.5999243e+02 1.3755184e+01 6.6045256e+00 3.5125506e+00\n",
      " 7.9218721e-01 1.2597267e-01 4.3685526e-02 3.2116208e-02 1.4770384e-02]\n",
      "iter 330\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 65\n",
      "Iter 72600: loss: 0.3628\n",
      "\n",
      "iter 331\ttrain cost\t0.36277711391448975\n",
      "[4.4508777e+04 9.1010767e+02 1.1177383e+01 5.0380445e+00 3.0674360e+00\n",
      " 8.6047709e-01 1.0296693e-01 3.2686409e-02 2.8229643e-02 1.7450616e-02]\n",
      "iter 332\ttotal variance to retain\t5.960464477539063e-08\n",
      "Epoch 66\n",
      "Iter 73700: loss: 0.3556\n",
      "\n",
      "iter 333\ttrain cost\t0.35561272501945496\n",
      "[5.3041223e+04 8.3141730e+02 1.5209053e+01 5.8512568e+00 4.2291636e+00\n",
      " 6.6994983e-01 1.2797694e-01 6.9806091e-02 3.9244033e-02 1.7554721e-02]\n",
      "iter 334\ttotal variance to retain\t1.1920928955078125e-07\n",
      "Epoch 67\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "scores3 = []\n",
    "for part in range(10):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((_train_x[0:(part*37)], _train_x[(part+1)*37:442]), axis=0)\n",
    "    train_y = np.concatenate((_train_y[0:(part*37)], _train_y[(part+1)*37:442]), axis=0)\n",
    "    test_x = _train_x[part*37:(part+1)*37]\n",
    "    test_y = _train_y[part*37:(part+1)*37]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    transpose_train_x = np.transpose(train_x)\n",
    "\n",
    "    cur_w = np.random.normal(0, 0.35, (number_of_neurons,1))\n",
    "    cur_B =  np.random.normal(0, 0.35, (dimensionality, number_of_neurons))\n",
    "    cur_biases = np.zeros((number_of_neurons))\n",
    "    cur_P = np.zeros((dimensionality, dimensionality)) \n",
    "    cur_w_regr = np.random.normal(0, 0.35, (number_of_neurons_regr,1))\n",
    "    cur_B_regr = np.random.normal(0, 0.35, (dimensionality, number_of_neurons_regr))\n",
    "    cur_bias_regr = np.random.normal(0, 0.35, (number_of_neurons_regr))\n",
    "\n",
    "    O = np.zeros((dimensionality, k)) \n",
    "    cur_iter = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init, feed_dict={Dataplace: transpose_train_x})\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" %(epoch))\n",
    "        for iteration in range(num_iters):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            sess.run(target, feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x,\n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            cur_iter = cur_iter+1\n",
    "        reses = []\n",
    "        for i in range(100):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            res = sess.run([loss_hybrid], feed_dict={Data: sample_x, outputs:sample_y, Lbd: Lambda, Dataplace: transpose_train_x, \n",
    "                                        tf_data_w: cur_w,\n",
    "                                        tf_data_B: cur_B,\n",
    "                                        tf_data_P: cur_P,\n",
    "                                        tf_w_regr: cur_w_regr,\n",
    "                                        tf_B_regr: cur_B_regr,\n",
    "                                        tf_bias_regr: cur_bias_regr})\n",
    "            reses.append(res)\n",
    "            cur_iter = cur_iter+1\n",
    "        print (\"Iter %d: loss: %.4f\\n\" %(cur_iter, np.mean(np.array(reses))))\n",
    "        lib.plot.plot('train cost', np.mean(np.array(reses)))\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "\n",
    "        cur_w, cur_B, cur_w_regr, cur_B_regr, cur_bias_regr = sess.run([w, B, w_regr, B_regr, bias_regr])\n",
    "\n",
    "        third_grad_psi = np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))\n",
    "        third_grad_psi_regr = np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))\n",
    "        for r in range(1000):\n",
    "            sess.run([tf_data_second])\n",
    "            np.concatenate((third_grad_psi, np.reshape(sess.run([grad_psi]), (BATCH_SIZE_2, dimensionality))), axis=0) \n",
    "            np.concatenate((third_grad_psi_regr, np.reshape(sess.run([grad_psi_regr]), (BATCH_SIZE_2, dimensionality))), axis=0)\n",
    "\n",
    "        M = (1-rho)*np.matmul(np.transpose(third_grad_psi), third_grad_psi)+\\\n",
    "            rho*np.matmul(np.transpose(third_grad_psi_regr), third_grad_psi_regr)\n",
    "        u, s, vh = np.linalg.svd(M, full_matrices=True)\n",
    "        O = u[:,0:k:1]\n",
    "        print(s)\n",
    "        cur_P = np.matmul(O, np.transpose(O))\n",
    "        tvr = 1-np.sum(np.multiply(s[0:k],s[0:k]))/np.sum(np.multiply(s,s))\n",
    "        lib.plot.plot('total variance to retain', tvr)\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "    reduced_train_x = np.matmul(train_x, O)\n",
    "    reduced_test_x = np.matmul(test_x, O)\n",
    "    reg = LinearRegression().fit(reduced_train_x, train_y)\n",
    "    score = reg.score(reduced_test_x, test_y)\n",
    "    scores.append(score)\n",
    "    print (\"Part %d: score on test %.4f\\n\" %(part, score))\n",
    "    color = [str(item/346.) for item in test_y]\n",
    "    plt2.close()\n",
    "    plt2.scatter(np.transpose(reduced_test_x)[0], np.transpose(reduced_test_x)[1], s=100, c=color)\n",
    "    plt2.xlabel(\"1st component\")\n",
    "    plt2.ylabel(\"2nd component\")\n",
    "    plt2.show()\n",
    "    sess.close()\n",
    "    print(cur_P)\n",
    "    \n",
    "    test_y_prediction = []\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict_regr(10, reduced_train_x, train_y, test_image)\n",
    "        test_y_prediction.append(pred)\n",
    "    score3 = 100*(1.0-np.mean((np.array(test_y_prediction)- test_y)**2)/np.var(test_y))\n",
    "    scores3.append(score3)\n",
    "    print('acc:', str(round(score3, 2))+'%')\n",
    "print (\"Average 10-NN acc on test %.4f\\n\" %(np.mean(np.array(scores3))))\n",
    "print (\"Average score on test %.4f\\n\" %(np.mean(np.array(scores))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
